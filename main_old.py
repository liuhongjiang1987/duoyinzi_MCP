"""
åŸºäº MCP çš„æ™ºèƒ½æ•°æ®åˆ†æå·¥å…· - å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹

é‡‡ç”¨ FastMCP æ¡†æ¶å®ç°å®Œæ•´çš„æ•°æ®åˆ†æå·¥ä½œæµï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®ä¸Šä¼ ä¸è§£æï¼ˆCSV/Excelæ ¼å¼æ”¯æŒï¼‰
- æ™ºèƒ½å­—æ®µåˆ†æï¼ˆæ•°å€¼/åˆ†ç±»å­—æ®µç»Ÿè®¡åˆ†æï¼‰
- ææ€§è°ƒæ•´ç³»ç»Ÿï¼ˆè‡ªåŠ¨æ£€æµ‹å’Œæ‰‹åŠ¨é…ç½®å­—æ®µææ€§ï¼‰
- éš¶å±åº¦è®¡ç®—ï¼ˆä¸‹ç•Œå‹éš¶å±åº¦å‡½æ•°ï¼Œå¤šçº§åˆ«è®¡ç®—ï¼Œç»è¿‡ä¸¥æ ¼æ•°å­¦éªŒè¯ï¼‰
- æŒä¹…åŒ–èµ„æºç®¡ç†ï¼ˆè·¨ä¼šè¯æ•°æ®ä¿å­˜å’Œæ¢å¤ï¼‰

é¡¹ç›®çŠ¶æ€ï¼šv1.1.0 - ç”Ÿäº§å°±ç»ªï¼Œç»è¿‡å®Œæ•´éªŒè¯å’Œä¼˜åŒ–

ä¸»è¦ç‰¹æ€§ï¼š
âœ… å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ï¼šä»æ•°æ®ä¸Šä¼ åˆ°éš¶å±åº¦è®¡ç®—çš„å…¨æµç¨‹æ”¯æŒ
âœ… æ™ºèƒ½ææ€§è°ƒæ•´ï¼šè‡ªåŠ¨æ£€æµ‹å­—æ®µææ€§ï¼Œæ”¯æŒæ‰‹åŠ¨é…ç½®å’Œæ•°å€¼è½¬æ¢
âœ… ä¸‹ç•Œå‹éš¶å±åº¦è®¡ç®—ï¼šåŸºäºæ•°å­¦å…¬å¼çš„å¤šçº§åˆ«éš¶å±åº¦è®¡ç®—ï¼Œç»è¿‡ä¸¥æ ¼æ•°å­¦éªŒè¯
âœ… æŒä¹…åŒ–èµ„æºç®¡ç†ï¼šè·¨ä¼šè¯æ•°æ®ä¿å­˜å’Œæ¢å¤
âœ… èµ„æºæ ¼å¼å…¼å®¹ï¼šæ”¯æŒçº¯IDå’Œå®Œæ•´URIä¸¤ç§èµ„æºæ ‡è¯†æ ¼å¼
âœ… å®Œå–„çš„é”™è¯¯å¤„ç†ï¼šè¯¦ç»†çš„éªŒè¯å’Œé”™è¯¯æç¤ºæœºåˆ¶
âœ… ç®—æ³•æ­£ç¡®æ€§éªŒè¯ï¼šä¸‹ç•Œå‹éš¶å±å‡½æ•°ç»è¿‡å¤šåœºæ™¯æµ‹è¯•éªŒè¯ï¼Œç¡®ä¿æ•°å­¦ä¸¥è°¨æ€§
âœ… é¡¹ç›®ç»“æ„ä¼˜åŒ–ï¼šæ¸…ç†å†—ä½™æ–‡ä»¶ï¼Œä¿æŒä»£ç æ•´æ´å’Œå¯ç»´æŠ¤æ€§

ç‰ˆæœ¬å†å²ï¼š
v1.0.0 - æ ¸å¿ƒåŠŸèƒ½å®Œæ•´å®ç°ï¼Œèµ„æºIDæ ¼å¼å…¼å®¹æ€§ä¿®å¤
v1.1.0 - ä¸‹ç•Œå‹éš¶å±å‡½æ•°éªŒè¯ä¼˜åŒ–ï¼Œé¡¹ç›®ç»“æ„æ•´ç†ï¼Œç”Ÿäº§å°±ç»ª
"""

import io
import uuid
import pandas as pd
import numpy as np
import os
import json
import pickle
from typing import Dict, Any, Optional, List
from mcp.server.fastmcp import FastMCP
from modules.data_layer.field_analyzer import analyze_numeric_fields, analyze_categorical_fields, auto_detect_polarity, apply_polarity_adjustment, generate_polarity_report
from modules.algorithm_layer.membership_functions import LowerBoundMembershipFunctions
from modules.algorithm_layer.topsis_comprehensive_evaluation import TOPSISComprehensiveEvaluation
from modules.algorithm_layer.vikor_comprehensive_evaluation import VIKORComprehensiveEvaluation

# åˆ›å»º MCP æœåŠ¡å™¨
mcp = FastMCP("SmartDataAnalyzer")

# æ•°æ®ç¼“å­˜ï¼ˆå†…å­˜ç¼“å­˜ï¼ŒSession çº§ï¼‰
DATA_CACHE: Dict[str, pd.DataFrame] = {}

# æŒä¹…åŒ–å­˜å‚¨ç›®å½•
PERSISTENT_STORAGE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "persistent_resources")

# MVPé˜¶æ®µï¼šä¸åˆ›å»ºæŒä¹…åŒ–å­˜å‚¨ç›®å½•ï¼ˆç¦ç”¨ç£ç›˜å­˜å‚¨ï¼‰
# if not os.path.exists(PERSISTENT_STORAGE_DIR):
#     os.makedirs(PERSISTENT_STORAGE_DIR)

# èµ„æºç´¢å¼•æ–‡ä»¶è·¯å¾„
RESOURCE_INDEX_FILE = os.path.join(PERSISTENT_STORAGE_DIR, "resource_index.json")

# åŠ è½½èµ„æºç´¢å¼•ï¼ˆMVPé˜¶æ®µï¼šä¸åŠ è½½ç£ç›˜ç´¢å¼•æ–‡ä»¶ï¼‰
RESOURCE_INDEX = {}
# MVPé˜¶æ®µï¼šç¦ç”¨ç£ç›˜ç´¢å¼•åŠ è½½
# if os.path.exists(RESOURCE_INDEX_FILE):
#     try:
#         with open(RESOURCE_INDEX_FILE, 'r', encoding='utf-8') as f:
#             RESOURCE_INDEX = json.load(f)
#     except:
#         RESOURCE_INDEX = {}

# èµ„æºå‘½åè§„åˆ™å¸¸é‡
RESOURCE_TYPES = {
    "raw_data": "raw",           # åŸå§‹æ•°æ®ï¼ˆç”¨æˆ·ä¸Šä¼ ï¼‰
    "field_analysis": "fa",      # å­—æ®µåˆ†æç»“æœ
    "membership_calc": "mc",     # éš¶å±åº¦è®¡ç®—ç»“æœï¼ˆç›´æ¥ä½¿ç”¨rawæ•°æ®ï¼‰
    "multi_criteria": "mcr",     # å¤šå‡†åˆ™è®¡ç®—ç»“æœ
    "binary_semantic": "bs",     # äºŒå…ƒè¯­ä¹‰åˆ†å‰²ç»“æœ
    "other": "other"             # å…¶ä»–è®¡ç®—ç»“æœ
}


def generate_resource_id(resource_type: str, parent_resource_id: str = None, step_name: str = None) -> str:
    """
    ç”Ÿæˆæ ‡å‡†åŒ–çš„èµ„æºID
    
    Args:
        resource_type: èµ„æºç±»å‹ï¼ˆä½¿ç”¨RESOURCE_TYPESä¸­çš„é”®ï¼‰
        parent_resource_id: çˆ¶èµ„æºIDï¼ˆç”¨äºå»ºç«‹ä¾èµ–å…³ç³»ï¼‰
        step_name: æ­¥éª¤åç§°ï¼ˆå¯é€‰ï¼Œç”¨äºæè¿°æ€§æ ‡è¯†ï¼‰
        
    Returns:
        æ ‡å‡†åŒ–çš„èµ„æºIDæ ¼å¼ï¼š{type}_{uuid}_{parent_hash}_{step}
    """
    import hashlib
    import uuid
    
    # éªŒè¯èµ„æºç±»å‹
    if resource_type not in RESOURCE_TYPES:
        resource_type = "other"
    
    type_prefix = RESOURCE_TYPES[resource_type]
    
    # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
    unique_id = str(uuid.uuid4())[:8]
    
    # è®¡ç®—çˆ¶èµ„æºå“ˆå¸Œï¼ˆç”¨äºå»ºç«‹ä¾èµ–é“¾ï¼‰
    parent_hash = ""
    if parent_resource_id:
        parent_hash = hashlib.md5(parent_resource_id.encode()).hexdigest()[:6]
    
    # æ­¥éª¤åç§°å¤„ç†
    step_suffix = ""
    if step_name:
        # æ¸…ç†æ­¥éª¤åç§°ï¼Œåªä¿ç•™å­—æ¯æ•°å­—
        step_suffix = "_" + "".join(c for c in step_name if c.isalnum()).lower()[:10]
    
    # æ„å»ºå®Œæ•´èµ„æºID
    resource_id = f"{type_prefix}_{unique_id}"
    if parent_hash:
        resource_id += f"_{parent_hash}"
    if step_suffix:
        resource_id += step_suffix
    
    return resource_id


def get_resource_uri(resource_id: str) -> str:
    """
    æ ¹æ®èµ„æºIDç”Ÿæˆæ ‡å‡†URI
    
    Args:
        resource_id: æ ‡å‡†åŒ–çš„èµ„æºID
        
    Returns:
        èµ„æºURIæ ¼å¼ï¼šdata://{resource_id}
    """
    return f"data://{resource_id}"


def parse_resource_id(resource_id: str) -> Dict[str, str]:
    """
    è§£æèµ„æºIDï¼Œæå–ç±»å‹ã€çˆ¶èµ„æºç­‰ä¿¡æ¯
    
    Args:
        resource_id: æ ‡å‡†åŒ–çš„èµ„æºID
        
    Returns:
        è§£æåçš„èµ„æºä¿¡æ¯
    """
    parts = resource_id.split("_")
    
    if len(parts) < 2:
        return {"type": "unknown", "id": resource_id}
    
    type_code = parts[0]
    resource_type = "unknown"
    
    # åå‘æŸ¥æ‰¾ç±»å‹
    for type_name, code in RESOURCE_TYPES.items():
        if code == type_code:
            resource_type = type_name
            break
    
    result = {
        "type": resource_type,
        "type_code": type_code,
        "unique_id": parts[1],
        "id": resource_id
    }
    
    # è§£æçˆ¶èµ„æºå“ˆå¸Œ
    if len(parts) > 2 and len(parts[2]) == 6:
        result["parent_hash"] = parts[2]
    
    # è§£ææ­¥éª¤åç§°
    if len(parts) > 3:
        result["step_name"] = parts[3]
    
    return result


def save_resource_to_persistent_storage(resource_id: str, df: pd.DataFrame) -> str:
    """
    å°†èµ„æºä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨ï¼ˆMVPé˜¶æ®µï¼šä»…å†…å­˜å­˜å‚¨ï¼Œä¸æŒä¹…åŒ–åˆ°æ–‡ä»¶ï¼‰
    
    Args:
        resource_id: èµ„æºID
        df: è¦ä¿å­˜çš„DataFrame
        
    Returns:
        è™šæ‹Ÿæ–‡ä»¶è·¯å¾„ï¼ˆä»…ç”¨äºå…¼å®¹ï¼‰
    """
    # MVPé˜¶æ®µï¼šæ›´æ–°å†…å­˜ç´¢å¼•å’Œæ•°æ®ç¼“å­˜
    parsed_info = parse_resource_id(resource_id)
    uri = get_resource_uri(resource_id)
    
    # æ›´æ–°èµ„æºç´¢å¼•
    RESOURCE_INDEX[resource_id] = {
        "uri": uri,
        "type": parsed_info["type"],
        "type_code": parsed_info["type_code"],
        "data_shape": f"{len(df)} è¡Œ Ã— {len(df.columns)} åˆ—",
        "file_path": f"memory://{resource_id}",  # è™šæ‹Ÿå†…å­˜è·¯å¾„
        "created_time": pd.Timestamp.now().isoformat(),
        "parent_hash": parsed_info.get("parent_hash", ""),
        "step_name": parsed_info.get("step_name", "")
    }
    
    # ä¿å­˜åˆ°å†…å­˜ç¼“å­˜
    DATA_CACHE[uri] = df.copy()
    
    # ä¸ä¿å­˜ç´¢å¼•æ–‡ä»¶åˆ°ç£ç›˜ï¼ˆMVPé˜¶æ®µï¼‰
    # with open(RESOURCE_INDEX_FILE, 'w', encoding='utf-8') as f:
    #     json.dump(RESOURCE_INDEX, f, ensure_ascii=False, indent=2)
    
    return f"memory://{resource_id}"


def load_resource_from_persistent_storage(resource_id: str) -> pd.DataFrame:
    """
    ä»æŒä¹…åŒ–å­˜å‚¨åŠ è½½èµ„æºï¼ˆMVPé˜¶æ®µï¼šä»…ä»å†…å­˜åŠ è½½ï¼‰
    
    Args:
        resource_id: èµ„æºIDï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        
    Returns:
        åŠ è½½çš„DataFrame
        
    Raises:
        ValueError: èµ„æºä¸å­˜åœ¨æˆ–ä¸åœ¨å†…å­˜ä¸­
    """
    # å¦‚æœæ²¡æœ‰æä¾›resource_idï¼Œå°è¯•è‡ªåŠ¨å‘ç°mcèµ„æº
    if resource_id is None:
        resource_id = find_latest_membership_resource()
        if resource_id is None:
            raise ValueError("æœªæ‰¾åˆ°éš¶å±åº¦è®¡ç®—ç»“æœèµ„æºï¼ˆmcå¼€å¤´çš„èµ„æºï¼‰ï¼Œè¯·å…ˆæ‰§è¡Œéš¶å±åº¦è®¡ç®—")
        print(f"è‡ªåŠ¨å‘ç°éš¶å±åº¦èµ„æº: {resource_id}")
    
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id and resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    if resource_id not in RESOURCE_INDEX:
        raise ValueError(f"èµ„æºä¸å­˜åœ¨äºå­˜å‚¨: {resource_id}")
    
    # MVPé˜¶æ®µï¼šä»…ä»å†…å­˜ç¼“å­˜åŠ è½½
    uri = RESOURCE_INDEX[resource_id]["uri"]
    if uri not in DATA_CACHE:
        raise ValueError(f"èµ„æºä¸åœ¨å†…å­˜ç¼“å­˜ä¸­ï¼ˆMVPé˜¶æ®µä¸æ”¯æŒç£ç›˜åŠ è½½ï¼‰: {resource_id}")
    
    return DATA_CACHE[uri]


def load_all_persistent_resources():
    """
    åŠ è½½æ‰€æœ‰æŒä¹…åŒ–èµ„æºåˆ°å†…å­˜ç¼“å­˜ï¼ˆMVPé˜¶æ®µï¼šæ­¤åŠŸèƒ½å·²ç¦ç”¨ï¼‰
    """
    print("â„¹ï¸  MVPé˜¶æ®µï¼šç¦ç”¨æŒä¹…åŒ–èµ„æºåŠ è½½ï¼Œä»…ä½¿ç”¨å†…å­˜ç¼“å­˜")
    # MVPé˜¶æ®µï¼šä¸åŠ è½½ä»»ä½•æŒä¹…åŒ–èµ„æº
    # for resource_id, resource_info in RESOURCE_INDEX.items():
    #     try:
    #         df = load_resource_from_persistent_storage(resource_id)
    #         DATA_CACHE[resource_info["uri"]] = df
    #         print(f"âœ… å·²åŠ è½½æŒä¹…åŒ–èµ„æº: {resource_id} ({resource_info['data_shape']})")
    #     except Exception as e:
    #         print(f"âŒ åŠ è½½èµ„æºå¤±è´¥ {resource_id}: {e}")


# MVPé˜¶æ®µï¼šç¦ç”¨æŒä¹…åŒ–å­˜å‚¨ï¼Œä»…ä½¿ç”¨å†…å­˜ç¼“å­˜
# load_all_persistent_resources()  # æ³¨é‡Šæ‰æŒä¹…åŒ–åŠ è½½


@mcp.tool()
def get_resource_dependency_chain(resource_id: str) -> Dict[str, Any]:
    """
    è·å–èµ„æºçš„ä¾èµ–é“¾ä¿¡æ¯ - è¿½è¸ªèµ„æºç”Ÿæˆè·¯å¾„
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - åˆ†ææŒ‡å®šèµ„æºçš„å®Œæ•´ç”Ÿæˆè·¯å¾„å’Œä¾èµ–å…³ç³»
    - æ”¯æŒçº¯IDå’Œå®Œæ•´URIä¸¤ç§æ ¼å¼çš„èµ„æºæ ‡è¯†ç¬¦
    - è‡ªåŠ¨å¤„ç†èµ„æºIDæ ¼å¼å…¼å®¹æ€§ï¼ˆç§»é™¤data://å‰ç¼€ï¼‰
    
    åº”ç”¨åœºæ™¯ï¼š
    - è°ƒè¯•èµ„æºç”Ÿæˆæµç¨‹
    - åˆ†ææ•°æ®å¤„ç†é“¾
    - èµ„æºå…³ç³»å¯è§†åŒ–
    
    Args:
        resource_id: èµ„æºæ ‡è¯†ç¬¦ï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        
    Returns:
        Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
        - resource_id: åŸå§‹èµ„æºID
        - parsed_info: è§£æåçš„èµ„æºä¿¡æ¯
        - dependency_chain: ä¾èµ–é“¾åˆ—è¡¨ï¼ˆä»å½“å‰èµ„æºåˆ°æ ¹èµ„æºï¼‰
        - chain_length: ä¾èµ–é“¾é•¿åº¦
        
    ç¤ºä¾‹ï¼š
    >>> get_resource_dependency_chain("raw_abc123")
    {
        "resource_id": "raw_abc123",
        "parsed_info": {...},
        "dependency_chain": [...],
        "chain_length": 1
    }
    """
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    parsed_info = parse_resource_id(resource_id)
    
    # è·å–æ‰€æœ‰èµ„æºä¿¡æ¯
    all_resources = {}
    for uri in DATA_CACHE.keys():
        if uri.startswith("data://"):
            res_id = uri[7:]  # ç§»é™¤ "data://" å‰ç¼€
            all_resources[res_id] = parse_resource_id(res_id)
    
    # æ„å»ºä¾èµ–é“¾
    dependency_chain = []
    current_resource = parsed_info
    
    while current_resource:
        dependency_chain.append(current_resource)
        
        # æŸ¥æ‰¾çˆ¶èµ„æº
        parent_hash = current_resource.get("parent_hash")
        if not parent_hash:
            break
            
        # åœ¨æ‰€æœ‰èµ„æºä¸­æŸ¥æ‰¾åŒ¹é…çš„çˆ¶èµ„æº
        parent_resource = None
        for res_id, res_info in all_resources.items():
            if res_info.get("unique_id") == parent_hash:
                parent_resource = res_info
                break
        
        if not parent_resource:
            break
            
        current_resource = parent_resource
    
    return {
        "resource_id": resource_id,
        "parsed_info": parsed_info,
        "dependency_chain": dependency_chain,
        "chain_length": len(dependency_chain)
    }





@mcp.tool()
def generate_membership_config_template(resource_id: str) -> str:
    """
    ç”Ÿæˆä¸‹ç•Œå‹éš¶å±åº¦è®¡ç®—é…ç½®æ¨¡æ¿ - è‡ªåŠ¨åŒ–é…ç½®ç”Ÿæˆ
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - åŸºäºæ•°æ®ç‰¹å¾è‡ªåŠ¨ç”Ÿæˆä¸‹ç•Œå‹éš¶å±åº¦å‡½æ•°çš„é…ç½®æ¨¡æ¿
    - æ™ºèƒ½æ£€æµ‹å¹¶ä¼˜å…ˆä½¿ç”¨ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº
    - æ”¯æŒçº¯IDå’Œå®Œæ•´URIä¸¤ç§æ ¼å¼çš„èµ„æºæ ‡è¯†ç¬¦
    - è‡ªåŠ¨å¤„ç†èµ„æºIDæ ¼å¼å…¼å®¹æ€§ï¼ˆç§»é™¤data://å‰ç¼€ï¼‰
    
    ç®—æ³•ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨ä¸‹ç•Œå‹çº§åˆ«éš¶å±å‡½æ•°ï¼ˆåŸºäºå…¬å¼4-11åˆ°4-13ï¼‰
    - ä¸ºæ¯ä¸ªæ•°å€¼å­—æ®µç”Ÿæˆé»˜è®¤çš„3çº§åˆ«å‚æ•°é…ç½®
    - å‚æ•°åŸºäºå­—æ®µç»Ÿè®¡ç‰¹å¾ï¼ˆæœ€å¤§å€¼ã€æœ€å°å€¼ã€å¹³å‡å€¼ï¼‰
    
    å·¥ä½œæµç¨‹ï¼š
    1. æ£€æµ‹æ˜¯å¦å­˜åœ¨ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº
    2. åˆ†ææ•°æ®é›†çš„æ•°å€¼å­—æ®µç‰¹å¾
    3. ä¸ºæ¯ä¸ªå­—æ®µç”Ÿæˆé»˜è®¤çº§åˆ«å‚æ•°
    4. ç”ŸæˆMarkdownæ ¼å¼çš„é…ç½®æ¨¡æ¿
    
    Args:
        resource_id: åŸå§‹æ•°æ®èµ„æºæ ‡è¯†ç¬¦ï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        
    Returns:
        str: Markdownæ ¼å¼çš„ä¸‹ç•Œå‹å‡½æ•°çº§åˆ«å‚æ•°é…ç½®æ¨¡æ¿ï¼ŒåŒ…å«ï¼š
        - èµ„æºä¿¡æ¯æ‘˜è¦
        - å­—æ®µç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼
        - æ¨èé…ç½®æ¨¡æ¿ï¼ˆJSONæ ¼å¼ï¼‰
        - é…ç½®è¯´æ˜å’Œä½¿ç”¨æŒ‡å—
        
    ç¤ºä¾‹è¾“å‡ºï¼š
    ```markdown
    # ä¸‹ç•Œå‹éš¶å±åº¦è®¡ç®—é…ç½®æ¨¡æ¿
    
    ## èµ„æºä¿¡æ¯
    - **èµ„æºID**: `raw_abc123`
    - **æ•°å€¼å­—æ®µ**: age, salary
    - **æ•°æ®å½¢çŠ¶**: 100 è¡Œ Ã— 5 åˆ—
    
    ## å­—æ®µç»Ÿè®¡ä¿¡æ¯
    | å­—æ®µ | æœ€å°å€¼ | æœ€å¤§å€¼ | å¹³å‡å€¼ | æ ‡å‡†å·® |
    |------|--------|--------|--------|--------|
    | age | 20.00 | 60.00 | 35.50 | 8.25 |
    
    ## æ¨èé…ç½®æ¨¡æ¿
    ```json
    {
      "çº§åˆ«å‚æ•°": {
        "age": [48.0, 30.0, 12.0],
        "salary": [80000.0, 50000.0, 20000.0]
      }
    }
    ```
    """
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº
    polarity_adjusted_resource_id = None
    for cached_uri in DATA_CACHE.keys():
        if cached_uri.startswith("data://") and "polarity_adjusted" in cached_uri:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å½“å‰èµ„æºçš„ææ€§è°ƒæ•´ç‰ˆæœ¬
            adjusted_parsed = parse_resource_id(cached_uri[7:])  # ç§»é™¤ "data://" å‰ç¼€
            if (adjusted_parsed["type"] == "raw_data" and 
                adjusted_parsed.get("parent_hash") == resource_id):
                polarity_adjusted_resource_id = cached_uri[7:]  # ç§»é™¤ "data://" å‰ç¼€
                break
    
    # ä¼˜å…ˆä½¿ç”¨ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº
    if polarity_adjusted_resource_id:
        print(f"ğŸ” æ£€æµ‹åˆ°ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº: {polarity_adjusted_resource_id}")
        print("ğŸ“Š å°†ä½¿ç”¨ææ€§è°ƒæ•´åçš„æ•°æ®è¿›è¡Œéš¶å±åº¦è®¡ç®—é…ç½®")
        uri = get_resource_uri(polarity_adjusted_resource_id)
        used_resource_id = polarity_adjusted_resource_id
    else:
        uri = get_resource_uri(resource_id)
        used_resource_id = resource_id
    
    if uri not in DATA_CACHE:
        raise ValueError(f"èµ„æºä¸å­˜åœ¨: {uri}")
    
    df = DATA_CACHE[uri]
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºrawæ•°æ®
    parsed_info = parse_resource_id(used_resource_id)
    if parsed_info["type"] != "raw_data":
        print(f"âš ï¸ è­¦å‘Š: ä½¿ç”¨éåŸå§‹æ•°æ®ç”Ÿæˆé…ç½®æ¨¡æ¿ï¼Œå»ºè®®ä½¿ç”¨rawæ•°æ®")
    
    # è·å–æ•°å€¼å­—æ®µ
    import numpy as np
    numeric_fields = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if not numeric_fields:
        raise ValueError("æ•°æ®é›†ä¸­æ— æ•°å€¼å­—æ®µï¼Œæ— æ³•ç”Ÿæˆéš¶å±åº¦è®¡ç®—é…ç½®")
    
    # ä¸ºæ¯ä¸ªæ•°å€¼å­—æ®µç”Ÿæˆé»˜è®¤çº§åˆ«å‚æ•°ï¼ˆä¸‹ç•Œå‹å‡½æ•°éœ€è¦å¤šä¸ªçº§åˆ«å‚æ•°ï¼‰
    level_params_template = {}
    field_statistics = {}
    for field in numeric_fields:
        field_data = df[field].dropna()
        if len(field_data) > 0:
            min_val = float(field_data.min())
            max_val = float(field_data.max())
            
            # ä¸ºæ¯ä¸ªå­—æ®µç”Ÿæˆé»˜è®¤çº§åˆ«å‚æ•°åˆ—è¡¨ï¼ˆ3ä¸ªçº§åˆ«ï¼‰
            level_params_template[field] = [
                round(max_val * 0.8, 2),  # çº§åˆ«1å‚æ•°
                round(max_val * 0.5, 2),  # çº§åˆ«2å‚æ•°
                round(max_val * 0.2, 2)   # çº§åˆ«3å‚æ•°
            ]
            
            field_statistics[field] = {
                "min": min_val,
                "max": max_val,
                "mean": float(field_data.mean()),
                "std": float(field_data.std())
            }
    
    # ç”ŸæˆMarkdownæ ¼å¼çš„é…ç½®æ¨¡æ¿
    markdown_content = f"""# ä¸‹ç•Œå‹éš¶å±åº¦è®¡ç®—é…ç½®æ¨¡æ¿

## èµ„æºä¿¡æ¯
- **èµ„æºID**: `{used_resource_id}`
- **èµ„æºç±»å‹**: {'ææ€§è°ƒæ•´åçš„æ•°æ®' if polarity_adjusted_resource_id else 'åŸå§‹æ•°æ®'}
- **æ•°å€¼å­—æ®µ**: {', '.join(numeric_fields)}
- **æ•°æ®å½¢çŠ¶**: {len(df)} è¡Œ Ã— {len(df.columns)} åˆ—

## å­—æ®µç»Ÿè®¡ä¿¡æ¯

| å­—æ®µ | æœ€å°å€¼ | æœ€å¤§å€¼ | å¹³å‡å€¼ | æ ‡å‡†å·® |
|------|--------|--------|--------|--------|
"""
    
    # æ·»åŠ å­—æ®µç»Ÿè®¡è¡¨æ ¼è¡Œ
    for field in numeric_fields:
        stats = field_statistics[field]
        markdown_content += f"| {field} | {stats['min']:.2f} | {stats['max']:.2f} | {stats['mean']:.2f} | {stats['std']:.2f} |\n"
    
    markdown_content += """
## æ¨èé…ç½®æ¨¡æ¿

```json
{
  "çº§åˆ«å‚æ•°": {
"""
    
    # æ·»åŠ çº§åˆ«å‚æ•°é…ç½®
    for i, field in enumerate(numeric_fields):
        params = level_params_template[field]
        markdown_content += f'    "{field}": {params}'
        if i < len(numeric_fields) - 1:
            markdown_content += ","
        markdown_content += "\n"
    
    markdown_content += """  }
}
```

## é…ç½®è¯´æ˜

### ä¸‹ç•Œå‹éš¶å±åº¦å‡½æ•°ç‰¹ç‚¹
- ä½¿ç”¨ä¸‹ç•Œå‹çº§åˆ«éš¶å±å‡½æ•°ï¼ˆåŸºäºå…¬å¼4-11åˆ°4-13ï¼‰
- æ¯ä¸ªå­—æ®µéœ€è¦é…ç½®çº§åˆ«å‚æ•°åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š`[15, 10, 5]` å¯¹åº”3ä¸ªçº§åˆ«
- çº§åˆ«å‚æ•°åº”æŒ‰ä»å¤§åˆ°å°é¡ºåºæ’åˆ—
- æ€»çº§åˆ«æ•°åº”ä¸çº§åˆ«å‚æ•°æ•°é‡ä¸€è‡´

### ä½¿ç”¨æµç¨‹
1. å¤åˆ¶ä¸Šé¢çš„é…ç½®æ¨¡æ¿
2. æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´çº§åˆ«å‚æ•°
3. è°ƒç”¨ `validate_membership_config(resource_id, config)` éªŒè¯é…ç½®
4. éªŒè¯é€šè¿‡åè°ƒç”¨ `calculate_membership_with_config(resource_id, config)` æ‰§è¡Œè®¡ç®—

### é…ç½®ç¤ºä¾‹
```json
{
  "çº§åˆ«å‚æ•°": {
    "å­—æ®µ1": [15, 10, 5],
    "å­—æ®µ2": [180, 90, 60]
  }
}
```

**æ³¨æ„**: é…ç½®ä¸­ä¸å†éœ€è¦"éš¶å±å‡½æ•°"å­—æ®µï¼Œç³»ç»Ÿé»˜è®¤ä½¿ç”¨ä¸‹ç•Œå‹å‡½æ•°ã€‚
"""
    
    return markdown_content


@mcp.tool()
def validate_membership_config(resource_id: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    éªŒè¯ä¸‹ç•Œå‹éš¶å±åº¦è®¡ç®—é…ç½®
    
    Args:
        resource_id: åŸå§‹æ•°æ®èµ„æºæ ‡è¯†ç¬¦ï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        config: ç”¨æˆ·æä¾›çš„é…ç½®ä¿¡æ¯
        
    Returns:
        éªŒè¯ç»“æœ
    """
    # å¦‚æœæ²¡æœ‰æä¾›resource_idï¼Œå°è¯•è‡ªåŠ¨å‘ç°mcèµ„æº
    if resource_id is None:
        resource_id = find_latest_membership_resource()
        if resource_id is None:
            raise ValueError("æœªæ‰¾åˆ°éš¶å±åº¦è®¡ç®—ç»“æœèµ„æºï¼ˆmcå¼€å¤´çš„èµ„æºï¼‰ï¼Œè¯·å…ˆæ‰§è¡Œéš¶å±åº¦è®¡ç®—")
        print(f"è‡ªåŠ¨å‘ç°éš¶å±åº¦èµ„æº: {resource_id}")
    
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    uri = get_resource_uri(resource_id)
    
    if uri not in DATA_CACHE:
        raise ValueError(f"èµ„æºä¸å­˜åœ¨: {uri}")
    
    df = DATA_CACHE[uri]
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº
    is_polarity_adjusted = "polarity_adjusted" in resource_id
    
    # è·å–æ•°å€¼å­—æ®µ
    import numpy as np
    numeric_fields = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # éªŒè¯é…ç½®ç»“æ„
    errors = []
    warnings = []
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    if "çº§åˆ«å‚æ•°" not in config:
        errors.append("é…ç½®ä¸­ç¼ºå°‘'çº§åˆ«å‚æ•°'å­—æ®µ")
    
    if errors:
        return {
            "is_valid": False,
            "errors": errors,
            "warnings": warnings
        }
    
    # è®¾ç½®éš¶å±å‡½æ•°ç±»å‹ä¸ºä¸‹ç•Œå‹
    membership_type = "ä¸‹ç•Œå‹"
    
    # éªŒè¯çº§åˆ«å‚æ•°
    level_params = config["çº§åˆ«å‚æ•°"]
    if not isinstance(level_params, dict):
        errors.append("'çº§åˆ«å‚æ•°'å­—æ®µå¿…é¡»ä¸ºå­—å…¸ç±»å‹")
    else:
        # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨
        for field in level_params.keys():
            if field not in numeric_fields:
                errors.append(f"å­—æ®µ '{field}' åœ¨æ•°æ®é›†ä¸­ä¸å­˜åœ¨")
        
        # éªŒè¯æ‰€æœ‰å­—æ®µçš„çº§åˆ«å‚æ•°æ•°é‡ä¸€è‡´
        level_counts = {}
        for field, params in level_params.items():
            if field in numeric_fields:
                if not isinstance(params, list):
                    errors.append(f"å­—æ®µ '{field}' çš„çº§åˆ«å‚æ•°å¿…é¡»ä¸ºåˆ—è¡¨")
                else:
                    # æ£€æŸ¥çº§åˆ«å‚æ•°æ•°é‡
                    if len(params) < 2:
                        errors.append(f"å­—æ®µ '{field}' çš„çº§åˆ«å‚æ•°è‡³å°‘éœ€è¦2ä¸ªï¼Œå½“å‰æä¾›{len(params)}ä¸ª")
                    
                    # æ£€æŸ¥çº§åˆ«å‚æ•°æ˜¯å¦æŒ‰ä»å¤§åˆ°å°é¡ºåºæ’åˆ—
                    if len(params) > 1:
                        for i in range(len(params) - 1):
                            # æ£€æŸ¥å‚æ•°aå€¼æ˜¯å¦æŒ‰ä»å¤§åˆ°å°é¡ºåºæ’åˆ—
                            # å¤„ç†ä¸¤ç§æ ¼å¼çš„å‚æ•°ï¼šæ•°å€¼åˆ—è¡¨æˆ–åŒ…å«"a"é”®çš„å­—å…¸åˆ—è¡¨
                            if isinstance(params[i], dict) and "a" in params[i]:
                                current_val = params[i].get("a", 0)
                                next_val = params[i + 1].get("a", 0)
                            else:
                                current_val = params[i]
                                next_val = params[i + 1]
                            
                            if current_val <= next_val:
                                errors.append(f"å­—æ®µ '{field}' çš„çº§åˆ«å‚æ•°åº”æŒ‰ä»å¤§åˆ°å°é¡ºåºæ’åˆ—")
                                break
                    
                    level_counts[field] = len(params)
        
        # æ£€æŸ¥çº§åˆ«å‚æ•°æ•°é‡æ˜¯å¦åœ¨æ‰€æœ‰å­—æ®µä¸­ä¸€è‡´
        if level_counts:
            unique_counts = set(level_counts.values())
            if len(unique_counts) > 1:
                warnings.append(f"å„å­—æ®µçš„çº§åˆ«å‚æ•°æ•°é‡ä¸ä¸€è‡´: {level_counts}ï¼Œå»ºè®®ä¿æŒä¸€è‡´ä»¥è·å¾—æ›´å¥½çš„è¯„ä¼°ç»“æœ")
    
    # éªŒè¯æ€»çº§åˆ«æ•°
    if "æ€»çº§åˆ«æ•°" in config:
        total_levels = config["æ€»çº§åˆ«æ•°"]
        if not isinstance(total_levels, int) or total_levels < 2:
            errors.append("'æ€»çº§åˆ«æ•°'å¿…é¡»ä¸ºå¤§äºç­‰äº2çš„æ•´æ•°")
        elif level_params:
            # æ£€æŸ¥æ€»çº§åˆ«æ•°æ˜¯å¦ä¸çº§åˆ«å‚æ•°æ•°é‡ä¸€è‡´
            first_field_params = next(iter(level_params.values()))
            if isinstance(first_field_params, list) and total_levels != len(first_field_params):
                warnings.append(f"æ€»çº§åˆ«æ•°({total_levels})ä¸ç¬¬ä¸€ä¸ªå­—æ®µçš„çº§åˆ«å‚æ•°æ•°é‡({len(first_field_params)})ä¸ä¸€è‡´")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é…ç½®ä½†æ•°æ®ä¸­ä¸å­˜åœ¨çš„å­—æ®µ
    configured_fields = set(level_params.keys())
    available_fields = set(numeric_fields)
    missing_config = available_fields - configured_fields
    if missing_config:
        warnings.append(f"ä»¥ä¸‹æ•°å€¼å­—æ®µæœªé…ç½®çº§åˆ«å‚æ•°: {list(missing_config)}")
    
    return {
        "is_valid": len(errors) == 0,
        "errors": errors,
        "warnings": warnings,
        "config_summary": {
            "membership_type": membership_type,
            "configured_fields": list(configured_fields),
            "available_fields": numeric_fields,
            "level_params_summary": level_counts if level_params else {}
        }
    }


@mcp.tool()
def calculate_membership_with_config(resource_id: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä½¿ç”¨ä¸‹ç•Œå‹å‡½æ•°é…ç½®ä¿¡æ¯æ‰§è¡Œéš¶å±åº¦è®¡ç®— - å¤šçº§åˆ«æ¨¡ç³Šç»¼åˆè¯„ä»·å·¥å…·
    
    è¯¥å‡½æ•°å®ç°åŸºäºä¸‹ç•Œå‹éš¶å±åº¦å‡½æ•°çš„æ¨¡ç³Šç»¼åˆè¯„ä»·ï¼Œå°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºéš¶å±åº¦çŸ©é˜µæ ¼å¼ï¼Œ
    ä¸ºåç»­çš„å¤šå‡†åˆ™å†³ç­–åˆ†ææä¾›æ ‡å‡†åŒ–çš„è¾“å…¥æ•°æ®ã€‚
    
    Args:
        resource_id: åŸå§‹æ•°æ®èµ„æºæ ‡è¯†ç¬¦ï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
                     ä¾‹å¦‚ï¼š'raw_data_001' æˆ– 'data://raw_data_001'
        config: ç”¨æˆ·æä¾›çš„ä¸‹ç•Œå‹å‡½æ•°é…ç½®ä¿¡æ¯ï¼ŒåŒ…å«å„è¯„ä»·å› å­çš„çº§åˆ«å‚æ•°

    é‡è¦ï¼šå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ¨¡ç‰ˆè¾“å‡ºç»“æœï¼Œä¸å¾—æœ‰æ¨¡ç‰ˆä»¥å¤–çš„ä»»ä½•å…¶å®ƒä¿¡æ¯ï¼ï¼            
    Returns:
        å›å¤æ¨¡ç‰ˆ
        â€˜â€™â€˜
        ## ğŸ“Š éš¶å±åº¦è®¡ç®—æ¦‚è¿°
        - **è®¡ç®—æ–¹æ³•**ï¼šä¸‹ç•Œå‹éš¶å±åº¦å‡½æ•°
        - **ä¸šåŠ¡å¯¹è±¡æ•°é‡**ï¼š3ä¸ª
        - **è¯„ä»·å› å­æ•°é‡**ï¼š4ä¸ª
        - **è¯„ä¼°çº§åˆ«æ•°é‡**ï¼š4çº§ï¼ˆçº§åˆ«1-çº§åˆ«4ï¼‰
        
        ## ğŸ“ˆ éš¶å±åº¦çŸ©é˜µç»“æœï¼ˆè¿™é‡Œä»…å±•ç¤ºä¸€ä¸ªå¯¹è±¡çš„æ•°æ®ï¼Œç”¨æˆ·å¯ä»¥å›å¤è¦æ±‚ä¸‹è½½å…¨éƒ¨æ•°æ®ï¼‰
        
        ### ğŸ“Œ ä¸šåŠ¡å¯¹è±¡1
        | è¯„ä»·å› å­ | çº§åˆ«1 | çº§åˆ«2 | çº§åˆ«3 | çº§åˆ«4 |
        |---------|-------|-------|-------|-------|
        | è¯„ä»·å› å­1 | 1.0000 | 0.8865 | 0.5910 | 0.2955 |
        | è¯„ä»·å› å­2 | 0.7797 | 1.0000 | 0.5619 | 0.3746 |
        | è¯„ä»·å› å­3 | 1.0000 | 0.2836 | 0.1418 | 0.0709 |
        | è¯„ä»·å› å­4 | 1.0000 | 0.8571 | 0.5714 | 0.2857 |
        
        ## ğŸ“‹ å¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
        1. **éš¶å±åº¦çŸ©é˜µ** - æ ¸å¿ƒè®¡ç®—ç»“æœï¼Œå¯ç›´æ¥ç”¨äºTOPSIS/VIKORè¯„ä¼°
        2. **åŸå§‹æ•°æ®è¡¨** - è®¡ç®—å‰çš„åŸå§‹è¾“å…¥æ•°æ®
        3. **é…ç½®å‚æ•°è¡¨** - ä½¿ç”¨çš„éš¶å±åº¦å‡½æ•°å‚æ•°é…ç½®
        
        ## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®
        - ä½¿ç”¨TOPSISæ–¹æ³•è¿›è¡Œç»¼åˆè¯„ä¼°ï¼ˆperform_topsis_comprehensive_evaluationå·¥å…·ï¼‰
        - ä½¿ç”¨VIKORæ–¹æ³•è¿›è¡Œå¦¥åæ’åºåˆ†æï¼ˆperform_vikor_comprehensive_evaluationå·¥å…·ï¼‰
        - æŸ¥çœ‹éš¶å±åº¦çŸ©é˜µè¯¦ç»†ä¿¡æ¯ï¼ˆexport_resource_to_csvå·¥å…·ï¼‰
        - éªŒè¯éš¶å±åº¦è®¡ç®—ç»“æœï¼ˆvalidate_membership_configå·¥å…·ï¼‰
        â€™â€˜â€™
    """
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº
    polarity_adjusted_resource_id = None
    for cached_uri in DATA_CACHE.keys():
        if cached_uri.startswith("data://") and "polarity_adjusted" in cached_uri:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å½“å‰èµ„æºçš„ææ€§è°ƒæ•´ç‰ˆæœ¬
            adjusted_parsed = parse_resource_id(cached_uri[7:])  # ç§»é™¤ "data://" å‰ç¼€
            if (adjusted_parsed["type"] == "raw_data" and 
                adjusted_parsed.get("parent_hash") == resource_id):
                polarity_adjusted_resource_id = cached_uri[7:]  # ç§»é™¤ "data://" å‰ç¼€
                break
    
    # ä¼˜å…ˆä½¿ç”¨ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº
    if polarity_adjusted_resource_id:
        print(f"ğŸ” æ£€æµ‹åˆ°ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº: {polarity_adjusted_resource_id}")
        print("ğŸ“Š å°†ä½¿ç”¨ææ€§è°ƒæ•´åçš„æ•°æ®è¿›è¡Œéš¶å±åº¦è®¡ç®—")
        used_resource_id = polarity_adjusted_resource_id
    else:
        used_resource_id = resource_id
    
    # è‡ªåŠ¨éªŒè¯é…ç½®
    validation_result = validate_membership_config(used_resource_id, config)
    if not validation_result["is_valid"]:
        raise ValueError(f"é…ç½®éªŒè¯å¤±è´¥: {validation_result['errors']}")
    
    uri = get_resource_uri(used_resource_id)
    df = DATA_CACHE[uri]
    
    # è·å–é…ç½®ä¿¡æ¯
    level_params = config["çº§åˆ«å‚æ•°"]
    
    # è®¡ç®—éš¶å±åº¦çŸ©é˜µ
    membership_results = []
    
    for _, row in df.iterrows():
        membership_row = {}
        for field, params in level_params.items():
            if field in df.columns:
                value = row[field]
                total_levels = len(params)
                
                # è®¡ç®—æ¯ä¸ªçº§åˆ«çš„éš¶å±åº¦
                level_memberships = {}
                # æå–å‚æ•°å€¼åˆ—è¡¨ï¼ˆaå€¼ï¼‰
                # å¤„ç†ä¸¤ç§æ ¼å¼çš„å‚æ•°ï¼šæ•°å€¼åˆ—è¡¨æˆ–åŒ…å«"a"é”®çš„å­—å…¸åˆ—è¡¨
                if isinstance(params[0], dict) and "a" in params[0]:
                    param_values = [p["a"] for p in params]
                else:
                    param_values = params  # ç›´æ¥ä½¿ç”¨æ•°å€¼åˆ—è¡¨
                
                for level in range(1, total_levels + 1):
                    membership = LowerBoundMembershipFunctions.lower_bound_level_membership(
                        value, param_values, level, total_levels
                    )
                    level_memberships[f"çº§åˆ«{level}"] = round(float(membership), 4)
                
                membership_row[field] = level_memberships
        
        membership_results.append(membership_row)
    
    # åˆ›å»ºæ‰å¹³åŒ–çš„éš¶å±åº¦çŸ©é˜µDataFrameï¼ˆç¬¦åˆTOPSISè¦æ±‚æ ¼å¼ï¼‰
    flattened_results = []
    
    for idx, (_, row) in enumerate(df.iterrows()):
        # è·å–ä¸šåŠ¡å¯¹è±¡æ ‡è¯†
        obj_name = f"ä¸šåŠ¡å¯¹è±¡{idx+1}"
        if len(df.columns) > 0:
            # å°è¯•ä½¿ç”¨ç¬¬ä¸€åˆ—çš„å€¼ä½œä¸ºå¯¹è±¡å
            first_col_value = row.iloc[0]
            if pd.notna(first_col_value):
                obj_name = f"ä¸šåŠ¡å¯¹è±¡{idx+1}({str(first_col_value)[:20]})"
        
        # ä¸ºæ¯ä¸ªå­—æ®µå’Œçº§åˆ«åˆ›å»ºæ‰å¹³åŒ–è®°å½•
        for field, params in level_params.items():
            if field in df.columns and field in membership_results[idx]:
                total_levels = len(params)
                for level in range(1, total_levels + 1):
                    level_name = f"çº§åˆ«{level}"
                    membership_value = membership_results[idx][field].get(level_name, 0.0)
                    
                    flattened_results.append({
                        'ä¸šåŠ¡å¯¹è±¡': obj_name,
                        'è¯„ä»·å› å­': field,
                        'è¯„ä¼°çº§åˆ«': level_name,
                        'éš¶å±åº¦': membership_value
                    })
    
    # åˆ›å»ºç¬¦åˆTOPSISè¦æ±‚çš„éš¶å±åº¦çŸ©é˜µ
    membership_df = pd.DataFrame(flattened_results)
    
    # ç”Ÿæˆè®¡ç®—ç»“æœèµ„æºID
    result_resource_id = generate_resource_id(
        "membership_calc", 
        parent_resource_id=resource_id,
        step_name="membership_calculation"
    )
    result_uri = get_resource_uri(result_resource_id)
    
    # ç¼“å­˜è®¡ç®—ç»“æœåˆ°å†…å­˜
    DATA_CACHE[result_uri] = membership_df
    
    # æ›´æ–°èµ„æºç´¢å¼•ï¼ˆä¿®å¤éš¶å±åº¦è®¡ç®—ç»“æœèµ„æºä¸å­˜åœ¨çš„æ ¹æœ¬é—®é¢˜ï¼‰
    RESOURCE_INDEX[result_resource_id] = {
        'uri': result_uri,
        'type': 'membership_calc',
        'rows': len(membership_df),
        'columns': len(membership_df.columns),
        'column_names': list(membership_df.columns),
        'parent_resource_id': resource_id
    }
    
    # MVPé˜¶æ®µï¼šä¸ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨ï¼ˆå·²åœ¨å†…å­˜ä¸­ï¼‰
    # save_resource_to_persistent_storage(result_resource_id, membership_df)
    
    # ç”Ÿæˆæ¯ä¸ªä¸šåŠ¡å¯¹è±¡çš„éš¶å±åº¦çŸ©é˜µæ ¼å¼åŒ–è¾“å‡º
    membership_matrices_formatted = []
    for idx, (_, row) in enumerate(df.iterrows()):
        # è·å–ä¸šåŠ¡å¯¹è±¡æ ‡è¯†ï¼ˆä½¿ç”¨ç´¢å¼•æˆ–ç¬¬ä¸€åˆ—ä½œä¸ºæ ‡è¯†ï¼‰
        obj_name = f"ä¸šåŠ¡å¯¹è±¡{idx+1}"
        if len(df.columns) > 0:
            # å°è¯•ä½¿ç”¨ç¬¬ä¸€åˆ—çš„å€¼ä½œä¸ºå¯¹è±¡å
            first_col_value = row.iloc[0]
            if pd.notna(first_col_value):
                obj_name += f"({str(first_col_value)[:20]})"  # æˆªå–å‰20ä¸ªå­—ç¬¦
        
        # æ„å»ºçŸ©é˜µè¡¨æ ¼
        matrix_lines = [f"### ğŸ“Œ {obj_name}"]
        matrix_lines.append("")
        
        # è·å–æ‰€æœ‰çº§åˆ«
        all_levels = []
        for field in level_params.keys():
            if field in df.columns and field in membership_results[idx]:
                levels = list(membership_results[idx][field].keys())
                if levels:
                    all_levels = levels
                    break
        
        # æ„å»ºè¡¨å¤´
        if all_levels:
            header = "| è¯„ä»·å› å­ | " + " | ".join(all_levels) + " |"
            separator = "|---------" + "|-------" * len(all_levels) + "|"
            matrix_lines.append(header)
            matrix_lines.append(separator)
            
            # æ·»åŠ æ¯è¡Œæ•°æ®
            for field in level_params.keys():
                if field in df.columns and field in membership_results[idx]:
                    row_data = [field]
                    for level in all_levels:
                        if level in membership_results[idx][field]:
                            row_data.append(f"{membership_results[idx][field][level]:.4f}")
                        else:
                            row_data.append("0.0000")
                    matrix_lines.append("| " + " | ".join(row_data) + " |")
        
        matrix_lines.append("")  # æ·»åŠ ç©ºè¡Œåˆ†éš”
        membership_matrices_formatted.append("\n".join(matrix_lines))
    
    # è®¡ç®—æ‘˜è¦
    calculation_summary = {
        "input_data_shape": f"{len(df)} è¡Œ Ã— {len(df.columns)} åˆ—",
        "membership_type": "ä¸‹ç•Œå‹",
        "configured_fields": list(level_params.keys()),
        "total_levels": {field: len(params) for field, params in level_params.items()},
        "membership_matrix_shape": f"{len(membership_df)} è¡Œ Ã— {len(membership_df.columns)} åˆ—",
        "calculation_time": "å®æ—¶è®¡ç®—",
        "total_objects": len(df),
        "formatted_matrices": membership_matrices_formatted  # æ·»åŠ æ ¼å¼åŒ–çŸ©é˜µ
    }
    
    # ç”Ÿæˆå¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
    available_tables = """1. **éš¶å±åº¦çŸ©é˜µ** - æ ¸å¿ƒè®¡ç®—ç»“æœï¼Œå¯ç›´æ¥ç”¨äºTOPSIS/VIKORè¯„ä¼°
2. **åŸå§‹æ•°æ®è¡¨** - è®¡ç®—å‰çš„åŸå§‹è¾“å…¥æ•°æ®
3. **é…ç½®å‚æ•°è¡¨** - ä½¿ç”¨çš„éš¶å±åº¦å‡½æ•°å‚æ•°é…ç½®"""
    
    # ç”Ÿæˆä¸‹ä¸€æ­¥æ“ä½œå»ºè®®
    next_actions = """- ä½¿ç”¨TOPSISæ–¹æ³•è¿›è¡Œç»¼åˆè¯„ä¼°ï¼ˆperform_topsis_comprehensive_evaluationå·¥å…·ï¼‰
- ä½¿ç”¨VIKORæ–¹æ³•è¿›è¡Œå¦¥åæ’åºåˆ†æï¼ˆperform_vikor_comprehensive_evaluationå·¥å…·ï¼‰
- æŸ¥çœ‹éš¶å±åº¦çŸ©é˜µè¯¦ç»†ä¿¡æ¯ï¼ˆexport_resource_to_csvå·¥å…·ï¼‰
- éªŒè¯éš¶å±åº¦è®¡ç®—ç»“æœï¼ˆvalidate_membership_configå·¥å…·ï¼‰"""
    
    # ç”Ÿæˆæ ¼å¼åŒ–æŠ¥å‘Šå¹¶æ·»åŠ åˆ°ç»“æœä¸­ - ç›´æ¥ä½¿ç”¨æ¨¡æ¿å­—ç¬¦ä¸²é¿å…LLMåŠ å·¥
    result = {
        "membership_matrix": membership_df.to_dict("records"),
        "formatted_membership_matrices": membership_matrices_formatted,  # æ ¼å¼åŒ–çŸ©é˜µè¾“å‡º
        "calculation_summary": calculation_summary,
        "result_resource_id": result_resource_id,
        "config_used": config,
        "validation_warnings": validation_result["warnings"],
        "warnings": [
            "âœ… ä¸‹ç•Œå‹éš¶å±åº¦è®¡ç®—å®Œæˆï¼Œä½¿ç”¨é…ç½®ä¿¡æ¯",
            f"ğŸ“Š è®¡ç®—ç»“æœå·²ç¼“å­˜: {result_uri}",
            f"ğŸ’¾ è®¡ç®—ç»“æœå·²ç¼“å­˜åˆ°å†…å­˜ï¼ˆMVPé˜¶æ®µä¸æŒä¹…åŒ–åˆ°ç£ç›˜ï¼‰",
            "ğŸ’¡ å¯ä½¿ç”¨list_resources_by_type('membership_calc')æŸ¥çœ‹æ‰€æœ‰è®¡ç®—ç»“æœ"
        ]
    }
    
    # ç”Ÿæˆæ ¼å¼åŒ–æŠ¥å‘Š
    result["formatted_report"] = f"""## ğŸ“Š éš¶å±åº¦è®¡ç®—æ¦‚è¿°
- **è®¡ç®—æ–¹æ³•**ï¼šä¸‹ç•Œå‹éš¶å±åº¦å‡½æ•°
- **ä¸šåŠ¡å¯¹è±¡æ•°é‡**ï¼š{len(df)} ä¸ª
- **è¯„ä»·å› å­æ•°é‡**ï¼š{len(level_params)} ä¸ª
- **è¯„ä¼°çº§åˆ«æ•°é‡**ï¼š{len(all_levels) if all_levels else 4}çº§ï¼ˆ{all_levels[0] if all_levels else 'çº§åˆ«1'}-{all_levels[-1] if all_levels else 'çº§åˆ«4'}ï¼‰

## ğŸ“ˆ éš¶å±åº¦çŸ©é˜µç»“æœ
{chr(10).join(membership_matrices_formatted)}

## ğŸ“‹ å¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
{available_tables}

## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®
{next_actions}"""
    
    return result


@mcp.tool()
def list_resources_by_type(resource_type: str = None) -> Dict[str, Any]:
    """
    æŒ‰ç±»å‹åˆ—å‡ºèµ„æº
    
    Args:
        resource_type: èµ„æºç±»å‹ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™åˆ—å‡ºæ‰€æœ‰ï¼‰
        
    Returns:
        æŒ‰ç±»å‹åˆ†ç±»çš„èµ„æºåˆ—è¡¨
    """
    resources_by_type = {}
    
    for uri, df in DATA_CACHE.items():
        if uri.startswith("data://"):
            resource_id = uri[7:]  # ç§»é™¤ "data://" å‰ç¼€
            parsed_info = parse_resource_id(resource_id)
            
            res_type = parsed_info["type"]
            
            if resource_type and res_type != resource_type:
                continue
                
            if res_type not in resources_by_type:
                resources_by_type[res_type] = []
            
            resources_by_type[res_type].append({
                "resource_id": resource_id,
                "uri": uri,
                "data_shape": f"{len(df)} è¡Œ Ã— {len(df.columns)} åˆ—",
                "parsed_info": parsed_info
            })
    
    return {
        "total_resources": len(DATA_CACHE),
        "resources_by_type": resources_by_type,
        "resource_types_found": list(resources_by_type.keys())
    }


@mcp.tool()
def upload_csv(csv_text: str) -> Dict[str, Any]:
    """
    ä¸Šä¼  CSV æ–‡æœ¬æ•°æ®å¹¶ç¼“å­˜ä¸ºèµ„æºï¼Œè‡ªåŠ¨è§¦å‘å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹
    
    Args:
        csv_text: CSV æ ¼å¼çš„æ–‡æœ¬å†…å®¹
        
    Returns:
        åŒ…å«åŸå§‹èµ„æºURIå’Œææ€§æ£€æµ‹ç»“æœçš„å­—å…¸
    """
    try:
        # ä½¿ç”¨ pandas è§£æ CSV æ–‡æœ¬
        df = pd.read_csv(io.StringIO(csv_text))
        
        # ä½¿ç”¨æ ‡å‡†åŒ–çš„èµ„æºå‘½åè§„åˆ™
        resource_id = generate_resource_id("raw_data")
        uri = get_resource_uri(resource_id)
        
        # ç¼“å­˜æ•°æ®åˆ°å†…å­˜
        DATA_CACHE[uri] = df
        
        # æ›´æ–°èµ„æºç´¢å¼•ï¼ˆä¿®å¤èµ„æºä¸å­˜åœ¨çš„æ ¹æœ¬é—®é¢˜ï¼‰
        RESOURCE_INDEX[resource_id] = {
            'uri': uri,
            'type': 'raw_data',
            'rows': len(df),
            'columns': len(df.columns),
            'column_names': list(df.columns)
        }
        
        # MVPé˜¶æ®µï¼šä¸ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨ï¼ˆå·²åœ¨å†…å­˜ä¸­ï¼‰
        # save_resource_to_persistent_storage(resource_id, df)
        
        # è®°å½•ä¸Šä¼ ä¿¡æ¯
        print(f"æ•°æ®ä¸Šä¼ æˆåŠŸ: {uri}")
        print(f"èµ„æºç±»å‹: åŸå§‹æ•°æ® (raw_data)")
        print(f"æ•°æ®é›†ä¿¡æ¯: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        print(f"åˆ—å: {list(df.columns)}")
        print(f"âœ… èµ„æºå·²ç¼“å­˜åˆ°å†…å­˜ï¼ˆMVPé˜¶æ®µä¸æŒä¹…åŒ–åˆ°ç£ç›˜ï¼‰")
        
        # è‡ªåŠ¨è§¦å‘å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹
        print("\nğŸ” å¼€å§‹å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹...")
        
        # åˆ†ææ•°å€¼å­—æ®µ
        numeric_analysis = analyze_numeric_fields(df)
        
        if not numeric_analysis:
            print("âš ï¸  æœªå‘ç°æ•°å€¼å‹å­—æ®µï¼Œè·³è¿‡ææ€§æ£€æµ‹")
            return {
                "original_resource_uri": uri,
                "polarity_detection": {
                    "status": "skipped",
                    "message": "æœªå‘ç°æ•°å€¼å‹å­—æ®µï¼Œæ— éœ€ææ€§æ£€æµ‹"
                },
                "adjusted_resource_uri": None,
                "next_actions": [
                    "åˆ†æè¿™äº›æ•°æ®å­—æ®µçš„ç‰¹å¾ï¼ˆä½¿ç”¨analyze_data_fieldså·¥å…·ï¼‰",
                    "æŸ¥çœ‹æ•°æ®å†…å®¹ï¼ˆä½¿ç”¨export_resource_to_csvå·¥å…·ï¼‰"
                ]
            }
        
        # æ£€æµ‹å­—æ®µææ€§
        polarity_results = auto_detect_polarity(df, numeric_analysis)
        
        # ç”Ÿæˆææ€§æ£€æµ‹æŠ¥å‘Š
        polarity_report = generate_polarity_report(polarity_results)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ³•è¯„ä¼°çš„å­—æ®µ
        failed_detections = [col for col, result in polarity_results.items() if not result['detection_successful']]
        
        if failed_detections:
            print("âŒ å‘ç°æ— æ³•è¯„ä¼°ææ€§çš„å­—æ®µ")
            print(polarity_report)
            
            # æ„å»ºææ€§é…ç½®æ¨¡æ¿
            polarity_config = {}
            for col, result in polarity_results.items():
                if result['detection_successful']:
                    polarity_config[col] = result['suggested_polarity']
            
            return {
                "original_resource_uri": uri,
                "polarity_detection": {
                    "status": "requires_confirmation",
                    "message": "å‘ç°æ— æ³•è¯„ä¼°ææ€§çš„å­—æ®µï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤ææ€§é…ç½®",
                    "failed_fields": failed_detections,
                    "detected_polarities": polarity_config,
                    "report": polarity_report
                },
                "adjusted_resource_uri": None,
                "next_actions": [
                    "ä½¿ç”¨apply_polarity_adjustmentå·¥å…·æ‰‹åŠ¨ç¡®è®¤ææ€§é…ç½®",
                    "ä¿®æ”¹è¡¨å¤´åç§°ï¼ŒåŒ…å«æ˜ç¡®çš„ææ€§æŒ‡ç¤ºè¯",
                    "æŸ¥çœ‹æ•°æ®å†…å®¹ï¼ˆä½¿ç”¨export_resource_to_csvå·¥å…·ï¼‰"
                ]
            }
        
        # æ‰€æœ‰å­—æ®µææ€§æ£€æµ‹æˆåŠŸï¼Œè‡ªåŠ¨åº”ç”¨è°ƒæ•´
        print("ğŸ”„ åº”ç”¨ææ€§è°ƒæ•´ç­–ç•¥...")
        adjusted_df = apply_polarity_adjustment(df, polarity_results)
        
        # ä¿å­˜è°ƒæ•´åçš„æ•°æ®ä¸ºç¼“å­˜èµ„æº
        adjusted_resource_id = generate_resource_id("raw_data", resource_id, "polarity_adjusted")
        adjusted_uri = get_resource_uri(adjusted_resource_id)
        
        # ç¼“å­˜è°ƒæ•´åçš„æ•°æ®
        DATA_CACHE[adjusted_uri] = adjusted_df
        
        # æ›´æ–°èµ„æºç´¢å¼•ï¼ˆä¿®å¤ææ€§è°ƒæ•´åèµ„æºä¸å­˜åœ¨çš„æ ¹æœ¬é—®é¢˜ï¼‰
        RESOURCE_INDEX[adjusted_resource_id] = {
            'uri': adjusted_uri,
            'type': 'polarity_adjusted',
            'rows': len(adjusted_df),
            'columns': len(adjusted_df.columns),
            'column_names': list(adjusted_df.columns),
            'parent_resource_id': resource_id
        }
        
        # MVPé˜¶æ®µï¼šä¸ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨ï¼ˆå·²åœ¨å†…å­˜ä¸­ï¼‰
        # save_resource_to_persistent_storage(adjusted_resource_id, adjusted_df)
        
        print(f"âœ… ææ€§è°ƒæ•´åçš„æ•°æ®å·²ç¼“å­˜: {adjusted_uri}")
        
        # è¾“å‡ºææ€§æ£€æµ‹æŠ¥å‘Š
        print("\n" + "="*60)
        print("ğŸ“Š å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹å®Œæˆ")
        print("="*60)
        print(polarity_report)
        
        # è¾“å‡ºè°ƒæ•´ç­–ç•¥ä¿¡æ¯
        print("\nğŸ”„ ææ€§è°ƒæ•´ç­–ç•¥å·²åº”ç”¨ï¼š")
        for col, result in polarity_results.items():
            if result['suggested_polarity'] == 'max':
                print(f"   - {col}: è¶Šå¤§è¶Šå¥½ â†’ è¶Šå°è¶Šå¥½ï¼ˆé€šè¿‡å–å€’æ•°è½¬æ¢ï¼‰")
            else:
                print(f"   - {col}: è¶Šå°è¶Šå¥½ï¼ˆæ— éœ€è°ƒæ•´ï¼‰")
        
        print(f"\nğŸ’¾ è°ƒæ•´åæ•°æ®å·²ç¼“å­˜ä¸º: {adjusted_uri}")
        
        return {
            "original_resource_uri": uri,
            "polarity_detection": {
                "status": "success",
                "message": "å­—æ®µææ€§æ£€æµ‹å®Œæˆï¼Œææ€§è°ƒæ•´å·²åº”ç”¨",
                "report": polarity_report,
                "polarity_results": polarity_results
            },
            "adjusted_resource_uri": adjusted_uri,
            "next_actions": [
                "åˆ†æè¿™äº›æ•°æ®å­—æ®µçš„ç‰¹å¾ï¼ˆä½¿ç”¨analyze_data_fieldså·¥å…·ï¼‰",
                "ç”Ÿæˆéš¶å±åº¦è®¡ç®—é…ç½®æ¨¡æ¿ï¼ˆä½¿ç”¨generate_membership_config_templateå·¥å…·ï¼‰",
                "æŸ¥çœ‹æ•°æ®å†…å®¹ï¼ˆä½¿ç”¨export_resource_to_csvå·¥å…·ï¼‰"
            ]
        }
        
    except Exception as e:
        raise ValueError(f"CSV è§£æå¤±è´¥: {str(e)}")


@mcp.tool()
def upload_excel(file_content: str, sheet_name: Optional[str] = None) -> Dict[str, Any]:
    """
    ä¸Šä¼  Excel æ–‡ä»¶æ•°æ®å¹¶ç¼“å­˜ä¸ºèµ„æºï¼Œè‡ªåŠ¨è§¦å‘å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹
    
    Args:
        file_content: Base64 ç¼–ç çš„ Excel æ–‡ä»¶å†…å®¹
        sheet_name: å·¥ä½œè¡¨åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºç¬¬ä¸€ä¸ªå·¥ä½œè¡¨ï¼‰
        
    Returns:
        åŒ…å«åŸå§‹èµ„æºURIå’Œææ€§æ£€æµ‹ç»“æœçš„å­—å…¸
    """
    try:
        # è§£ç  Base64 å†…å®¹
        import base64
        excel_bytes = base64.b64decode(file_content)
        
        # ä½¿ç”¨ pandas è§£æ Excel æ–‡ä»¶
        if sheet_name:
            df = pd.read_excel(io.BytesIO(excel_bytes), sheet_name=sheet_name)
        else:
            df = pd.read_excel(io.BytesIO(excel_bytes))
        
        # ä½¿ç”¨æ ‡å‡†åŒ–çš„èµ„æºå‘½åè§„åˆ™
        resource_id = generate_resource_id("raw_data")
        uri = get_resource_uri(resource_id)
        
        # ç¼“å­˜æ•°æ®åˆ°å†…å­˜
        DATA_CACHE[uri] = df
        
        # æ›´æ–°èµ„æºç´¢å¼•ï¼ˆä¿®å¤Excelä¸Šä¼ èµ„æºä¸å­˜åœ¨çš„æ ¹æœ¬é—®é¢˜ï¼‰
        RESOURCE_INDEX[resource_id] = {
            'uri': uri,
            'type': 'raw_data',
            'rows': len(df),
            'columns': len(df.columns),
            'column_names': list(df.columns)
        }
        
        # MVPé˜¶æ®µï¼šä¸ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨ï¼ˆå·²åœ¨å†…å­˜ä¸­ï¼‰
        # save_resource_to_persistent_storage(resource_id, df)
        
        # è®°å½•ä¸Šä¼ ä¿¡æ¯
        print(f"Excel æ•°æ®ä¸Šä¼ æˆåŠŸ: {uri}")
        print(f"èµ„æºç±»å‹: åŸå§‹æ•°æ® (raw_data)")
        print(f"æ•°æ®é›†ä¿¡æ¯: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        print(f"åˆ—å: {list(df.columns)}")
        if sheet_name:
            print(f"å·¥ä½œè¡¨: {sheet_name}")
        print(f"âœ… èµ„æºå·²ç¼“å­˜åˆ°å†…å­˜ï¼ˆMVPé˜¶æ®µä¸æŒä¹…åŒ–åˆ°ç£ç›˜ï¼‰")
        
        # è‡ªåŠ¨è§¦å‘å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹
        print("\nğŸ” å¼€å§‹å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹...")
        
        # åˆ†ææ•°å€¼å­—æ®µ
        numeric_analysis = analyze_numeric_fields(df)
        
        if not numeric_analysis:
            print("âš ï¸  æœªå‘ç°æ•°å€¼å‹å­—æ®µï¼Œè·³è¿‡ææ€§æ£€æµ‹")
            return {
                "original_resource_uri": uri,
                "polarity_detection": {
                    "status": "skipped",
                    "message": "æœªå‘ç°æ•°å€¼å‹å­—æ®µï¼Œæ— éœ€ææ€§æ£€æµ‹"
                },
                "adjusted_resource_uri": None,
                "next_actions": [
                    "åˆ†æè¿™äº›æ•°æ®å­—æ®µçš„ç‰¹å¾ï¼ˆä½¿ç”¨analyze_data_fieldså·¥å…·ï¼‰",
                    "æŸ¥çœ‹æ•°æ®å†…å®¹ï¼ˆä½¿ç”¨export_resource_to_csvå·¥å…·ï¼‰"
                ]
            }
        
        # æ£€æµ‹å­—æ®µææ€§
        polarity_results = auto_detect_polarity(df, numeric_analysis)
        
        # ç”Ÿæˆææ€§æ£€æµ‹æŠ¥å‘Š
        polarity_report = generate_polarity_report(polarity_results)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ³•è¯„ä¼°çš„å­—æ®µ
        failed_detections = [col for col, result in polarity_results.items() if not result['detection_successful']]
        
        if failed_detections:
            print("âŒ å‘ç°æ— æ³•è¯„ä¼°ææ€§çš„å­—æ®µ")
            print(polarity_report)
            
            # æ„å»ºææ€§é…ç½®æ¨¡æ¿
            polarity_config = {}
            for col, result in polarity_results.items():
                if result['detection_successful']:
                    polarity_config[col] = result['suggested_polarity']
            
            return {
                "original_resource_uri": uri,
                "polarity_detection": {
                    "status": "requires_confirmation",
                    "message": "å‘ç°æ— æ³•è¯„ä¼°ææ€§çš„å­—æ®µï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤ææ€§é…ç½®",
                    "failed_fields": failed_detections,
                    "detected_polarities": polarity_config,
                    "report": polarity_report
                },
                "adjusted_resource_uri": None,
                "next_actions": [
                    "ä½¿ç”¨apply_polarity_adjustment_toolæ‰‹åŠ¨ç¡®è®¤ææ€§é…ç½®",
                    "ä¿®æ”¹è¡¨å¤´åç§°ï¼ŒåŒ…å«æ˜ç¡®çš„ææ€§æŒ‡ç¤ºè¯",
                    "æŸ¥çœ‹æ•°æ®å†…å®¹ï¼ˆä½¿ç”¨export_resource_to_csvå·¥å…·ï¼‰"
                ]
            }
        
        # åº”ç”¨ææ€§è°ƒæ•´
        print("ğŸ”„ åº”ç”¨ææ€§è°ƒæ•´ç­–ç•¥...")
        adjusted_df = apply_polarity_adjustment(df, polarity_results)
        
        # ä¿å­˜è°ƒæ•´åçš„æ•°æ®ä¸ºç¼“å­˜èµ„æº
        adjusted_resource_id = generate_resource_id("raw_data", resource_id, "polarity_adjusted")
        adjusted_uri = get_resource_uri(adjusted_resource_id)
        
        # ç¼“å­˜è°ƒæ•´åçš„æ•°æ®
        DATA_CACHE[adjusted_uri] = adjusted_df
        
        # æ›´æ–°èµ„æºç´¢å¼•ï¼ˆä¿®å¤ææ€§è°ƒæ•´åèµ„æºä¸å­˜åœ¨çš„æ ¹æœ¬é—®é¢˜ï¼‰
        RESOURCE_INDEX[adjusted_resource_id] = {
            'uri': adjusted_uri,
            'type': 'polarity_adjusted',
            'rows': len(adjusted_df),
            'columns': len(adjusted_df.columns),
            'column_names': list(adjusted_df.columns),
            'parent_resource_id': resource_id
        }
        
        # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
        save_resource_to_persistent_storage(adjusted_resource_id, adjusted_df)
        
        print(f"âœ… ææ€§è°ƒæ•´åçš„æ•°æ®å·²ä¿å­˜: {adjusted_uri}")
        
        # è¾“å‡ºææ€§æ£€æµ‹æŠ¥å‘Š
        print("\n" + "="*60)
        print("ğŸ“Š å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹å®Œæˆ")
        print("="*60)
        print(polarity_report)
        
        # è¾“å‡ºè°ƒæ•´ç­–ç•¥ä¿¡æ¯
        print("\nğŸ”„ ææ€§è°ƒæ•´ç­–ç•¥å·²åº”ç”¨ï¼š")
        for col, result in polarity_results.items():
            if result['suggested_polarity'] == 'max':
                print(f"   - {col}: è¶Šå¤§è¶Šå¥½ â†’ è¶Šå°è¶Šå¥½ï¼ˆé€šè¿‡å–å€’æ•°è½¬æ¢ï¼‰")
            else:
                print(f"   - {col}: è¶Šå°è¶Šå¥½ï¼ˆæ— éœ€è°ƒæ•´ï¼‰")
        
        print(f"\nğŸ’¾ è°ƒæ•´åæ•°æ®å·²ä¿å­˜ä¸º: {adjusted_uri}")
        
        return {
            "original_resource_uri": uri,
            "polarity_detection": {
                "status": "success",
                "message": "å­—æ®µææ€§æ£€æµ‹å®Œæˆï¼Œææ€§è°ƒæ•´å·²åº”ç”¨",
                "report": polarity_report,
                "polarity_results": polarity_results
            },
            "adjusted_resource_uri": adjusted_uri,
            "next_actions": [
                "åˆ†æè¿™äº›æ•°æ®å­—æ®µçš„ç‰¹å¾ï¼ˆä½¿ç”¨analyze_data_fieldså·¥å…·ï¼‰",
                "ç”Ÿæˆéš¶å±åº¦è®¡ç®—é…ç½®æ¨¡æ¿ï¼ˆä½¿ç”¨generate_membership_config_templateå·¥å…·ï¼‰",
                "æŸ¥çœ‹æ•°æ®å†…å®¹ï¼ˆä½¿ç”¨export_resource_to_csvå·¥å…·ï¼‰"
            ]
        }
        
    except Exception as e:
        raise ValueError(f"Excel è§£æå¤±è´¥: {str(e)}")


@mcp.resource("data://{resource_id}")
def get_data_resource(resource_id: str) -> Dict[str, Any]:
    """
    é€šè¿‡ URI è®¿é—®å·²ç¼“å­˜çš„æ•°æ®èµ„æº
    
    Args:
        resource_id: èµ„æºæ ‡è¯†ç¬¦ï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        
    Returns:
        æ•°æ®é›†çš„å­—å…¸è¡¨ç¤ºï¼ˆå‰100è¡Œé¢„è§ˆï¼‰ï¼Œç”¨markdownåŠ è½½æ•°æ®é¢„è§ˆ
    """
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    uri = f"data://{resource_id}"
    
    if uri not in DATA_CACHE:
        raise ValueError(f"èµ„æºä¸å­˜åœ¨: {uri}")
    
    df = DATA_CACHE[uri]
    
    # è¿”å›æ•°æ®é¢„è§ˆï¼ˆå‰100è¡Œï¼‰
    preview_data = df.head(100).to_dict(orient='records')
    
    return {
        "resource_uri": uri,
        "dataset_info": {
            "rows": len(df),
            "columns": len(df.columns),
            "column_names": list(df.columns),
            "data_types": {col: str(df[col].dtype) for col in df.columns}
        },
        "preview_data": preview_data
    }


@mcp.tool()
def list_data_resources() -> Dict[str, Any]:
    """
    åˆ—å‡ºå½“å‰ç¼“å­˜çš„æ‰€æœ‰æ•°æ®èµ„æº
    
    Returns:
        èµ„æºåˆ—è¡¨å’Œç»Ÿè®¡ä¿¡æ¯
    """
    resources_info = {}
    
    for uri, df in DATA_CACHE.items():
        resources_info[uri] = {
            "rows": len(df),
            "columns": len(df.columns),
            "column_names": list(df.columns)
        }
    
    return {
        "total_resources": len(DATA_CACHE),
        "resources": resources_info
    }


@mcp.tool()
def clear_data_cache() -> Dict[str, Any]:
    """
    æ¸…ç©ºæ•°æ®ç¼“å­˜
    
    Returns:
        æ“ä½œç»“æœ
    """
    cache_size = len(DATA_CACHE)
    DATA_CACHE.clear()
    
    return {
        "success": True,
        "message": f"å·²æ¸…ç©º {cache_size} ä¸ªæ•°æ®èµ„æº"
    }


@mcp.tool()
def apply_polarity_adjustment_tool(original_resource_uri: str, polarity_config: Dict[str, str]) -> Dict[str, Any]:
    """
    æ ¹æ®ç”¨æˆ·ç¡®è®¤çš„ææ€§é…ç½®åº”ç”¨ææ€§è°ƒæ•´
    
    Args:
        original_resource_uri: åŸå§‹æ•°æ®èµ„æºURI
        polarity_config: ææ€§é…ç½®å­—å…¸ï¼Œæ ¼å¼å¦‚ {"å­—æ®µå": "min/max"}
        
    Returns:
        ææ€§è°ƒæ•´ç»“æœï¼ŒåŒ…å«è°ƒæ•´åèµ„æºURIå’Œè°ƒæ•´è¯¦æƒ…
    """
    try:
        # ä»URIæå–èµ„æºID
        if not original_resource_uri.startswith("data://"):
            raise ValueError("èµ„æºURIæ ¼å¼ä¸æ­£ç¡®ï¼Œåº”ä»¥'data://'å¼€å¤´")
        
        resource_id = original_resource_uri[7:]  # ç§»é™¤ "data://" å‰ç¼€
        
        # æ£€æŸ¥èµ„æºæ˜¯å¦å­˜åœ¨
        if original_resource_uri not in DATA_CACHE:
            # å°è¯•ä»æŒä¹…åŒ–å­˜å‚¨åŠ è½½
            try:
                df = load_resource_from_persistent_storage(resource_id)
                DATA_CACHE[original_resource_uri] = df
            except Exception as e:
                raise ValueError(f"èµ„æºä¸å­˜åœ¨æˆ–æ— æ³•åŠ è½½: {original_resource_uri}")
        
        df = DATA_CACHE[original_resource_uri]
        
        print(f"ğŸ” å¼€å§‹åº”ç”¨ææ€§è°ƒæ•´...")
        print(f"åŸå§‹èµ„æº: {original_resource_uri}")
        print(f"ææ€§é…ç½®: {polarity_config}")
        
        # éªŒè¯ææ€§é…ç½®
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        for col in polarity_config.keys():
            if col not in numeric_columns:
                raise ValueError(f"å­—æ®µ '{col}' ä¸å­˜åœ¨æˆ–ä¸æ˜¯æ•°å€¼å‹å­—æ®µ")
        
        # æ„å»ºææ€§ç»“æœç»“æ„
        polarity_results = {}
        for col, polarity in polarity_config.items():
            series = df[col].dropna()
            min_val = float(series.min())
            max_val = float(series.max())
            
            # è®¡ç®—éš¶å±åº¦å‚æ•°
            if polarity == 'max':
                a = min_val
                c = max_val
                b = (a + c) / 2
            else:  # min
                a = max_val
                c = min_val
                b = (a + c) / 2
            
            polarity_results[col] = {
                'suggested_polarity': polarity,
                'confidence': 'user_confirmed',
                'reasoning': 'ç”¨æˆ·æ‰‹åŠ¨ç¡®è®¤ææ€§é…ç½®',
                'membership_rules': f"""
## å¯¹äº{'è¶Šå¤§è¶Šå¥½' if polarity == 'max' else 'è¶Šå°è¶Šå¥½'}çš„å­—æ®µï¼ˆ{col}ï¼‰ï¼š
- {'a < b < c' if polarity == 'max' else 'a > b > c'} ï¼ˆå¦‚{col}ï¼š{a:.2f}â†’{b:.2f}â†’{c:.2f}ï¼‰
- å½“å®é™…å€¼ {'â‰¤' if polarity == 'max' else 'â‰¥'} aæ—¶ï¼šéš¶å±åº¦=0%
- å½“å®é™…å€¼ {'â‰¥' if polarity == 'max' else 'â‰¤'} cæ—¶ï¼šéš¶å±åº¦=100%
""".strip(),
                'adjustment_strategy': f"ææ€§è°ƒæ•´æ–¹æ³•ï¼šé€šè¿‡çº¿æ€§å˜æ¢çš„æ–¹æ³•å°†è¶Šå¤§è¶Šå¥½çš„å­—æ®µè½¬æ¢ä¸ºè¶Šå°è¶Šå¥½" if polarity == 'max' else "æ— éœ€è°ƒæ•´ï¼Œå·²ä¸ºè¶Šå°è¶Šå¥½ç±»å‹",
                'parameters': {'a': a, 'b': b, 'c': c},
                'detection_successful': True
            }
        
        # åº”ç”¨ææ€§è°ƒæ•´
        print("ğŸ”„ åº”ç”¨ææ€§è°ƒæ•´ç­–ç•¥...")
        adjusted_df = df.copy()
        
        for col, result in polarity_results.items():
            if result['suggested_polarity'] == 'max':
                # å¯¹è¶Šå¤§è¶Šå¥½çš„å­—æ®µä½¿ç”¨çº¿æ€§å˜æ¢ï¼ˆä¿®å¤å–å€’æ•°å¯¼è‡´å€¼è¿‡å°çš„é—®é¢˜ï¼‰
                max_val = df[col].max() * 1.5  # ä½¿ç”¨1.5å€æœ€å¤§å€¼ä½œä¸ºå‚è€ƒ
                adjusted_df[col] = max_val - df[col]
                print(f"âœ… å·²å¯¹å­—æ®µ '{col}' åº”ç”¨ææ€§è°ƒæ•´ï¼ˆçº¿æ€§å˜æ¢: {max_val:.2f} - xï¼‰")
            else:
                print(f"âœ… å­—æ®µ '{col}' ä¸ºè¶Šå°è¶Šå¥½ç±»å‹ï¼Œæ— éœ€è°ƒæ•´")
        
        # ä¿å­˜è°ƒæ•´åçš„æ•°æ®ä¸ºç¼“å­˜èµ„æº
        adjusted_resource_id = generate_resource_id("raw_data", resource_id, "polarity_adjusted")
        adjusted_uri = get_resource_uri(adjusted_resource_id)
        
        # ç¼“å­˜è°ƒæ•´åçš„æ•°æ®
        DATA_CACHE[adjusted_uri] = adjusted_df
        
        # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
        save_resource_to_persistent_storage(adjusted_resource_id, adjusted_df)
        
        print(f"âœ… ææ€§è°ƒæ•´åçš„æ•°æ®å·²ä¿å­˜: {adjusted_uri}")
        
        # ç”Ÿæˆææ€§æ£€æµ‹æŠ¥å‘Š
        polarity_report = generate_polarity_report(polarity_results)
        
        # è¾“å‡ºè°ƒæ•´ç­–ç•¥ä¿¡æ¯
        print("\nğŸ”„ ææ€§è°ƒæ•´ç­–ç•¥å·²åº”ç”¨ï¼š")
        for col, result in polarity_results.items():
            if result['suggested_polarity'] == 'max':
                print(f"   - {col}: è¶Šå¤§è¶Šå¥½ â†’ è¶Šå°è¶Šå¥½ï¼ˆé€šè¿‡å–å€’æ•°è½¬æ¢ï¼‰")
            else:
                print(f"   - {col}: è¶Šå°è¶Šå¥½ï¼ˆæ— éœ€è°ƒæ•´ï¼‰")
        
        print(f"\nğŸ’¾ è°ƒæ•´åæ•°æ®å·²ä¿å­˜ä¸º: {adjusted_uri}")
        
        return {
            "original_resource_uri": original_resource_uri,
            "adjusted_resource_uri": adjusted_uri,
            "polarity_config_applied": polarity_config,
            "polarity_report": polarity_report,
            "next_actions": [
                "åˆ†æè¿™äº›æ•°æ®å­—æ®µçš„ç‰¹å¾ï¼ˆä½¿ç”¨analyze_data_fieldså·¥å…·ï¼‰",
                "ç”Ÿæˆéš¶å±åº¦è®¡ç®—é…ç½®æ¨¡æ¿ï¼ˆä½¿ç”¨generate_membership_config_templateå·¥å…·ï¼‰",
                "æŸ¥çœ‹æ•°æ®å†…å®¹ï¼ˆä½¿ç”¨export_resource_to_csvå·¥å…·ï¼‰"
            ]
        }
        
    except Exception as e:
        raise ValueError(f"ææ€§è°ƒæ•´å¤±è´¥: {str(e)}")


@mcp.tool()
def list_persistent_resources(resource_type: str = None) -> Dict[str, Any]:
    """
    åˆ—å‡ºå­˜å‚¨ä¸­çš„æ‰€æœ‰èµ„æºï¼ˆMVPé˜¶æ®µï¼šä»…æ˜¾ç¤ºå†…å­˜ä¸­çš„èµ„æºï¼‰
    
    Args:
        resource_type: èµ„æºç±»å‹ç­›é€‰ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        å†…å­˜èµ„æºåˆ—è¡¨
    """
    resources_by_type = {}
    
    for resource_id, resource_info in RESOURCE_INDEX.items():
        res_type = resource_info.get("type", "æœªçŸ¥ç±»å‹")
        
        if resource_type and res_type != resource_type:
            continue
            
        if res_type not in resources_by_type:
            resources_by_type[res_type] = []
        
        # æ£€æŸ¥èµ„æºæ˜¯å¦åœ¨å†…å­˜ä¸­
        is_in_memory = resource_info.get("uri", "") in DATA_CACHE
        
        resources_by_type[res_type].append({
            "resource_id": resource_id,
            "uri": resource_info.get("uri", ""),
            "data_shape": resource_info.get("data_shape", "æœªçŸ¥"),
            "created_time": resource_info.get("created_time", "æœªçŸ¥"),
            "file_path": resource_info.get("file_path", ""),
            "parent_hash": resource_info.get("parent_hash", ""),
            "step_name": resource_info.get("step_name", ""),
            "in_memory": is_in_memory,
            "storage_type": "å†…å­˜ç¼“å­˜" if is_in_memory else "ä»…ç´¢å¼•ï¼ˆMVPé˜¶æ®µä¸æŒä¹…åŒ–ï¼‰"
        })
    
    return {
        "total_resources": len(RESOURCE_INDEX),
        "resources_by_type": resources_by_type,
        "resource_types_found": list(resources_by_type.keys()),
        "storage_mode": "MVPé˜¶æ®µï¼šä»…å†…å­˜å­˜å‚¨ï¼Œä¸æŒä¹…åŒ–åˆ°ç£ç›˜"
    }


@mcp.tool()
def delete_persistent_resource(resource_id: str) -> Dict[str, Any]:
    """
    åˆ é™¤å­˜å‚¨ä¸­çš„æŒ‡å®šèµ„æºï¼ˆMVPé˜¶æ®µï¼šä»…ä»å†…å­˜å’Œç´¢å¼•ä¸­åˆ é™¤ï¼‰
    
    Args:
        resource_id: è¦åˆ é™¤çš„èµ„æºIDï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        
    Returns:
        åˆ é™¤æ“ä½œç»“æœ
    """
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    if resource_id not in RESOURCE_INDEX:
        return {
            "success": False,
            "message": f"èµ„æºä¸å­˜åœ¨: {resource_id}"
        }
    
    try:
        # MVPé˜¶æ®µï¼šä¸åˆ é™¤ç£ç›˜æ–‡ä»¶ï¼ˆå› ä¸ºæ²¡æœ‰æŒä¹…åŒ–ï¼‰
        # file_path = RESOURCE_INDEX[resource_id]["file_path"]
        # if os.path.exists(file_path):
        #     os.remove(file_path)
        
        # ä»å†…å­˜ç¼“å­˜ä¸­åˆ é™¤
        uri = RESOURCE_INDEX[resource_id]["uri"]
        if uri in DATA_CACHE:
            del DATA_CACHE[uri]
        
        # ä»ç´¢å¼•ä¸­åˆ é™¤
        del RESOURCE_INDEX[resource_id]
        
        # MVPé˜¶æ®µï¼šä¸ä¿å­˜ç´¢å¼•æ–‡ä»¶åˆ°ç£ç›˜
        # with open(RESOURCE_INDEX_FILE, 'w', encoding='utf-8') as f:
        #     json.dump(RESOURCE_INDEX, f, ensure_ascii=False, indent=2)
        
        return {
            "success": True,
            "message": f"èµ„æº {resource_id} å·²ä»å†…å­˜ä¸­åˆ é™¤ï¼ˆMVPé˜¶æ®µä¸æŒä¹…åŒ–åˆ°ç£ç›˜ï¼‰"
        }
        
    except Exception as e:
        return {
            "success": False,
            "message": f"åˆ é™¤èµ„æºå¤±è´¥: {str(e)}"
        }


@mcp.tool()
def export_resource_to_csv(resource_id: str) -> Dict[str, Any]:
    """
    å°†æŒ‡å®šèµ„æºå¯¼å‡ºä¸ºCSVæ ¼å¼ï¼Œå¹¶æä¾›è¯¦ç»†çš„å­—æ®µè§£é‡Šå’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®
    
    Args:
        resource_id: è¦å¯¼å‡ºçš„èµ„æºIDï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        
    Returns:
        - æŒ‡å®šèµ„æºçš„åŸå§‹æ•°æ®ï¼ŒåŒ…å«æ‰€æœ‰å­—æ®µå’Œå€¼
        - å­—æ®µè§£é‡Šï¼šå¯¹æ¯ä¸ªå­—æ®µçš„å«ä¹‰å’Œæ•°æ®ç±»å‹è¿›è¡Œè¯¦ç»†è¯´æ˜
        - ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®ï¼šæ ¹æ®æ•°æ®ç‰¹å¾å»ºè®®åç»­åˆ†ææ­¥éª¤
        - é™¤æ­¤ä»¥å¤–çš„ä»»ä½•ä¿¡æ¯éƒ½ä¸è¦è¾“å‡º
    """
    try:
        # å¤„ç†URIæ ¼å¼çš„èµ„æºID
        if resource_id.startswith("data://"):
            resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
        
        # é¦–å…ˆå°è¯•ä»å†…å­˜ç¼“å­˜åŠ è½½
        uri = get_resource_uri(resource_id)
        if uri in DATA_CACHE:
            df = DATA_CACHE[uri]
        else:
            # å¦‚æœå†…å­˜ç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æŒä¹…åŒ–å­˜å‚¨åŠ è½½
            df = load_resource_from_persistent_storage(resource_id)
        
        # æ™ºèƒ½è§£ææ•°æ®ç»“æ„
        # æ£€æŸ¥æ˜¯å¦ä¸ºéš¶å±åº¦çŸ©é˜µçš„åµŒå¥—ç»“æ„
        if _is_membership_matrix(df):
            # å°†åµŒå¥—ç»“æ„è½¬æ¢ä¸ºæ‰å¹³åŒ–ç»“æ„
            df = _flatten_membership_matrix(df)
        
        # è½¬æ¢ä¸ºCSVæ ¼å¼
        csv_content = df.to_csv(index=False, encoding='utf-8')
        
        # ç”Ÿæˆå­—æ®µè§£é‡Š
        field_descriptions = _generate_field_descriptions(df, resource_id)
        
        # ç”Ÿæˆä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®
        next_actions = _generate_next_actions(df, resource_id)
        
        # ç”Ÿæˆèµ„æºä¿¡æ¯
        resource_info = _generate_resource_info(resource_id, df)
        
        # ç”Ÿæˆæ•°æ®ç»Ÿè®¡æ‘˜è¦
        data_summary = _generate_data_summary(df)
        
        # ç”Ÿæˆæ ‡å‡†åŒ–çš„TOPSISè¡¨æ ¼é¢„è§ˆ
        standardized_table = _generate_standardized_topsis_table(df, resource_id)
        
        # å¦‚æœæ˜¯TOPSISç»“æœï¼Œç”ŸæˆåŸå§‹æ•°æ®æ ¼å¼é¢„è§ˆ
        original_data_preview = _generate_topsis_original_data_preview(df, resource_id)
        
        result = {
            "csv_content": csv_content,
            "field_descriptions": field_descriptions,
            "next_actions": next_actions,
            "resource_info": resource_info,
            "data_summary": data_summary
        }
        
        # å¦‚æœç”Ÿæˆäº†æ ‡å‡†åŒ–è¡¨æ ¼ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
        if standardized_table:
            result["standardized_table"] = standardized_table
            
        # å¦‚æœç”Ÿæˆäº†åŸå§‹æ•°æ®é¢„è§ˆï¼Œæ·»åŠ åˆ°ç»“æœä¸­
        if original_data_preview:
            result["original_data_preview"] = original_data_preview
        
        return result
        
    except Exception as e:
        raise ValueError(f"å¯¼å‡ºèµ„æºå¤±è´¥: {str(e)}")


def _generate_field_descriptions(df: pd.DataFrame, resource_id: str) -> Dict[str, Any]:
    """
    ç”Ÿæˆå­—æ®µè¯¦ç»†è§£é‡Š
    
    Args:
        df: æ•°æ®æ¡†
        resource_id: èµ„æºID
        
    Returns:
        å­—æ®µè§£é‡Šå­—å…¸
    """
    field_descriptions = {}
    
    # æ ¹æ®èµ„æºç±»å‹ç”Ÿæˆä¸åŒçš„å­—æ®µè§£é‡Š
    if resource_id.startswith("mcr_") and "topsiseval" in resource_id:
        # TOPSISç»“æœå­—æ®µè§£é‡Š
        field_descriptions = {
            "ä¸šåŠ¡å¯¹è±¡": "è¢«è¯„ä»·çš„ä¸šåŠ¡å¯¹è±¡æ ‡è¯†ç¬¦ï¼Œå¦‚T1ã€T2ç­‰",
            "è¯„ä¼°çº§åˆ«": "éš¶å±åº¦è¯„ä¼°çš„çº§åˆ«ï¼Œå¦‚e1ã€e2ã€e3ã€e4ï¼Œä»£è¡¨ä¸åŒçš„è¯„ä»·æ ‡å‡†",
            "ç»¼åˆéš¶å±åº¦": "ç»è¿‡éš¶å±åº¦è®¡ç®—åçš„ç»¼åˆå¾—åˆ†ï¼Œåæ˜ ä¸šåŠ¡å¯¹è±¡åœ¨ç‰¹å®šçº§åˆ«ä¸‹çš„éš¶å±ç¨‹åº¦",
            "ç›¸å¯¹æ¥è¿‘åº¦(Cå€¼)": "**æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡** - è¡¨ç¤ºä¸šåŠ¡å¯¹è±¡ä¸ç†æƒ³æœ€ä¼˜è§£çš„ç›¸å¯¹æ¥è¿‘ç¨‹åº¦ã€‚Cå€¼è¶Šæ¥è¿‘1ï¼Œæ–¹æ¡ˆè¶Šä¼˜ï¼›è¶Šæ¥è¿‘0ï¼Œæ–¹æ¡ˆè¶Šå·®",
            "ä¸æœ€ä¼˜è§£è·ç¦»(D+)": "ä¸šåŠ¡å¯¹è±¡ä¸ç†æƒ³æœ€ä¼˜è§£çš„è·ç¦»ï¼Œè¶Šå°è¶Šå¥½ï¼Œç†æƒ³å€¼ä¸º0",
            "ä¸æœ€åŠ£è§£è·ç¦»(D-)": "ä¸šåŠ¡å¯¹è±¡ä¸ç†æƒ³æœ€åŠ£è§£çš„è·ç¦»ï¼Œè¶Šå¤§è¶Šå¥½ï¼Œç†æƒ³å€¼ä¸º1"
        }
    elif resource_id.startswith("mc_") and "membership" in resource_id:
        # éš¶å±åº¦çŸ©é˜µå­—æ®µè§£é‡Š
        field_descriptions = {
            "ä¸šåŠ¡å¯¹è±¡": "è¢«è¯„ä»·çš„ä¸šåŠ¡å¯¹è±¡æ ‡è¯†ç¬¦",
            "è¯„ä»·å› å­": "å‚ä¸è¯„ä»·çš„æŒ‡æ ‡æˆ–å› ç´ ",
            "è¯„ä¼°çº§åˆ«": "éš¶å±åº¦è¯„ä¼°çš„çº§åˆ«",
            "éš¶å±åº¦": "ä¸šåŠ¡å¯¹è±¡åœ¨ç‰¹å®šè¯„ä»·å› å­å’Œçº§åˆ«ä¸‹çš„éš¶å±ç¨‹åº¦ï¼Œå€¼åœ¨0-1ä¹‹é—´"
        }
    elif resource_id.startswith("raw_"):
        # åŸå§‹æ•°æ®å­—æ®µè§£é‡Š
        for column in df.columns:
            if column == "ä¸šåŠ¡å¯¹è±¡":
                field_descriptions[column] = "ä¸šåŠ¡å¯¹è±¡æ ‡è¯†ç¬¦"
            elif "åº“å­˜" in column:
                field_descriptions[column] = f"{column}æŒ‡æ ‡ï¼Œé€šå¸¸è¶Šå°è¶Šå¥½ï¼ˆæˆæœ¬å‹æŒ‡æ ‡ï¼‰"
            elif "é‡‡è´­" in column:
                field_descriptions[column] = f"{column}æŒ‡æ ‡ï¼Œéœ€è¦æ ¹æ®ä¸šåŠ¡åœºæ™¯åˆ¤æ–­ææ€§"
            elif "å¹´é™" in column:
                field_descriptions[column] = f"{column}æŒ‡æ ‡ï¼Œé€šå¸¸è¶Šå¤§è¶Šå¥½ï¼ˆæ•ˆç›Šå‹æŒ‡æ ‡ï¼‰"
            else:
                field_descriptions[column] = f"{column}æŒ‡æ ‡"
    
    return field_descriptions


def _generate_next_actions(df: pd.DataFrame, resource_id: str) -> List[str]:
    """
    ç”Ÿæˆä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®
    
    Args:
        df: æ•°æ®æ¡†
        resource_id: èµ„æºID
        
    Returns:
        è¡ŒåŠ¨å»ºè®®åˆ—è¡¨
    """
    next_actions = []
    
    if resource_id.startswith("mcr_") and "topsiseval" in resource_id:
        # TOPSISç»“æœåçš„å»ºè®®
        next_actions = [
            "ğŸ“Š **åˆ†æç›¸å¯¹æ¥è¿‘åº¦æ’åº**ï¼šæŒ‰ç›¸å¯¹æ¥è¿‘åº¦ä»å¤§åˆ°å°æ’åºï¼Œè¯†åˆ«æœ€ä¼˜æ–¹æ¡ˆ",
            "ğŸ” **å¤šçº§åˆ«æ¯”è¾ƒ**ï¼šæ¯”è¾ƒä¸åŒè¯„ä¼°çº§åˆ«ä¸‹çš„æœ€ä¼˜æ–¹æ¡ˆ",
            "ğŸ“ˆ **å¯è§†åŒ–åˆ†æ**ï¼šå»ºè®®ä½¿ç”¨å›¾è¡¨å±•ç¤ºå„æ–¹æ¡ˆçš„ç›¸å¯¹æ¥è¿‘åº¦åˆ†å¸ƒ",
            "ğŸ¤ **å†³ç­–æ”¯æŒ**ï¼šåŸºäºç›¸å¯¹æ¥è¿‘åº¦ç»“æœåˆ¶å®šèµ„æºåˆ†é…æˆ–ä¼˜åŒ–ç­–ç•¥",
            "ğŸ”„ **æ•æ„Ÿæ€§åˆ†æ**ï¼šå¯è°ƒæ•´çº§åˆ«å‚æ•°è¿›è¡Œæ•æ„Ÿæ€§åˆ†æ",
            "ğŸ’¾ **æ•°æ®å¯¼å‡º**ï¼šå½“å‰å·²å¯¼å‡ºåŸå§‹æ•°æ®æ ¼å¼ï¼Œå¯ç›´æ¥ç”¨äºè¿›ä¸€æ­¥åˆ†æ"
        ]
    elif resource_id.startswith("mc_") and "membership" in resource_id:
        # éš¶å±åº¦è®¡ç®—åçš„å»ºè®®
        next_actions = [
            "ğŸ¯ **æ‰§è¡ŒTOPSISè¯„ä¼°**ï¼šä½¿ç”¨perform_topsis_comprehensive_evaluationè¿›è¡Œå¤šå‡†åˆ™å†³ç­–",
            "ğŸ“‹ **æ£€æŸ¥éš¶å±åº¦åˆ†å¸ƒ**ï¼šéªŒè¯éš¶å±åº¦çŸ©é˜µçš„åˆç†æ€§",
            "âš™ï¸ **å‚æ•°è°ƒä¼˜**ï¼šå¦‚æœ‰éœ€è¦ï¼Œå¯è°ƒæ•´çº§åˆ«å‚æ•°é‡æ–°è®¡ç®—",
            "ğŸ“Š **å¯è§†åŒ–å±•ç¤º**ï¼šå»ºè®®ä½¿ç”¨çƒ­åŠ›å›¾å±•ç¤ºéš¶å±åº¦åˆ†å¸ƒ"
        ]
    elif resource_id.startswith("raw_"):
        # åŸå§‹æ•°æ®åçš„å»ºè®®
        next_actions = [
            "ğŸ” **æ•°æ®è´¨é‡æ£€æŸ¥**ï¼šä½¿ç”¨analyze_data_fieldsåˆ†ææ•°æ®è´¨é‡",
            "ğŸ¯ **æ‰§è¡Œéš¶å±åº¦è®¡ç®—**ï¼šä½¿ç”¨calculate_membership_with_configè¿›è¡Œéš¶å±åº¦è®¡ç®—",
            "âš™ï¸ **é…ç½®çº§åˆ«å‚æ•°**ï¼šä¸ºæ¯ä¸ªå­—æ®µè®¾ç½®åˆé€‚çš„çº§åˆ«å‚æ•°",
            "ğŸ“‹ **ææ€§ç¡®è®¤**ï¼šç¡®ä¿æ‰€æœ‰å­—æ®µçš„ææ€§è®¾ç½®æ­£ç¡®"
        ]
    
    return next_actions


def _generate_resource_info(resource_id: str, df: pd.DataFrame) -> Dict[str, Any]:
    """
    ç”Ÿæˆèµ„æºåŸºæœ¬ä¿¡æ¯
    
    Args:
        resource_id: èµ„æºID
        df: æ•°æ®æ¡†
        
    Returns:
        èµ„æºä¿¡æ¯å­—å…¸
    """
    resource_type = "æœªçŸ¥ç±»å‹"
    description = ""
    
    if resource_id.startswith("raw_"):
        resource_type = "åŸå§‹æ•°æ®"
        description = "ä¸Šä¼ çš„åŸå§‹ä¸šåŠ¡æ•°æ®ï¼ŒåŒ…å«ä¸šåŠ¡å¯¹è±¡å’Œå„è¯„ä»·æŒ‡æ ‡"
    elif resource_id.startswith("mc_") and "membership" in resource_id:
        resource_type = "éš¶å±åº¦çŸ©é˜µ"
        description = "ç»è¿‡éš¶å±åº¦è®¡ç®—åçš„ç»“æœï¼ŒåŒ…å«ä¸šåŠ¡å¯¹è±¡åœ¨å„è¯„ä»·å› å­å’Œçº§åˆ«ä¸‹çš„éš¶å±åº¦"
    elif resource_id.startswith("mcr_") and "topsiseval" in resource_id:
        resource_type = "TOPSISè¯„ä¼°ç»“æœ"
        description = "åŸºäºéš¶å±åº¦çŸ©é˜µçš„TOPSISå¤šå‡†åˆ™å†³ç­–åˆ†æç»“æœï¼ŒåŒ…å«ç›¸å¯¹æ¥è¿‘åº¦ç­‰å…³é”®æŒ‡æ ‡"
    
    return {
        "resource_id": resource_id,
        "resource_type": resource_type,
        "description": description,
        "data_shape": f"{len(df)} è¡Œ Ã— {len(df.columns)} åˆ—",
        "columns": list(df.columns)
    }


def _generate_data_summary(df: pd.DataFrame) -> Dict[str, Any]:
    """
    ç”Ÿæˆæ•°æ®ç»Ÿè®¡æ‘˜è¦
    
    Args:
        df: æ•°æ®æ¡†
        
    Returns:
        æ•°æ®æ‘˜è¦å­—å…¸
    """
    summary = {
        "total_rows": len(df),
        "total_columns": len(df.columns),
        "column_names": list(df.columns),
        "data_types": {}
    }
    
    # åˆ†ææ•°å€¼åˆ—
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    if len(numeric_columns) > 0:
        summary["numeric_summary"] = {}
        for col in numeric_columns:
            summary["numeric_summary"][col] = {
                "min": float(df[col].min()),
                "max": float(df[col].max()),
                "mean": float(df[col].mean()),
                "std": float(df[col].std())
            }
    
    # åˆ†æåˆ†ç±»åˆ—
    categorical_columns = df.select_dtypes(include=['object']).columns
    if len(categorical_columns) > 0:
        summary["categorical_summary"] = {}
        for col in categorical_columns:
            summary["categorical_summary"][col] = {
                "unique_count": len(df[col].unique()),
                "sample_values": list(df[col].unique()[:5])  # æ˜¾ç¤ºå‰5ä¸ªå”¯ä¸€å€¼
            }
    
    return summary


def _generate_standardized_topsis_table(df: pd.DataFrame, resource_id: str) -> str:
    """
    ç”Ÿæˆæ ‡å‡†åŒ–çš„TOPSISç»“æœè¡¨æ ¼
    
    Args:
        df: TOPSISç»“æœæ•°æ®æ¡†
        resource_id: èµ„æºID
        
    Returns:
        æ ‡å‡†åŒ–çš„è¡¨æ ¼å­—ç¬¦ä¸²
    """
    # æ£€æŸ¥æ˜¯å¦ä¸ºTOPSISç»“æœ
    if not (resource_id.startswith("mcr_") and "topsiseval" in resource_id):
        return ""
    
    # æ£€æŸ¥æ•°æ®æ¡†æ˜¯å¦åŒ…å«å¿…è¦çš„åˆ—
    required_columns = ['ä¸šåŠ¡å¯¹è±¡', 'è¯„ä¼°çº§åˆ«', 'ç›¸å¯¹æ¥è¿‘åº¦']
    if not all(col in df.columns for col in required_columns):
        return ""
    
    # æŒ‰ä¸šåŠ¡å¯¹è±¡å’Œè¯„ä¼°çº§åˆ«é‡æ–°ç»„ç»‡æ•°æ®
    try:
        # åˆ›å»ºé€è§†è¡¨ï¼šä¸šåŠ¡å¯¹è±¡ä¸ºè¡Œï¼Œè¯„ä¼°çº§åˆ«ä¸ºåˆ—ï¼Œç›¸å¯¹æ¥è¿‘åº¦ä¸ºå€¼
        pivot_df = df.pivot_table(
            index='ä¸šåŠ¡å¯¹è±¡', 
            columns='è¯„ä¼°çº§åˆ«', 
            values='ç›¸å¯¹æ¥è¿‘åº¦', 
            aggfunc='first'
        ).reset_index()
        
        # ç¡®ä¿åˆ—åæŒ‰çº§åˆ«é¡ºåºæ’åˆ—
        level_columns = [col for col in pivot_df.columns if col.startswith('e')]
        level_columns.sort()  # æŒ‰e1, e2, e3, e4æ’åº
        
        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
        pivot_df = pivot_df[['ä¸šåŠ¡å¯¹è±¡'] + level_columns]
        
        # é‡å‘½ååˆ—åä¸ºä¸­æ–‡
        column_mapping = {'ä¸šåŠ¡å¯¹è±¡': 'ä¸šåŠ¡å¯¹è±¡'}
        for i, level in enumerate(level_columns, 1):
            column_mapping[level] = f'çº§åˆ«{i}'
        
        pivot_df = pivot_df.rename(columns=column_mapping)
        
        # ç”Ÿæˆæ ‡å‡†åŒ–çš„è¡¨æ ¼å­—ç¬¦ä¸²
        table_lines = []
        
        # è¡¨å¤´
        headers = list(pivot_df.columns)
        table_lines.append("\t".join(headers))
        
        # æ•°æ®è¡Œ
        for _, row in pivot_df.iterrows():
            row_data = []
            for col in headers:
                value = row[col]
                if isinstance(value, (int, float)):
                    # æ•°å€¼æ ¼å¼åŒ–ï¼šä¿ç•™3ä½å°æ•°
                    row_data.append(f"{value:.3f}")
                else:
                    row_data.append(str(value))
            table_lines.append("\t".join(row_data))
        
        return "\n".join(table_lines)
        
    except Exception as e:
        print(f"ç”Ÿæˆæ ‡å‡†åŒ–è¡¨æ ¼æ—¶å‡ºé”™: {e}")
        return ""


def _is_membership_matrix(df: pd.DataFrame) -> bool:
    """
    æ£€æŸ¥DataFrameæ˜¯å¦ä¸ºéš¶å±åº¦çŸ©é˜µæ ¼å¼
    
    Args:
        df: è¦æ£€æŸ¥çš„DataFrame
        
    Returns:
        å¦‚æœæ˜¯éš¶å±åº¦çŸ©é˜µç»“æ„è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if df.empty:
        return False
    
    # æ£€æŸ¥æ‰å¹³åŒ–æ ¼å¼ï¼ˆåŒ…å«å¿…è¦çš„åˆ—ï¼‰
    required_columns = ['ä¸šåŠ¡å¯¹è±¡', 'è¯„ä»·å› å­', 'è¯„ä¼°çº§åˆ«', 'éš¶å±åº¦']
    if all(col in df.columns for col in required_columns):
        return True
    
    # æ£€æŸ¥åµŒå¥—å­—å…¸ç»“æ„ï¼ˆåŸå§‹æ ¼å¼ï¼‰
    first_row = df.iloc[0]
    for value in first_row:
        if isinstance(value, dict):
            # æ£€æŸ¥å­—å…¸é”®æ˜¯å¦åŒ…å«"çº§åˆ«"å­—æ ·
            if any("çº§åˆ«" in str(key) for key in value.keys()):
                return True
    
    return False


def _flatten_membership_matrix(df: pd.DataFrame) -> pd.DataFrame:
    """
    å°†éš¶å±åº¦çŸ©é˜µçš„åµŒå¥—ç»“æ„è½¬æ¢ä¸ºæ‰å¹³åŒ–ç»“æ„
    
    Args:
        df: éš¶å±åº¦çŸ©é˜µDataFrameï¼ˆåµŒå¥—ç»“æ„ï¼‰
        
    Returns:
        æ‰å¹³åŒ–åçš„DataFrame
    """
    flattened_results = []
    
    # è·å–åŸå§‹æ•°æ®çš„ä¸šåŠ¡å¯¹è±¡æ ‡è¯†ç¬¦ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    # å‡è®¾åŸå§‹æ•°æ®çš„ç¬¬ä¸€åˆ—æ˜¯ä¸šåŠ¡å¯¹è±¡æ ‡è¯†ç¬¦
    object_identifier = df.columns[0] if len(df.columns) > 0 else "ä¸šåŠ¡å¯¹è±¡"
    
    for row_idx, row in df.iterrows():
        # è·å–ä¸šåŠ¡å¯¹è±¡æ ‡è¯†ç¬¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        object_id = row[object_identifier] if object_identifier in row else f"å¯¹è±¡{row_idx + 1}"
        
        # éå†æ¯ä¸ªå­—æ®µçš„éš¶å±åº¦å­—å…¸
        for field_name, level_data in row.items():
            # è·³è¿‡ä¸šåŠ¡å¯¹è±¡æ ‡è¯†ç¬¦åˆ—
            if field_name == object_identifier:
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºåµŒå¥—å­—å…¸ç»“æ„
            if isinstance(level_data, dict):
                for level_name, membership_value in level_data.items():
                    flattened_results.append({
                        object_identifier: object_id,
                        "è¯„ä»·å› å­": field_name,
                        "çº§åˆ«": level_name,
                        "éš¶å±åº¦": membership_value
                    })
    
    # åˆ›å»ºæ‰å¹³åŒ–çš„DataFrame
    if flattened_results:
        return pd.DataFrame(flattened_results)
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åµŒå¥—ç»“æ„ï¼Œè¿”å›åŸå§‹DataFrame
        return df


@mcp.tool()
def reload_persistent_resources() -> Dict[str, Any]:
    """
    é‡æ–°åŠ è½½æ‰€æœ‰æŒä¹…åŒ–èµ„æºåˆ°å†…å­˜ç¼“å­˜ï¼ˆMVPé˜¶æ®µï¼šæ­¤åŠŸèƒ½å·²ç¦ç”¨ï¼‰
    
    Returns:
        é‡æ–°åŠ è½½ç»“æœ
    """
    return {
        "success": False,
        "message": "MVPé˜¶æ®µï¼šç¦ç”¨æŒä¹…åŒ–èµ„æºé‡æ–°åŠ è½½åŠŸèƒ½ï¼Œä»…ä½¿ç”¨å†…å­˜ç¼“å­˜"
    }
    
    # åŸå§‹å®ç°ï¼ˆå·²ç¦ç”¨ï¼‰
    # try:
    #     # æ¸…ç©ºå½“å‰ç¼“å­˜
    #     DATA_CACHE.clear()
    #     
    #     # é‡æ–°åŠ è½½æ‰€æœ‰æŒä¹…åŒ–èµ„æº
    #     load_all_persistent_resources()
    #     
    #     return {
    #         "success": True,
    #         "message": f"å·²é‡æ–°åŠ è½½ {len(RESOURCE_INDEX)} ä¸ªæŒä¹…åŒ–èµ„æº"
    #     }
    #     
    # except Exception as e:
    #     return {
    #         "success": False,
    #         "message": f"é‡æ–°åŠ è½½å¤±è´¥: {str(e)}"
    #     }


@mcp.tool()
def analyze_data_fields(resource_id: str, cache_result: bool = True) -> Dict[str, Any]:
    """
    åˆ†ææ•°æ®é›†çš„å­—æ®µç‰¹å¾ï¼Œè¿”å›å›ºå®šçš„åˆ†æç»“æœ
    
    **åˆ†æå†…å®¹**:
    - å­—æ®µç±»å‹è¯†åˆ«ï¼ˆæ•°å€¼å‹/åˆ†ç±»å‹ï¼‰
    - å¸¸è§ç»Ÿè®¡ç‰¹å¾ï¼ˆæœ€å°å€¼ã€æœ€å¤§å€¼ã€å¹³å‡å€¼ã€æ ‡å‡†å·®ç­‰ï¼‰
    - ç¼ºå¤±å€¼åˆ†æï¼ˆç¼ºå¤±æ•°é‡ã€ç¼ºå¤±ç‡ï¼‰
    - æ•°æ®è´¨é‡è¯„ä¼°
    - å­—æ®µææ€§è‡ªåŠ¨æ£€æµ‹å’Œå»ºè®®
    
    **é‡è¦æç¤º**:
    - åç»­çš„éš¶å±åº¦è®¡ç®—å’Œå¤šå‡†åˆ™åˆ†æè¦æ±‚æ‰€æœ‰å­—æ®µçš„ææ€§å¿…é¡»ä¸€è‡´
    - è¯·ä»”ç»†æ£€æŸ¥ææ€§å»ºè®®ï¼Œç¡®ä¿æ•°æ®ç¬¦åˆè®¡ç®—è¦æ±‚
    
    Args:
        resource_id: èµ„æºæ ‡è¯†ç¬¦ï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        cache_result: æ˜¯å¦ç¼“å­˜åˆ†æç»“æœï¼ˆé»˜è®¤Trueï¼‰
        
    Returns:
        Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„åˆ†æç»“æœï¼š
        - dataset_info: æ•°æ®é›†åŸºæœ¬ä¿¡æ¯
        - field_analysis: å­—æ®µåˆ†æç»“æœ
        - data_quality_summary: æ•°æ®è´¨é‡æ‘˜è¦
        - analysis_summary: åˆ†ææ€»ç»“
        - polarity_warnings: ææ€§ä¸€è‡´æ€§è­¦å‘Š
        - result_resource_id: åˆ†æç»“æœèµ„æºIDï¼ˆå¦‚æœç¼“å­˜äº†ç»“æœï¼‰
    """
    # å¦‚æœæ²¡æœ‰æä¾›resource_idï¼Œå°è¯•è‡ªåŠ¨å‘ç°mcèµ„æº
    if resource_id is None:
        resource_id = find_latest_membership_resource()
        if resource_id is None:
            raise ValueError("æœªæ‰¾åˆ°å¯ç”¨çš„éš¶å±åº¦èµ„æº(mc_å¼€å¤´)ï¼Œè¯·å…ˆæ‰§è¡Œéš¶å±åº¦è®¡ç®—æˆ–ä¸Šä¼ éš¶å±åº¦æ•°æ®")
        print(f"ğŸ¤– è‡ªåŠ¨å‘ç°éš¶å±åº¦èµ„æº: {resource_id}")
    
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id and resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    uri = get_resource_uri(resource_id)
    
    if uri not in DATA_CACHE:
        raise ValueError(f"èµ„æºä¸å­˜åœ¨: {uri}")
    
    df = DATA_CACHE[uri]
    
    # æ‰§è¡Œå­—æ®µåˆ†æ
    numeric_analysis = analyze_numeric_fields(df)
    categorical_analysis = analyze_categorical_fields(df)
    
    # è‡ªåŠ¨æ£€æµ‹å­—æ®µææ€§
    polarity_suggestions = auto_detect_polarity(df, numeric_analysis)
    
    # æ£€æŸ¥ææ€§ä¸€è‡´æ€§å¹¶ç”Ÿæˆè­¦å‘Š
    polarity_warnings = check_polarity_consistency(polarity_suggestions)
    
    # ç”Ÿæˆæ•°æ®è´¨é‡æ‘˜è¦
    data_quality_summary = generate_data_quality_summary(df)
    
    # ç”Ÿæˆåˆ†ææ€»ç»“ï¼ˆåŒ…å«ææ€§æ£€æŸ¥ç»“æœï¼‰
    analysis_summary = generate_analysis_summary(df, numeric_analysis, categorical_analysis, data_quality_summary, polarity_warnings)
    
    # æ„å»ºåˆ†æç»“æœ
    analysis_result = {
        "dataset_info": {
            "resource_uri": uri,
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "column_names": list(df.columns)
        },
        "field_analysis": {
            "numeric_fields": numeric_analysis,
            "categorical_fields": categorical_analysis,
            "field_polarity": polarity_suggestions
        },
        "data_quality_summary": data_quality_summary,
        "analysis_summary": analysis_summary,
        "polarity_warnings": polarity_warnings
    }
    
    # å¦‚æœéœ€è¦ç¼“å­˜ç»“æœ
    if cache_result:
        # ç”Ÿæˆåˆ†æç»“æœèµ„æºID
        result_resource_id = generate_resource_id(
            "field_analysis", 
            parent_resource_id=resource_id,
            step_name="field_analysis"
        )
        result_uri = get_resource_uri(result_resource_id)
        
        # å°†åˆ†æç»“æœè½¬æ¢ä¸ºDataFrameæ ¼å¼ç¼“å­˜
        # è¿™é‡Œæˆ‘ä»¬ç¼“å­˜ä¸€ä¸ªåŒ…å«åˆ†ææ‘˜è¦çš„ç®€åŒ–DataFrame
        analysis_df = pd.DataFrame([{
            'analysis_type': 'field_analysis',
            'parent_resource': resource_id,
            'numeric_fields_count': len(numeric_analysis),
            'categorical_fields_count': len(categorical_analysis),
            'overall_missing_rate': data_quality_summary['overall_quality']['overall_missing_rate'],
            'quality_rating': data_quality_summary['overall_quality']['quality_rating'],
            'polarity_consistency': polarity_warnings['is_consistent']
        }])
        
        DATA_CACHE[result_uri] = analysis_df
        analysis_result["result_resource_id"] = result_resource_id
        
        print(f"å­—æ®µåˆ†æç»“æœå·²ç¼“å­˜: {result_uri}")
    
    return analysis_result


def check_polarity_consistency(polarity_suggestions: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ£€æŸ¥å­—æ®µææ€§ä¸€è‡´æ€§
    
    Args:
        polarity_suggestions: å­—æ®µææ€§å»ºè®®å­—å…¸
        
    Returns:
        ææ€§ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ
    """
    if not polarity_suggestions:
        return {
            "is_consistent": True,
            "message": "æ— æ•°å€¼å­—æ®µï¼Œæ— éœ€ææ€§æ£€æŸ¥",
            "polarity_distribution": {},
            "warnings": []
        }
    
    # ç»Ÿè®¡ææ€§åˆ†å¸ƒ
    polarity_counts = {}
    for field_info in polarity_suggestions.values():
        polarity = field_info.get('suggested_polarity', 'unknown')
        polarity_counts[polarity] = polarity_counts.get(polarity, 0) + 1
    
    # æ£€æŸ¥ä¸€è‡´æ€§
    is_consistent = len(polarity_counts) <= 1
    
    # ç”Ÿæˆè­¦å‘Šä¿¡æ¯
    warnings = []
    if not is_consistent:
        warnings.append("âš ï¸ æ£€æµ‹åˆ°æ··åˆææ€§å­—æ®µï¼åç»­è®¡ç®—å¯èƒ½äº§ç”Ÿé”™è¯¯ç»“æœ")
        warnings.append("   å»ºè®®ç»Ÿä¸€æ‰€æœ‰å­—æ®µçš„ææ€§ï¼ˆå…¨éƒ¨ä¸ºæå¤§å‹æˆ–æå°å‹ï¼‰")
    
    if 'unknown' in polarity_counts:
        warnings.append("âš ï¸ éƒ¨åˆ†å­—æ®µææ€§æ— æ³•è‡ªåŠ¨è¯†åˆ«ï¼Œè¯·æ‰‹åŠ¨ç¡®è®¤")
    
    return {
        "is_consistent": is_consistent,
        "message": "ææ€§ä¸€è‡´" if is_consistent else "ææ€§ä¸ä¸€è‡´",
        "polarity_distribution": polarity_counts,
        "warnings": warnings
    }


def generate_data_quality_summary(df: pd.DataFrame) -> Dict[str, Any]:
    """ç”Ÿæˆæ•°æ®è´¨é‡æ‘˜è¦"""
    
    total_rows = len(df)
    total_columns = len(df.columns)
    
    # è®¡ç®—æ€»ä½“ç¼ºå¤±æƒ…å†µ
    total_missing = df.isnull().sum().sum()
    total_cells = total_rows * total_columns
    overall_missing_rate = (total_missing / total_cells) * 100 if total_cells > 0 else 0
    
    # æŒ‰åˆ—ç»Ÿè®¡ç¼ºå¤±æƒ…å†µ
    column_missing_stats = {}
    for col in df.columns:
        missing_count = df[col].isnull().sum()
        missing_rate = (missing_count / total_rows) * 100
        column_missing_stats[col] = {
            "missing_count": int(missing_count),
            "missing_rate": float(missing_rate),
            "data_type": str(df[col].dtype)
        }
    
    # æ•°æ®è´¨é‡è¯„çº§
    if overall_missing_rate < 1:
        quality_rating = "ä¼˜ç§€"
    elif overall_missing_rate < 5:
        quality_rating = "è‰¯å¥½"
    elif overall_missing_rate < 10:
        quality_rating = "ä¸€èˆ¬"
    else:
        quality_rating = "è¾ƒå·®"
    
    return {
        "overall_quality": {
            "total_rows": total_rows,
            "total_columns": total_columns,
            "total_missing_cells": int(total_missing),
            "overall_missing_rate": float(overall_missing_rate),
            "quality_rating": quality_rating
        },
        "column_quality": column_missing_stats
    }


def generate_analysis_summary(df: pd.DataFrame, numeric_analysis: Dict, categorical_analysis: Dict, data_quality_summary: Dict, polarity_warnings: Dict = None) -> Dict[str, Any]:
    """ç”Ÿæˆåˆ†ææ€»ç»“"""
    
    numeric_count = len(numeric_analysis)
    categorical_count = len(categorical_analysis)
    total_fields = numeric_count + categorical_count
    
    # ç»Ÿè®¡å…³é”®æŒ‡æ ‡
    summary_stats = {
        "dataset_size": f"{len(df)} è¡Œ Ã— {len(df.columns)} åˆ—",
        "field_types": {
            "numeric_fields": numeric_count,
            "categorical_fields": categorical_count,
            "total_fields": total_fields
        },
        "data_quality": {
            "missing_rate": f"{data_quality_summary['overall_quality']['overall_missing_rate']:.2f}%",
            "quality_rating": data_quality_summary['overall_quality']['quality_rating']
        }
    }
    
    # ç”Ÿæˆå…³é”®å‘ç°
    key_findings = []
    
    # æ•°å€¼å­—æ®µå‘ç°
    if numeric_analysis:
        for col, analysis in numeric_analysis.items():
            basic_stats = analysis['basic_stats']
            data_quality = analysis['data_quality']
            
            findings = f"æ•°å€¼å­—æ®µ '{col}': "
            findings += f"èŒƒå›´ [{basic_stats['min']:.2f}, {basic_stats['max']:.2f}], "
            findings += f"å‡å€¼ {basic_stats['mean']:.2f}, "
            findings += f"ç¼ºå¤±ç‡ {data_quality['missing_percentage']:.1f}%"
            key_findings.append(findings)
    
    # åˆ†ç±»å­—æ®µå‘ç°
    if categorical_analysis:
        for col, analysis in categorical_analysis.items():
            freq_analysis = analysis['frequency_analysis']
            data_quality = analysis['data_quality']
            
            findings = f"åˆ†ç±»å­—æ®µ '{col}': "
            findings += f"{data_quality['unique_count']} ä¸ªå”¯ä¸€å€¼, "
            findings += f"ç¼ºå¤±ç‡ {data_quality['missing_percentage']:.1f}%"
            key_findings.append(findings)
    
    # æ•°æ®è´¨é‡å‘ç°
    quality_stats = data_quality_summary['overall_quality']
    if quality_stats['overall_missing_rate'] > 5:
        key_findings.append(f"âš ï¸  æ•°æ®è´¨é‡æ³¨æ„: æ€»ä½“ç¼ºå¤±ç‡ {quality_stats['overall_missing_rate']:.1f}% è¾ƒé«˜")
    else:
        key_findings.append(f"âœ… æ•°æ®è´¨é‡è‰¯å¥½: æ€»ä½“ç¼ºå¤±ç‡ {quality_stats['overall_missing_rate']:.1f}%")
    
    # ææ€§æ£€æŸ¥å‘ç°
    if polarity_warnings:
        if not polarity_warnings.get('is_consistent', True):
            key_findings.append("âš ï¸  **é‡è¦è­¦å‘Š**: æ£€æµ‹åˆ°æ··åˆææ€§å­—æ®µï¼")
            key_findings.append("   åç»­éš¶å±åº¦è®¡ç®—è¦æ±‚æ‰€æœ‰å­—æ®µææ€§ä¸€è‡´")
        else:
            key_findings.append("âœ… ææ€§ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
    
    # ç”Ÿæˆå»ºè®®
    recommendations = [
        "å»ºè®®å¯¹ç¼ºå¤±å€¼è¾ƒå¤šçš„å­—æ®µè¿›è¡Œæ•°æ®æ¸…æ´—",
        "æ•°å€¼å­—æ®µå¯è¿›ä¸€æ­¥è¿›è¡Œç›¸å…³æ€§åˆ†æå’Œåˆ†å¸ƒæ£€éªŒ",
        "åˆ†ç±»å­—æ®µå¯è¿›è¡Œé¢‘æ¬¡åˆ†æå’Œå¯è§†åŒ–å±•ç¤º"
    ]
    
    # æ·»åŠ ææ€§ç›¸å…³å»ºè®®
    if polarity_warnings and not polarity_warnings.get('is_consistent', True):
        recommendations.insert(0, "**é‡è¦**: è¯·ç»Ÿä¸€æ‰€æœ‰æ•°å€¼å­—æ®µçš„ææ€§ï¼ˆå…¨éƒ¨ä¸ºæå¤§å‹æˆ–æå°å‹ï¼‰")
        recommendations.insert(1, "æ£€æŸ¥å­—æ®µææ€§å»ºè®®ï¼Œç¡®ä¿ç¬¦åˆå®é™…ä¸šåŠ¡é€»è¾‘")
    
    return {
        "summary_statistics": summary_stats,
        "key_findings": key_findings,
        "recommendations": recommendations
    }


def find_latest_membership_resource() -> str:
    """
    æŸ¥æ‰¾æœ€æ–°çš„éš¶å±åº¦è®¡ç®—èµ„æºï¼ˆmcå¼€å¤´ï¼‰
    
    Returns:
        æœ€æ–°çš„mcèµ„æºIDï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
    """
    mc_resources = []
    
    # åœ¨DATA_CACHEä¸­æŸ¥æ‰¾mcå¼€å¤´çš„èµ„æº
    for uri, df in DATA_CACHE.items():
        if uri.startswith("data://mc_"):
            resource_id = uri[7:]  # ç§»é™¤ "data://" å‰ç¼€
            # éªŒè¯æ•°æ®æ ¼å¼æ˜¯å¦ä¸ºéš¶å±åº¦çŸ©é˜µ
            if _is_membership_matrix(df):
                mc_resources.append({
                    'resource_id': resource_id,
                    'uri': uri,
                    'rows': len(df),
                    'columns': len(df.columns)
                })
    
    if not mc_resources:
        return None
    
    # è¿”å›æœ€æ–°çš„mcèµ„æºï¼ˆæ ¹æ®èµ„æºIDä¸­çš„æ—¶é—´æˆ³æˆ–uuidæ’åºï¼‰
    # ç®€å•å¤„ç†ï¼šè¿”å›ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„mcèµ„æº
    return mc_resources[0]['resource_id']


def _generate_topsis_formatted_table(comprehensive_scores: Dict[str, List[float]], weights: List[float] = None) -> str:
    """
    ç”Ÿæˆç‰¹å®šæ ¼å¼çš„TOPSISç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µï¼ˆVå€¼ï¼‰è¡¨æ ¼
    
    Args:
        comprehensive_scores: TOPSISç›¸å¯¹æ¥è¿‘åº¦è®¡ç®—ç»“æœ
        weights: æƒé‡åˆ—è¡¨
        
    Returns:
        æ ¼å¼åŒ–çš„è¡¨æ ¼å­—ç¬¦ä¸²
    """
    weight_str = f"[{', '.join([f'{w:.2f}' for w in weights])}]" if weights else "[ç­‰æƒé‡]"
    
    # æ„å»ºè¡¨æ ¼å¤´éƒ¨
    table_lines = [
        f"**TOPSISç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µï¼ˆVå€¼ï¼‰ (æƒé‡: {weight_str}):**",
        "| ä¸šåŠ¡å¯¹è±¡ | çº§åˆ«1 | çº§åˆ«2 | çº§åˆ«3 | çº§åˆ«4 |",
        "|---------|-------|-------|-------|-------|"
    ]
    
    # æ·»åŠ æ•°æ®è¡Œ
    for obj_name, scores in comprehensive_scores.items():
        # æ ¼å¼åŒ–ç›¸å¯¹æ¥è¿‘åº¦å€¼ï¼ˆä¿ç•™3ä½å°æ•°ï¼‰
        score_str = " | ".join([f"{score:.3f}" for score in scores])
        table_lines.append(f"| {obj_name:<8} | {score_str} |")
    
    return "\n".join(table_lines)


def _generate_topsis_available_tables(comprehensive_membership: Dict[str, Any], 
                                     comprehensive_scores: Dict[str, List[float]],
                                     membership_scores: Dict[str, List[float]]) -> str:
    """
    ç”Ÿæˆå¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
    
    Args:
        comprehensive_membership: TOPSISç»¼åˆéš¶å±åº¦è®¡ç®—ç»“æœ
        comprehensive_scores: ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µ
        membership_scores: ç»¼åˆéš¶å±åº¦çŸ©é˜µ
        
    Returns:
        å¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•å­—ç¬¦ä¸²
    """
    table_list = [
        "**ğŸ“Š å¯æŸ¥è¯¢çš„è¡¨æ ¼æ¸…å•ï¼š**",
        "",
        "æ‚¨å¯ä»¥é€šè¿‡å›å¤ä»¥ä¸‹åç§°æ¥æŸ¥çœ‹å¯¹åº”çš„è¯¦ç»†æ•°æ®è¡¨æ ¼ï¼š",
        "",
        "1. **ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µ** - æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºæ–¹æ¡ˆè¶Šä¼˜",
        "2. **ç»¼åˆéš¶å±åº¦çŸ©é˜µ** - åŸºäºç›¸å¯¹æ¥è¿‘åº¦è®¡ç®—çš„æ ‡å‡†åŒ–ç»“æœ",
        "3. **è·ç¦»çŸ©é˜µ** - åŒ…å«ä¸æœ€ä¼˜è§£å’Œæœ€åŠ£è§£çš„æ¬§æ°è·ç¦»",
        "4. **å®Œæ•´ç»“æœè¡¨** - åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å®Œæ•´æ•°æ®è¡¨æ ¼",
        "",
        "**æŸ¥è¯¢ç¤ºä¾‹ï¼š**",
        "- å›å¤ \"ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µ\" æŸ¥çœ‹è¯¦ç»†Vå€¼æ•°æ®",
        "- å›å¤ \"è·ç¦»çŸ©é˜µ\" æŸ¥çœ‹D+å’ŒD-è·ç¦»æ•°æ®",
        "- å›å¤ \"å®Œæ•´ç»“æœè¡¨\" æŸ¥çœ‹åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„ç»¼åˆè¡¨æ ¼",
        "",
        "**è¡¨æ ¼è¯´æ˜ï¼š**",
        "- **ç›¸å¯¹æ¥è¿‘åº¦ï¼ˆVå€¼ï¼‰**ï¼š0-1ä¹‹é—´ï¼Œè¶Šå¤§è¡¨ç¤ºè¶Šæ¥è¿‘æœ€ä¼˜è§£",
        "- **ç»¼åˆéš¶å±åº¦ï¼ˆuå€¼ï¼‰**ï¼šåŸºäºVå€¼è®¡ç®—çš„æ ‡å‡†åŒ–ç»“æœ",
        "- **D+è·ç¦»**ï¼šä¸ç†æƒ³æœ€ä¼˜è§£çš„è·ç¦»ï¼Œè¶Šå°è¶Šå¥½",
        "- **D-è·ç¦»**ï¼šä¸ç†æƒ³æœ€åŠ£è§£çš„è·ç¦»ï¼Œè¶Šå¤§è¶Šå¥½"
    ]
    
    return "\n".join(table_list)


def _generate_relative_closeness_table(comprehensive_scores: Dict[str, List[float]], weights: List[float] = None) -> str:
    """
    ç”Ÿæˆç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µï¼ˆVå€¼ï¼‰è¯¦ç»†è¡¨æ ¼
    
    Args:
        comprehensive_scores: ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µ
        weights: æƒé‡åˆ—è¡¨
        
    Returns:
        è¯¦ç»†çš„ç›¸å¯¹æ¥è¿‘åº¦è¡¨æ ¼å­—ç¬¦ä¸²
    """
    weight_str = f"[{', '.join([f'{w:.2f}' for w in weights])}]" if weights else "[ç­‰æƒé‡]"
    
    table_lines = [
        f"**ğŸ“Š ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µï¼ˆVå€¼ï¼‰è¯¦ç»†æ•°æ® (æƒé‡: {weight_str}):**",
        "",
        "| ä¸šåŠ¡å¯¹è±¡ | çº§åˆ«1 | çº§åˆ«2 | çº§åˆ«3 | çº§åˆ«4 | æœ€å¤§å€¼ | æœ€ä¼˜çº§åˆ« |",
        "|---------|-------|-------|-------|-------|--------|----------|"
    ]
    
    for obj_name, scores in comprehensive_scores.items():
        # æ ¼å¼åŒ–ç›¸å¯¹æ¥è¿‘åº¦å€¼ï¼ˆä¿ç•™4ä½å°æ•°ï¼‰
        score_str = " | ".join([f"{score:.4f}" for score in scores])
        
        # è®¡ç®—æœ€å¤§å€¼å’Œæœ€ä¼˜çº§åˆ«
        max_score = max(scores)
        best_level = scores.index(max_score) + 1
        
        table_lines.append(f"| {obj_name:<8} | {score_str} | {max_score:.4f} | e{best_level} |")
    
    table_lines.extend([
        "",
        "**è¯´æ˜ï¼š**",
        "- **ç›¸å¯¹æ¥è¿‘åº¦ï¼ˆVå€¼ï¼‰**ï¼š0-1ä¹‹é—´çš„æ•°å€¼ï¼Œè¶Šå¤§è¡¨ç¤ºè¯¥ä¸šåŠ¡å¯¹è±¡åœ¨è¯¥è¯„ä¼°çº§åˆ«ä¸Šè¶Šæ¥è¿‘ç†æƒ³æœ€ä¼˜è§£",
        "- **æœ€å¤§å€¼**ï¼šè¯¥ä¸šåŠ¡å¯¹è±¡åœ¨æ‰€æœ‰è¯„ä¼°çº§åˆ«ä¸­çš„æœ€é«˜ç›¸å¯¹æ¥è¿‘åº¦",
        "- **æœ€ä¼˜çº§åˆ«**ï¼šç›¸å¯¹æ¥è¿‘åº¦æœ€é«˜çš„è¯„ä¼°çº§åˆ«ï¼Œè¡¨ç¤ºè¯¥ä¸šåŠ¡å¯¹è±¡æœ€é€‚åˆçš„è¯„ä¼°ç­‰çº§",
        "",
        "**è§£è¯»å»ºè®®ï¼š**",
        "- é‡ç‚¹å…³æ³¨æ¯ä¸ªä¸šåŠ¡å¯¹è±¡çš„**æœ€å¤§å€¼**å’Œ**æœ€ä¼˜çº§åˆ«**",
        "- ç›¸å¯¹æ¥è¿‘åº¦>0.8è¡¨ç¤ºéå¸¸æ¥è¿‘æœ€ä¼˜è§£",
        "- ç›¸å¯¹æ¥è¿‘åº¦<0.3è¡¨ç¤ºè·ç¦»æœ€ä¼˜è§£è¾ƒè¿œ"
    ])
    
    return "\n".join(table_lines)


def _generate_membership_matrix_table(membership_scores: Dict[str, List[float]], weights: List[float] = None) -> str:
    """
    ç”Ÿæˆç»¼åˆéš¶å±åº¦çŸ©é˜µï¼ˆuå€¼ï¼‰è¯¦ç»†è¡¨æ ¼
    
    Args:
        membership_scores: ç»¼åˆéš¶å±åº¦çŸ©é˜µ
        weights: æƒé‡åˆ—è¡¨
        
    Returns:
        è¯¦ç»†çš„ç»¼åˆéš¶å±åº¦è¡¨æ ¼å­—ç¬¦ä¸²
    """
    weight_str = f"[{', '.join([f'{w:.2f}' for w in weights])}]" if weights else "[ç­‰æƒé‡]"
    
    table_lines = [
        f"**ğŸ“Š ç»¼åˆéš¶å±åº¦çŸ©é˜µï¼ˆuå€¼ï¼‰è¯¦ç»†æ•°æ® (æƒé‡: {weight_str}):**",
        "",
        "| ä¸šåŠ¡å¯¹è±¡ | çº§åˆ«1 | çº§åˆ«2 | çº§åˆ«3 | çº§åˆ«4 | éš¶å±åº¦å’Œ | éš¶å±åº¦åˆ†å¸ƒ |",
        "|---------|-------|-------|-------|-------|----------|------------|"
    ]
    
    for obj_name, scores in membership_scores.items():
        # æ ¼å¼åŒ–ç»¼åˆéš¶å±åº¦å€¼ï¼ˆä¿ç•™4ä½å°æ•°ï¼‰
        score_str = " | ".join([f"{score:.4f}" for score in scores])
        
        # è®¡ç®—éš¶å±åº¦å’Œå’Œåˆ†å¸ƒ
        total_score = sum(scores)
        distribution = "/".join([f"{score/total_score*100:.1f}%" for score in scores])
        
        table_lines.append(f"| {obj_name:<8} | {score_str} | {total_score:.4f} | {distribution} |")
    
    table_lines.extend([
        "",
        "**è¯´æ˜ï¼š**",
        "- **ç»¼åˆéš¶å±åº¦ï¼ˆuå€¼ï¼‰**ï¼šåŸºäºç›¸å¯¹æ¥è¿‘åº¦è®¡ç®—çš„æ ‡å‡†åŒ–ç»“æœï¼Œè¡¨ç¤ºä¸šåŠ¡å¯¹è±¡å±äºå„è¯„ä¼°çº§åˆ«çš„ç¨‹åº¦",
        "- **éš¶å±åº¦å’Œ**ï¼šæ‰€æœ‰çº§åˆ«éš¶å±åº¦çš„æ€»å’Œï¼Œç”¨äºéªŒè¯è®¡ç®—æ­£ç¡®æ€§ï¼ˆåº”æ¥è¿‘1.0ï¼‰",
        "- **éš¶å±åº¦åˆ†å¸ƒ**ï¼šå„è¯„ä¼°çº§åˆ«åœ¨æ€»éš¶å±åº¦ä¸­çš„å æ¯”",
        "",
        "**è§£è¯»å»ºè®®ï¼š**",
        "- éš¶å±åº¦åˆ†å¸ƒæ˜¾ç¤ºä¸šåŠ¡å¯¹è±¡åœ¨å„è¯„ä¼°çº§åˆ«çš„å½’å±ç¨‹åº¦",
        "- éš¶å±åº¦å’Œåº”æ¥è¿‘1.0ï¼Œè¡¨ç¤ºéš¶å±åº¦è®¡ç®—æ­£ç¡®",
        "- åˆ†å¸ƒç™¾åˆ†æ¯”å¸®åŠ©ç†è§£ä¸šåŠ¡å¯¹è±¡çš„è¯„ä¼°çº§åˆ«åå¥½"
    ])
    
    return "\n".join(table_lines)


def _generate_distance_matrix_table(comprehensive_membership: Dict[str, Any], weights: List[float] = None) -> str:
    """
    ç”Ÿæˆè·ç¦»çŸ©é˜µè¯¦ç»†è¡¨æ ¼
    
    Args:
        comprehensive_membership: TOPSISç»¼åˆéš¶å±åº¦è®¡ç®—ç»“æœ
        weights: æƒé‡åˆ—è¡¨
        
    Returns:
        è¯¦ç»†çš„è·ç¦»çŸ©é˜µè¡¨æ ¼å­—ç¬¦ä¸²
    """
    weight_str = f"[{', '.join([f'{w:.2f}' for w in weights])}]" if weights else "[ç­‰æƒé‡]"
    
    table_lines = [
        f"**ğŸ“Š è·ç¦»çŸ©é˜µè¯¦ç»†æ•°æ® (æƒé‡: {weight_str}):**",
        "",
        "| ä¸šåŠ¡å¯¹è±¡ | D+è·ç¦»ï¼ˆçº§åˆ«1ï¼‰ | D+è·ç¦»ï¼ˆçº§åˆ«2ï¼‰ | D+è·ç¦»ï¼ˆçº§åˆ«3ï¼‰ | D+è·ç¦»ï¼ˆçº§åˆ«4ï¼‰ | D-è·ç¦»ï¼ˆçº§åˆ«1ï¼‰ | D-è·ç¦»ï¼ˆçº§åˆ«2ï¼‰ | D-è·ç¦»ï¼ˆçº§åˆ«3ï¼‰ | D-è·ç¦»ï¼ˆçº§åˆ«4ï¼‰ |",
        "|---------|----------------|----------------|----------------|----------------|----------------|----------------|----------------|----------------|"
    ]
    
    for obj_name, distances in comprehensive_membership.items():
        d_plus = distances["D+"]
        d_minus = distances["D-"]
        
        # æ ¼å¼åŒ–è·ç¦»å€¼ï¼ˆä¿ç•™4ä½å°æ•°ï¼‰
        d_plus_str = " | ".join([f"{d:.4f}" for d in d_plus])
        d_minus_str = " | ".join([f"{d:.4f}" for d in d_minus])
        
        table_lines.append(f"| {obj_name:<8} | {d_plus_str} | {d_minus_str} |")
    
    table_lines.extend([
        "",
        "**è¯´æ˜ï¼š**",
        "- **D+è·ç¦»**ï¼šä¸šåŠ¡å¯¹è±¡ä¸ç†æƒ³æœ€ä¼˜è§£çš„æ¬§æ°è·ç¦»ï¼Œè¶Šå°è¡¨ç¤ºè¶Šæ¥è¿‘æœ€ä¼˜è§£",
        "- **D-è·ç¦»**ï¼šä¸šåŠ¡å¯¹è±¡ä¸ç†æƒ³æœ€åŠ£è§£çš„æ¬§æ°è·ç¦»ï¼Œè¶Šå¤§è¡¨ç¤ºè¶Šè¿œç¦»æœ€åŠ£è§£",
        "",
        "**è§£è¯»å»ºè®®ï¼š**",
        "- ç†æƒ³çš„ä¸šåŠ¡å¯¹è±¡åº”è¯¥å…·æœ‰**è¾ƒå°çš„D+è·ç¦»**å’Œ**è¾ƒå¤§çš„D-è·ç¦»**",
        "- D+è·ç¦»<0.3è¡¨ç¤ºéå¸¸æ¥è¿‘æœ€ä¼˜è§£",
        "- D-è·ç¦»>0.7è¡¨ç¤ºè¿œç¦»æœ€åŠ£è§£"
    ])
    
    return "\n".join(table_lines)


def _generate_complete_result_table(comprehensive_membership: Dict[str, Any], 
                                   comprehensive_scores: Dict[str, List[float]],
                                   membership_scores: Dict[str, List[float]],
                                   weights: List[float] = None) -> str:
    """
    ç”Ÿæˆå®Œæ•´ç»“æœè¡¨æ ¼
    
    Args:
        comprehensive_membership: TOPSISç»¼åˆéš¶å±åº¦è®¡ç®—ç»“æœ
        comprehensive_scores: ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µ
        membership_scores: ç»¼åˆéš¶å±åº¦çŸ©é˜µ
        weights: æƒé‡åˆ—è¡¨
        
    Returns:
        å®Œæ•´çš„ç»¼åˆç»“æœè¡¨æ ¼å­—ç¬¦ä¸²
    """
    weight_str = f"[{', '.join([f'{w:.2f}' for w in weights])}]" if weights else "[ç­‰æƒé‡]"
    
    table_lines = [
        f"**ğŸ“Š å®Œæ•´TOPSISç»“æœè¡¨æ ¼ (æƒé‡: {weight_str}):**",
        "",
        "| ä¸šåŠ¡å¯¹è±¡ | è¯„ä¼°çº§åˆ« | ç›¸å¯¹æ¥è¿‘åº¦ | ç»¼åˆéš¶å±åº¦ | D+è·ç¦» | D-è·ç¦» | çº§åˆ«æ’å |",
        "|---------|----------|------------|------------|--------|--------|----------|"
    ]
    
    for obj_name in comprehensive_membership.keys():
        for level_idx in range(len(comprehensive_scores[obj_name])):
            v_value = comprehensive_scores[obj_name][level_idx]
            u_value = membership_scores[obj_name][level_idx]
            d_plus = comprehensive_membership[obj_name]["D+"][level_idx]
            d_minus = comprehensive_membership[obj_name]["D-"][level_idx]
            
            # è®¡ç®—è¯¥çº§åˆ«åœ¨æ‰€æœ‰ä¸šåŠ¡å¯¹è±¡ä¸­çš„æ’å
            level_scores = [comprehensive_scores[obj][level_idx] for obj in comprehensive_scores.keys()]
            sorted_scores = sorted(level_scores, reverse=True)
            rank = sorted_scores.index(v_value) + 1
            
            table_lines.append(f"| {obj_name:<8} | e{level_idx+1} | {v_value:.4f} | {u_value:.4f} | {d_plus:.4f} | {d_minus:.4f} | {rank} |")
    
    table_lines.extend([
        "",
        "**è¯´æ˜ï¼š**",
        "- **ç›¸å¯¹æ¥è¿‘åº¦ï¼ˆVå€¼ï¼‰**ï¼šæ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ï¼Œè¶Šå¤§è¶Šå¥½",
        "- **ç»¼åˆéš¶å±åº¦ï¼ˆuå€¼ï¼‰**ï¼šæ ‡å‡†åŒ–ç»“æœï¼Œè¡¨ç¤ºå½’å±ç¨‹åº¦",
        "- **D+è·ç¦»**ï¼šä¸æœ€ä¼˜è§£çš„è·ç¦»ï¼Œè¶Šå°è¶Šå¥½",
        "- **D-è·ç¦»**ï¼šä¸æœ€åŠ£è§£çš„è·ç¦»ï¼Œè¶Šå¤§è¶Šå¥½",
        "- **çº§åˆ«æ’å**ï¼šè¯¥ä¸šåŠ¡å¯¹è±¡åœ¨è¯¥è¯„ä¼°çº§åˆ«ä¸­çš„ç›¸å¯¹æ’å",
        "",
        "**è§£è¯»å»ºè®®ï¼š**",
        "- æŒ‰ä¸šåŠ¡å¯¹è±¡åˆ†ç»„æŸ¥çœ‹å„è¯„ä¼°çº§åˆ«çš„è¡¨ç°",
        "- å…³æ³¨ç›¸å¯¹æ¥è¿‘åº¦å’Œçº§åˆ«æ’åè¿›è¡Œç»¼åˆè¯„ä¼°",
        "- ç»“åˆD+å’ŒD-è·ç¦»åˆ†ææ–¹æ¡ˆçš„ä¼˜åŠ£ç¨‹åº¦"
    ])
    
    return "\n".join(table_lines)


def _generate_topsis_next_actions(num_objects: int) -> str:
    """
    ç”ŸæˆTOPSISè®¡ç®—åçš„ä¸‹ä¸€æ­¥å»ºè®®
    
    Args:
        num_objects: è¯„ä¼°çš„ä¸šåŠ¡å¯¹è±¡æ•°é‡
        
    Returns:
        ä¸‹ä¸€æ­¥å»ºè®®å­—ç¬¦ä¸²
    """
    return f"""
**ä¸‹ä¸€æ­¥å»ºè®®ï¼š**
1. **ç»“æœè§£è¯»**ï¼šç›¸å¯¹æ¥è¿‘åº¦å€¼è¶Šå¤§è¡¨ç¤ºè¯¥ä¸šåŠ¡å¯¹è±¡åœ¨å¯¹åº”è¯„ä¼°çº§åˆ«ä¸‹è¶Šæ¥è¿‘æœ€ä¼˜è§£
2. **å†³ç­–æ”¯æŒ**ï¼šæ ¹æ®ç›¸å¯¹æ¥è¿‘åº¦å€¼è¿›è¡Œä¸šåŠ¡å¯¹è±¡æ’åºï¼Œé€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ
3. **è¿›ä¸€æ­¥åˆ†æ**ï¼šå¯è°ƒç”¨ `export_resource_to_csv` å‡½æ•°å¯¼å‡ºè¯¦ç»†æ•°æ®è¡¨æ ¼è¿›è¡Œæ·±å…¥åˆ†æ
4. **å¯¹æ¯”åˆ†æ**ï¼šå¯å°è¯•ä½¿ç”¨VIKORæ–¹æ³•è¿›è¡Œå¯¹æ¯”åˆ†æï¼Œè·å¾—ä¸åŒè§†è§’çš„å†³ç­–ç»“æœ

æœ¬æ¬¡è¯„ä¼°å…±åˆ†æäº† {num_objects} ä¸ªä¸šåŠ¡å¯¹è±¡ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨ç›¸å¯¹æ¥è¿‘åº¦è¾ƒé«˜çš„æ–¹æ¡ˆã€‚
"""


def _generate_topsis_original_data_preview(df: pd.DataFrame, resource_id: str) -> str:
    """
    ç”ŸæˆTOPSISç»“æœçš„åŸå§‹æ•°æ®æ ¼å¼é¢„è§ˆ
    
    Args:
        df: TOPSISç»“æœæ•°æ®æ¡†
        resource_id: èµ„æºID
        
    Returns:
        åŸå§‹æ•°æ®æ ¼å¼é¢„è§ˆå­—ç¬¦ä¸²
    """
    # æ£€æŸ¥æ˜¯å¦ä¸ºTOPSISç»“æœ
    if not (resource_id.startswith("mcr_") and "topsiseval" in resource_id):
        return ""
    
    # æ£€æŸ¥æ•°æ®æ¡†æ˜¯å¦åŒ…å«å¿…è¦çš„åˆ—
    required_columns = ['ä¸šåŠ¡å¯¹è±¡', 'è¯„ä¼°çº§åˆ«', 'ç›¸å¯¹æ¥è¿‘åº¦', 'ç»¼åˆéš¶å±åº¦', 'ä¸æœ€ä¼˜è§£è·ç¦»', 'ä¸æœ€åŠ£è§£è·ç¦»']
    if not all(col in df.columns for col in required_columns):
        return ""
    
    # è·å–å‰8è¡Œæ•°æ®ä½œä¸ºé¢„è§ˆ
    preview_df = df.head(8).copy()
    
    # æ ¼å¼åŒ–æ•°å€¼ï¼ˆä¿ç•™3ä½å°æ•°ï¼‰
    numeric_columns = ['ç›¸å¯¹æ¥è¿‘åº¦', 'ç»¼åˆéš¶å±åº¦', 'ä¸æœ€ä¼˜è§£è·ç¦»', 'ä¸æœ€åŠ£è§£è·ç¦»']
    for col in numeric_columns:
        if col in preview_df.columns:
            preview_df[col] = preview_df[col].apply(lambda x: f"{x:.3f}" if pd.notnull(x) else "")
    
    # æ„å»ºåŸå§‹æ•°æ®é¢„è§ˆ
    preview_lines = ["**åŸå§‹æ•°æ®æ ¼å¼:**"]
    preview_lines.append("   ä¸šåŠ¡å¯¹è±¡ è¯„ä¼°çº§åˆ«  ç›¸å¯¹æ¥è¿‘åº¦  ç»¼åˆéš¶å±åº¦  ä¸æœ€ä¼˜è§£è·ç¦»  ä¸æœ€åŠ£è§£è·ç¦»")
    
    for idx, row in preview_df.iterrows():
        line = f"{idx:<2}  {row['ä¸šåŠ¡å¯¹è±¡']:<8} {row['è¯„ä¼°çº§åˆ«']:<8} {row['ç›¸å¯¹æ¥è¿‘åº¦']:<10} {row['ç»¼åˆéš¶å±åº¦']:<10} {row['ä¸æœ€ä¼˜è§£è·ç¦»']:<12} {row['ä¸æœ€åŠ£è§£è·ç¦»']:<12}"
        preview_lines.append(line)
    
    # æ·»åŠ å­—æ®µè¯´æ˜
    preview_lines.append("")
    preview_lines.append("**å­—æ®µè¯´æ˜:**")
    preview_lines.append("- **ä¸šåŠ¡å¯¹è±¡**: è¢«è¯„ä»·çš„ä¸šåŠ¡å¯¹è±¡æ ‡è¯†ç¬¦")
    preview_lines.append("- **è¯„ä¼°çº§åˆ«**: éš¶å±åº¦è¯„ä¼°çš„çº§åˆ«ï¼ˆe1-e4ï¼‰")
    preview_lines.append("- **ç›¸å¯¹æ¥è¿‘åº¦**: å€¼è¶Šå¤§è¡¨ç¤ºæ–¹æ¡ˆè¶Šä¼˜")
    preview_lines.append("- **ç»¼åˆéš¶å±åº¦**: æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ï¼Œä¸šåŠ¡å¯¹è±¡åœ¨ç‰¹å®šçº§åˆ«ä¸‹çš„éš¶å±ç¨‹åº¦")
    preview_lines.append("- **ä¸æœ€ä¼˜è§£è·ç¦»**: ä¸ç†æƒ³æœ€ä¼˜è§£çš„è·ç¦»ï¼Œè¶Šå°è¶Šå¥½")
    preview_lines.append("- **ä¸æœ€åŠ£è§£è·ç¦»**: ä¸ç†æƒ³æœ€åŠ£è§£çš„è·ç¦»ï¼Œè¶Šå¤§è¶Šå¥½")
    
    return "\n".join(preview_lines)

# æŠ¥å‘Šæ ¼å¼å·²åœ¨å‡½æ•°æ–‡æ¡£å­—ç¬¦ä¸²ä¸­æ˜ç¡®å®šä¹‰ï¼Œç›´æ¥é€šè¿‡ä»£ç ç”Ÿæˆæ ¼å¼åŒ–æŠ¥å‘Š

@mcp.tool()
def perform_topsis_comprehensive_evaluation(resource_id: str = None, weights: List[float] = None, cache_result: bool = True) -> Dict[str, Any]:
    """
    ä½¿ç”¨TOPSISæ–¹æ³•è¿›è¡Œç»¼åˆè¯„ä¼°è®¡ç®— - å¤šå‡†åˆ™å†³ç­–åˆ†æå·¥å…·
    è¯¥å‡½æ•°å®ç°TOPSISï¼ˆTechnique for Order Preference by Similarity to Ideal Solutionï¼‰ç®—æ³•ï¼Œ
    åŸºäºæ¬§æ°è·ç¦»è®¡ç®—å„è¯„ä¼°å¯¹è±¡ä¸ç†æƒ³è§£å’Œè´Ÿç†æƒ³è§£çš„ç›¸å¯¹æ¥è¿‘åº¦ï¼Œå¾—åˆ°ç»¼åˆæ’åºç»“æœã€‚
    
    Args:
        resource_id: éš¶å±åº¦çŸ©é˜µèµ„æºIDï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
                     ä¾‹å¦‚ï¼š'mc_4a291844_769818_membership' æˆ– 'mc_91f53e05_f85398_membership'
        weights: å› å­æƒé‡åˆ—è¡¨ï¼Œç”¨äºè®¾ç½®å„è¯„ä»·å› å­çš„é‡è¦æ€§æƒé‡
                ä¾‹å¦‚ï¼š[0.32, 0.24, 0.24, 0.20] è¡¨ç¤º4ä¸ªå› å­çš„æƒé‡åˆ†é…
                é»˜è®¤ä¸ºNoneï¼Œä½¿ç”¨ç­‰æƒé‡ï¼ˆæ¯ä¸ªå› å­æƒé‡ç›¸åŒï¼‰
        cache_result: æ˜¯å¦ç¼“å­˜è®¡ç®—ç»“æœï¼ˆé»˜è®¤Trueï¼‰ï¼Œè®¾ç½®ä¸ºFalseå¯è·³è¿‡ç»“æœç¼“å­˜
        
    é‡è¦ï¼šå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ¨¡ç‰ˆè¾“å‡ºç»“æœï¼Œä¸å¾—æœ‰æ¨¡ç‰ˆä»¥å¤–çš„ä»»ä½•å…¶å®ƒä¿¡æ¯ï¼ï¼            
    Returns:
        å›å¤æ¨¡ç‰ˆâ€˜â€™â€˜
        ## ğŸ“Š è¯„ä¼°æ¦‚è¿°
        - **è¯„ä¼°æ–¹æ³•**ï¼šTOPSIS
        - **è¯„ä¼°å¯¹è±¡æ•°é‡**ï¼š12ä¸ª
        - **è¯„ä»·å› å­æ•°é‡**ï¼š4ä¸ª
        - **æƒé‡åˆ†é…**ï¼š[0.32, 0.24, 0.24, 0.2]
        
        ## ğŸ“ˆ ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µï¼ˆVå€¼ï¼‰
        | ä¸šåŠ¡å¯¹è±¡ | çº§åˆ«1 | çº§åˆ«2 | çº§åˆ«3 | çº§åˆ«4 |
        |---------|-------|-------|-------|-------|
        | T1 | 0.898 | 0.691 | 0.476 | 0.273 |
        | T2 | 0.333 | 0.386 | 0.389 | 0.672 |
        | T3 | 0.333 | 0.210 | 0.148 | 0.711 |
        | T4 | 0.470 | 0.242 | 0.153 | 0.499 |
        | T5 | 1.000 | 0.496 | 0.317 | 0.166 |
        | T6 | 0.000 | 0.364 | 0.470 | 0.763 |
        | T7 | 1.000 | 0.579 | 0.389 | 0.205 |
        
        ## ğŸ“‹ å¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
        1. **ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µ** - æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºæ–¹æ¡ˆè¶Šä¼˜
        2. **ç»¼åˆéš¶å±åº¦çŸ©é˜µ** - åŸºäºç›¸å¯¹æ¥è¿‘åº¦è®¡ç®—çš„æ ‡å‡†åŒ–ç»“æœ
        3. **è·ç¦»çŸ©é˜µ** - åŒ…å«ä¸æœ€ä¼˜è§£å’Œæœ€åŠ£è§£çš„æ¬§æ°è·ç¦»
        4. **å®Œæ•´ç»“æœè¡¨** - åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å®Œæ•´æ•°æ®è¡¨æ ¼
        
        ## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®â€™â€˜â€™
    """
    # å¦‚æœæ²¡æœ‰æä¾›resource_idï¼Œå°è¯•è‡ªåŠ¨å‘ç°mcèµ„æº
    if resource_id is None:
        resource_id = find_latest_membership_resource()
        if resource_id is None:
            raise ValueError("æœªæ‰¾åˆ°å¯ç”¨çš„éš¶å±åº¦èµ„æº(mc_å¼€å¤´)ï¼Œè¯·å…ˆæ‰§è¡Œéš¶å±åº¦è®¡ç®—æˆ–ä¸Šä¼ éš¶å±åº¦æ•°æ®")
        print(f"ğŸ¤– è‡ªåŠ¨å‘ç°éš¶å±åº¦èµ„æº: {resource_id}")
    
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id and resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    uri = get_resource_uri(resource_id)
    
    if uri not in DATA_CACHE:
        raise ValueError(f"èµ„æºä¸å­˜åœ¨: {uri}")
    
    # è·å–éš¶å±åº¦çŸ©é˜µæ•°æ®
    membership_df = DATA_CACHE[uri]
    
    # æ£€æŸ¥æ•°æ®æ ¼å¼
    if not _is_membership_matrix(membership_df):
        raise ValueError("æ•°æ®æ ¼å¼é”™è¯¯ï¼šéœ€è¦éš¶å±åº¦çŸ©é˜µæ ¼å¼çš„æ•°æ®")
    
    # è½¬æ¢éš¶å±åº¦çŸ©é˜µä¸ºè§„èŒƒåŒ–ç‰¹å¾å€¼çŸ©é˜µæ ¼å¼
    normalized_matrices = _convert_membership_to_normalized_matrix(membership_df)
    
    # åˆ›å»ºTOPSISè¯„ä¼°å™¨
    topsis_evaluator = TOPSISComprehensiveEvaluation()
    
    # è®¡ç®—TOPSISç»¼åˆéš¶å±åº¦
    comprehensive_membership = topsis_evaluator.calculate_comprehensive_membership(normalized_matrices, weights)
    
    # è®¡ç®—ç›¸å¯¹æ¥è¿‘åº¦ï¼ˆVå€¼ï¼‰
    comprehensive_scores = topsis_evaluator.get_comprehensive_scores(comprehensive_membership)
    
    # è®¡ç®—æœ€ç»ˆç»¼åˆéš¶å±åº¦ï¼ˆuå€¼ï¼‰
    membership_scores = topsis_evaluator.calculate_comprehensive_membership_scores(comprehensive_scores)
    
    # ç”Ÿæˆç»“æœæ‘˜è¦
    summary = {
        "evaluation_method": "TOPSIS",
        "total_objects": len(comprehensive_membership),
        "total_factors": len(weights) if weights else 4,  # é»˜è®¤4ä¸ªå› å­
        "weight_distribution": weights if weights else [0.25, 0.25, 0.25, 0.25],
        "sample_results": {}
    }
    
    # æ·»åŠ å‰3ä¸ªå¯¹è±¡çš„ç¤ºä¾‹ç»“æœ
    sample_objects = list(comprehensive_membership.keys())[:3]
    for obj_name in sample_objects:
        summary["sample_results"][obj_name] = {
            "D+_distances": comprehensive_membership[obj_name]["D+"][:2],  # å‰2ä¸ªçº§åˆ«
            "D-_distances": comprehensive_membership[obj_name]["D-"][:2],  # å‰2ä¸ªçº§åˆ«
            "relative_closeness": comprehensive_scores[obj_name][:2],      # å‰2ä¸ªçº§åˆ«
            "membership_scores": membership_scores[obj_name][:2]          # å‰2ä¸ªçº§åˆ«
        }
    
    # ç”Ÿæˆç‰¹å®šæ ¼å¼çš„TOPSISç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µè¡¨æ ¼
    topsis_table = _generate_topsis_formatted_table(comprehensive_scores, weights)
    
    # ç”Ÿæˆå¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
    available_tables = _generate_topsis_available_tables(comprehensive_membership, comprehensive_scores, membership_scores)
    
    # ç”Ÿæˆä¸‹ä¸€æ­¥å»ºè®®
    next_actions = _generate_topsis_next_actions(len(comprehensive_membership))
    
    result = {
        "resource_id": resource_id,
        "comprehensive_membership": comprehensive_membership,
        "comprehensive_scores": comprehensive_scores,
        "membership_scores": membership_scores,
        "summary": summary,
        "topsis_table": topsis_table,
        "available_tables": available_tables,
        "next_actions": next_actions
    }
    
    # å¦‚æœéœ€è¦ç¼“å­˜ç»“æœ
    if cache_result:
        # ç”Ÿæˆç»“æœèµ„æºID
        result_resource_id = generate_resource_id(
            "multi_criteria", 
            parent_resource_id=resource_id,
            step_name="topsis_evaluation"
        )
        result_uri = get_resource_uri(result_resource_id)
        
        # å°†ç»“æœè½¬æ¢ä¸ºDataFrameæ ¼å¼ç¼“å­˜
        result_data = []
        for obj_name, scores in membership_scores.items():
            for level_idx, score in enumerate(scores):
                result_data.append({
                    'ä¸šåŠ¡å¯¹è±¡': obj_name,
                    'è¯„ä¼°çº§åˆ«': f'e{level_idx + 1}',
                    'ç»¼åˆéš¶å±åº¦': score,
                    'ç›¸å¯¹æ¥è¿‘åº¦': comprehensive_scores[obj_name][level_idx],
                    'ä¸æœ€ä¼˜è§£è·ç¦»': comprehensive_membership[obj_name]["D+"][level_idx],
                    'ä¸æœ€åŠ£è§£è·ç¦»': comprehensive_membership[obj_name]["D-"][level_idx]
                })
        
        result_df = pd.DataFrame(result_data)
        DATA_CACHE[result_uri] = result_df
        
        # æ³¨å†Œèµ„æºåˆ°ç´¢å¼•ä¸­ï¼ˆè§£å†³é¢„è§ˆé—®é¢˜ï¼‰
        RESOURCE_INDEX[result_resource_id] = {
            "uri": result_uri,
            "data_shape": f"{len(result_df)}è¡ŒÃ—{len(result_df.columns)}åˆ—",
            "resource_type": "topsis_evaluation",
            "description": "TOPSISç»¼åˆå¾—åˆ†çŸ©é˜µï¼ˆç›¸å¯¹æ¥è¿‘åº¦Vå€¼ï¼‰"
        }
        result["result_resource_id"] = result_resource_id
        
        print(f"TOPSISç»¼åˆå¾—åˆ†çŸ©é˜µå·²ç¼“å­˜: {result_uri}")
    
    # ç”Ÿæˆæ ¼å¼åŒ–æŠ¥å‘Šå¹¶æ·»åŠ åˆ°ç»“æœä¸­ - ç›´æ¥ä½¿ç”¨æ¨¡æ¿å­—ç¬¦ä¸²é¿å…LLMåŠ å·¥
    summary = result.get("summary", {})
    total_objects = summary.get("total_objects", 0)
    total_factors = summary.get("total_factors", 0)
    weight_distribution = summary.get("weight_distribution", [])
    
    result["formatted_report"] = f"""## ğŸ“Š è¯„ä¼°æ¦‚è¿°
- **è¯„ä¼°æ–¹æ³•**ï¼šTOPSISï¼ˆé€¼è¿‘ç†æƒ³è§£æ’åºæ³•ï¼‰
- **è¯„ä¼°å¯¹è±¡æ•°é‡**ï¼š{total_objects} ä¸ª
- **è¯„ä»·å› å­æ•°é‡**ï¼š{total_factors} ä¸ª
- **æƒé‡åˆ†é…**ï¼š{weight_distribution}

## ğŸ“ˆ ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µï¼ˆVå€¼ï¼‰
{topsis_table}

## ğŸ“‹ å¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
{available_tables}

## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®
{next_actions}"""
    
    return result


@mcp.tool()
def perform_vikor_comprehensive_evaluation(resource_id: str, weights: List[float] = None, v: float = 0.5, cache_result: bool = True) -> Dict[str, Any]:
    """
    ä½¿ç”¨VIKORæ–¹æ³•è¿›è¡Œç»¼åˆè¯„ä¼°è®¡ç®— - å¤šå‡†åˆ™å¦¥åæ’åºåˆ†æå·¥å…·
    
    è¯¥å‡½æ•°å®ç°VIKORï¼ˆVIKOR compromise ranking methodï¼‰ç®—æ³•ï¼Œ
    åŸºäºå¦¥åè§£çš„æ¦‚å¿µè¿›è¡Œå¤šå±æ€§å†³ç­–åˆ†æï¼Œè®¡ç®—Så€¼ï¼ˆç¾¤ä½“æ•ˆç”¨ï¼‰ã€Rå€¼ï¼ˆä¸ªä½“é—æ†¾ï¼‰å’ŒQå€¼ï¼ˆå¦¥åè§£ï¼‰ã€‚
    
    
    Args:
        resource_id: éš¶å±åº¦çŸ©é˜µèµ„æºIDï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
                     ä¾‹å¦‚ï¼š'membership_matrix_001' æˆ– 'data://membership_matrix_001'
        weights: å› å­æƒé‡åˆ—è¡¨ï¼Œç”¨äºè®¾ç½®å„è¯„ä»·å› å­çš„é‡è¦æ€§æƒé‡
                ä¾‹å¦‚ï¼š[0.32, 0.24, 0.24, 0.20] è¡¨ç¤º4ä¸ªå› å­çš„æƒé‡åˆ†é…
                é»˜è®¤ä¸ºNoneï¼Œä½¿ç”¨ç­‰æƒé‡ï¼ˆæ¯ä¸ªå› å­æƒé‡ç›¸åŒï¼‰
        v: ç­–ç•¥æƒé‡ï¼Œ0â‰¤vâ‰¤1ï¼Œè°ƒèŠ‚ç¾¤ä½“æ•ˆç”¨ä¸ä¸ªä½“é—æ†¾çš„æƒè¡¡
           v=0ï¼šä»¥ä¸ªä½“æœ€å¤§é—æ†¾ä¸ºåŸºç¡€ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
           v=1ï¼šä»¥ç¾¤ä½“å¤šæ•°æ•ˆç”¨ä¸ºåŸºç¡€ï¼ˆæ¿€è¿›ç­–ç•¥ï¼‰
           v=0.5ï¼šå¹³è¡¡ç­–ç•¥ï¼ˆé»˜è®¤ï¼‰
        cache_result: æ˜¯å¦ç¼“å­˜è®¡ç®—ç»“æœï¼ˆé»˜è®¤Trueï¼‰ï¼Œè®¾ç½®ä¸ºFalseå¯è·³è¿‡ç»“æœç¼“å­˜
        
    Returnsï¼š
        
        formatted_reportæ¨¡æ¿æ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼æŒ‰ç…§æ­¤æ ¼å¼è¾“å‡ºï¼Œä¸å¾—æœ‰ä»»ä½•é¢å¤–å†…å®¹ï¼‰ï¼š
        
        ## ğŸ“Š è¯„ä¼°æ¦‚è¿°
        - **è¯„ä¼°æ–¹æ³•**ï¼šVIKORï¼ˆå¤šå‡†åˆ™å¦¥åæ’åºæ³•ï¼‰
        - **è¯„ä¼°å¯¹è±¡æ•°é‡**ï¼š5ä¸ª
        - **è¯„ä»·å› å­æ•°é‡**ï¼š4ä¸ª
        - **æƒé‡åˆ†é…**ï¼š[0.32, 0.24, 0.24, 0.2]
        - **ç­–ç•¥æƒé‡**ï¼šv = 0.5ï¼ˆ0=ä¿å®ˆç­–ç•¥ï¼Œ1=æ¿€è¿›ç­–ç•¥ï¼Œ0.5=å¹³è¡¡ç­–ç•¥ï¼‰
        
        ## ğŸ“ˆ å¦¥åæ’åºç»“æœï¼ˆQå€¼æ’åºï¼‰
        | ä¸šåŠ¡å¯¹è±¡ | Så€¼(ç¾¤ä½“æ•ˆç”¨) | Rå€¼(ä¸ªä½“é—æ†¾) | Qå€¼(å¦¥åè§£) | æ’åº |
        |----------|---------------|---------------|------------|------|
        | T2 | 0.4025 | 0.1364 | 0.0000 | 1 |
        | T1 | 0.3973 | 0.1455 | 0.2134 | 2 |
        | T4 | 0.4261 | 0.2070 | 0.7167 | 3 |
        | T3 | 0.5876 | 0.2006 | 0.9254 | 4 |
        | T5 | 0.5434 | 0.2229 | 1.0000 | 5 |
        
        ## ğŸ“‹ å¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
        - **VIKORç»¼åˆè¯„ä¼°è¡¨**ï¼šåŒ…å«Sã€Rã€Qå€¼çš„å®Œæ•´è¯„ä¼°ç»“æœ
        - **å¦¥åæ’åºè¡¨**ï¼šåŸºäºQå€¼çš„æ–¹æ¡ˆæ’åºç»“æœ
        - **æ•æ„Ÿæ€§åˆ†æè¡¨**ï¼šä¸åŒç­–ç•¥æƒé‡vä¸‹çš„ç»“æœå¯¹æ¯”
        
        ## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®
        1. **æŸ¥çœ‹è¯¦ç»†æ’åºç»“æœ**ï¼šåˆ†æQå€¼æ’åºï¼Œè¯†åˆ«æœ€ä¼˜å¦¥åè§£
        2. **æ•æ„Ÿæ€§åˆ†æ**ï¼šè°ƒæ•´ç­–ç•¥æƒé‡vï¼ˆå½“å‰v=0.5ï¼‰ï¼Œè§‚å¯Ÿç»“æœç¨³å®šæ€§
        3. **ç»“æœéªŒè¯**ï¼šç»“åˆSå€¼å’ŒRå€¼ï¼ŒéªŒè¯å¦¥åè§£çš„åˆç†æ€§
        4. **å†³ç­–æ”¯æŒ**ï¼šåŸºäºQå€¼æ’åºï¼Œä¸ºæœ€ç»ˆå†³ç­–æä¾›é‡åŒ–ä¾æ®
        
        ğŸ’¡ **VIKORç»“æœè§£è¯»**ï¼š
        - **Qå€¼è¶Šå°**ï¼šå¦¥åè§£è¶Šä¼˜ï¼Œæ¨èåº¦è¶Šé«˜
        - **Så€¼**ï¼šç¾¤ä½“æ•ˆç”¨ï¼Œè¶Šå°è¡¨ç¤ºæ•´ä½“è¡¨ç°è¶Šå¥½
        - **Rå€¼**ï¼šä¸ªä½“é—æ†¾ï¼Œè¶Šå°è¡¨ç¤ºæœ€å·®å±æ€§è¡¨ç°è¶Šå¥½
    """
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    uri = get_resource_uri(resource_id)
    
    if uri not in DATA_CACHE:
        raise ValueError(f"èµ„æºä¸å­˜åœ¨: {uri}")
    
    # è·å–éš¶å±åº¦çŸ©é˜µæ•°æ®
    membership_df = DATA_CACHE[uri]
    
    # æ£€æŸ¥æ•°æ®æ ¼å¼
    if not _is_membership_matrix(membership_df):
        raise ValueError("æ•°æ®æ ¼å¼é”™è¯¯ï¼šéœ€è¦éš¶å±åº¦çŸ©é˜µæ ¼å¼çš„æ•°æ®")
    
    # è½¬æ¢éš¶å±åº¦çŸ©é˜µä¸ºè§„èŒƒåŒ–ç‰¹å¾å€¼çŸ©é˜µæ ¼å¼
    normalized_matrices = _convert_membership_to_normalized_matrix(membership_df)
    
    # åˆ›å»ºVIKORè¯„ä¼°å™¨
    vikor_evaluator = VIKORComprehensiveEvaluation()
    
    # è®¡ç®—VIKORç»¼åˆå¾—åˆ†
    comprehensive_scores = vikor_evaluator.calculate_comprehensive_scores(normalized_matrices, weights, v)
    
    # ç”Ÿæˆç»“æœæ‘˜è¦
    summary = {
        "evaluation_method": "VIKOR",
        "total_objects": len(comprehensive_scores),
        "total_factors": len(weights) if weights else 4,  # é»˜è®¤4ä¸ªå› å­
        "weight_distribution": weights if weights else [0.25, 0.25, 0.25, 0.25],
        "strategy_weight": v,
        "sample_results": {}
    }
    
    # æ·»åŠ å‰3ä¸ªå¯¹è±¡çš„ç¤ºä¾‹ç»“æœ
    sample_objects = list(comprehensive_scores.keys())[:3]
    for obj_name in sample_objects:
        summary["sample_results"][obj_name] = {
            "S_values": comprehensive_scores[obj_name]["S"][:2],  # å‰2ä¸ªçº§åˆ«
            "R_values": comprehensive_scores[obj_name]["R"][:2],  # å‰2ä¸ªçº§åˆ«
            "Q_values": comprehensive_scores[obj_name]["Q"][:2]   # å‰2ä¸ªçº§åˆ«
        }
    
    result = {
        "resource_id": resource_id,
        "comprehensive_scores": comprehensive_scores,
        "summary": summary
    }
    
    # å¦‚æœéœ€è¦ç¼“å­˜ç»“æœ
    if cache_result:
        # ç”Ÿæˆç»“æœèµ„æºID
        result_resource_id = generate_resource_id(
            "multi_criteria", 
            parent_resource_id=resource_id,
            step_name="vikor_evaluation"
        )
        result_uri = get_resource_uri(result_resource_id)
        
        # å°†ç»“æœè½¬æ¢ä¸ºDataFrameæ ¼å¼ç¼“å­˜
        result_data = []
        for obj_name, scores in comprehensive_scores.items():
            for level_idx in range(len(scores["S"])):
                result_data.append({
                    'ä¸šåŠ¡å¯¹è±¡': obj_name,
                    'è¯„ä¼°çº§åˆ«': f'e{level_idx + 1}',
                    'Så€¼': scores["S"][level_idx],
                    'Rå€¼': scores["R"][level_idx],
                    'Qå€¼': scores["Q"][level_idx]
                })
        
        result_df = pd.DataFrame(result_data)
        DATA_CACHE[result_uri] = result_df
        result["result_resource_id"] = result_resource_id
        
        print(f"VIKORç»¼åˆè¯„ä¼°ç»“æœå·²ç¼“å­˜: {result_uri}")
    
    # ç”ŸæˆVIKORæ ¼å¼åŒ–æŠ¥å‘Šå¹¶æ·»åŠ åˆ°ç»“æœä¸­ - ç›´æ¥ä½¿ç”¨æ¨¡æ¿å­—ç¬¦ä¸²é¿å…LLMåŠ å·¥
    # æ„å»ºVIKORç»“æœè¡¨æ ¼
    comprehensive_scores = result.get("comprehensive_scores", {})
    vikor_table = "| ä¸šåŠ¡å¯¹è±¡ | Så€¼(ç¾¤ä½“æ•ˆç”¨) | Rå€¼(ä¸ªä½“é—æ†¾) | Qå€¼(å¦¥åè§£) | æ’åº |\n"
    vikor_table += "|----------|---------------|---------------|------------|------|\n"
    
    # è®¡ç®—Qå€¼æ’åº
    q_values = []
    for obj_name, scores in comprehensive_scores.items():
        if scores["Q"]:
            q_values.append((obj_name, scores["Q"][0]))
    
    # æŒ‰Qå€¼å‡åºæ’åˆ—ï¼ˆQå€¼è¶Šå°è¶Šå¥½ï¼‰
    q_values.sort(key=lambda x: x[1])
    
    for rank, (obj_name, q_val) in enumerate(q_values, 1):
        scores = comprehensive_scores[obj_name]
        s_val = scores["S"][0] if scores["S"] else 0
        r_val = scores["R"][0] if scores["R"] else 0
        vikor_table += f"| {obj_name} | {s_val:.4f} | {r_val:.4f} | {q_val:.4f} | {rank} |\n"
    
    # ç”Ÿæˆå¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•å’Œä¸‹ä¸€æ­¥å»ºè®®
    available_tables = """
- **VIKORç»¼åˆè¯„ä¼°è¡¨**ï¼šåŒ…å«Sã€Rã€Qå€¼çš„å®Œæ•´è¯„ä¼°ç»“æœ
- **å¦¥åæ’åºè¡¨**ï¼šåŸºäºQå€¼çš„æ–¹æ¡ˆæ’åºç»“æœ
- **æ•æ„Ÿæ€§åˆ†æè¡¨**ï¼šä¸åŒç­–ç•¥æƒé‡vä¸‹çš„ç»“æœå¯¹æ¯”
"""
    
    next_actions = f"""
1. **æŸ¥çœ‹è¯¦ç»†æ’åºç»“æœ**ï¼šåˆ†æQå€¼æ’åºï¼Œè¯†åˆ«æœ€ä¼˜å¦¥åè§£
2. **æ•æ„Ÿæ€§åˆ†æ**ï¼šè°ƒæ•´ç­–ç•¥æƒé‡vï¼ˆå½“å‰v={v}ï¼‰ï¼Œè§‚å¯Ÿç»“æœç¨³å®šæ€§
3. **ç»“æœéªŒè¯**ï¼šç»“åˆSå€¼å’ŒRå€¼ï¼ŒéªŒè¯å¦¥åè§£çš„åˆç†æ€§
4. **å†³ç­–æ”¯æŒ**ï¼šåŸºäºQå€¼æ’åºï¼Œä¸ºæœ€ç»ˆå†³ç­–æä¾›é‡åŒ–ä¾æ®

ğŸ’¡ **VIKORç»“æœè§£è¯»**ï¼š
- **Qå€¼è¶Šå°**ï¼šå¦¥åè§£è¶Šä¼˜ï¼Œæ¨èåº¦è¶Šé«˜
- **Så€¼**ï¼šç¾¤ä½“æ•ˆç”¨ï¼Œè¶Šå°è¡¨ç¤ºæ•´ä½“è¡¨ç°è¶Šå¥½
- **Rå€¼**ï¼šä¸ªä½“é—æ†¾ï¼Œè¶Šå°è¡¨ç¤ºæœ€å·®å±æ€§è¡¨ç°è¶Šå¥½
"""
    
    result["formatted_report"] = f"""## ğŸ“Š è¯„ä¼°æ¦‚è¿°
- **è¯„ä¼°æ–¹æ³•**ï¼šVIKORï¼ˆå¤šå‡†åˆ™å¦¥åæ’åºæ³•ï¼‰
- **è¯„ä¼°å¯¹è±¡æ•°é‡**ï¼š{summary.get("total_objects", 0)} ä¸ª
- **è¯„ä»·å› å­æ•°é‡**ï¼š{summary.get("total_factors", 0)} ä¸ª
- **æƒé‡åˆ†é…**ï¼š{summary.get("weight_distribution", [])}
- **ç­–ç•¥æƒé‡**ï¼šv = {v}ï¼ˆ0=ä¿å®ˆç­–ç•¥ï¼Œ1=æ¿€è¿›ç­–ç•¥ï¼Œ0.5=å¹³è¡¡ç­–ç•¥ï¼‰

## ğŸ“ˆ å¦¥åæ’åºç»“æœï¼ˆQå€¼æ’åºï¼‰
{vikor_table}

## ğŸ“‹ å¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
{available_tables}

## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®
{next_actions}"""
    
    return result


def _convert_membership_to_normalized_matrix(membership_df: pd.DataFrame) -> Dict[str, np.ndarray]:
    """
    å°†éš¶å±åº¦çŸ©é˜µè½¬æ¢ä¸ºè§„èŒƒåŒ–ç‰¹å¾å€¼çŸ©é˜µæ ¼å¼
    
    Args:
        membership_df: éš¶å±åº¦çŸ©é˜µDataFrameï¼ˆæ‰å¹³åŒ–æ ¼å¼ï¼‰
        
    Returns:
        è§„èŒƒåŒ–ç‰¹å¾å€¼çŸ©é˜µå­—å…¸ï¼Œæ ¼å¼ä¸º{"T1": çŸ©é˜µ, "T2": çŸ©é˜µ, ...}
    """
    # æŒ‰ä¸šåŠ¡å¯¹è±¡åˆ†ç»„
    grouped = membership_df.groupby('ä¸šåŠ¡å¯¹è±¡')
    
    normalized_matrices = {}
    
    for obj_name, group in grouped:
        # æŒ‰è¯„ä»·å› å­åˆ†ç»„
        factor_groups = group.groupby('è¯„ä»·å› å­')
        
        matrix = []
        factor_names = []
        
        for factor_name, factor_group in factor_groups:
            # æŒ‰çº§åˆ«æ’åº
            factor_group = factor_group.sort_values('è¯„ä¼°çº§åˆ«')
            
            # æå–éš¶å±åº¦å€¼ä½œä¸ºä¸€è¡Œ
            row = factor_group['éš¶å±åº¦'].tolist()
            matrix.append(row)
            factor_names.append(factor_name)
        
        if matrix:
            normalized_matrices[obj_name] = np.array(matrix)
    
    return normalized_matrices


if __name__ == "__main__":
    # è¿è¡Œ MCP æœåŠ¡å™¨
    mcp.run(transport="stdio")