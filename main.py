"""
MCP æ™ºèƒ½æ•°æ®åˆ†æå·¥å…·

ç‰ˆæœ¬ï¼šv0.9.0-MVP
çŠ¶æ€ï¼šMVPé˜¶æ®µ - å†…å­˜æ¨¡å¼ï¼ŒåŠŸèƒ½å®Œæ•´éªŒè¯é€šè¿‡

åŠŸèƒ½æ¦‚è¿°ï¼š
- åŸºäº FastMCP æ¡†æ¶çš„æ™ºèƒ½æ•°æ®åˆ†ææœåŠ¡å™¨
- æä¾›æ•°æ®ä¸Šä¼ ã€å­—æ®µåˆ†æã€ææ€§è°ƒæ•´ã€éš¶å±åº¦è®¡ç®—ç­‰å®Œæ•´å·¥ä½œæµ
- MVPé˜¶æ®µé‡‡ç”¨å†…å­˜ç¼“å­˜æœºåˆ¶ï¼Œæ”¯æŒä¼šè¯å†…æ•°æ®ç®¡ç†
- ä¸‰å±‚æ¶æ„è®¾è®¡ï¼šæ•°æ®å±‚ã€ç®—æ³•å±‚ã€äº¤äº’å±‚

æ ¸å¿ƒç‰¹æ€§ï¼š
âœ… æ•°æ®ä¸Šä¼ ä¸è§£æï¼ˆCSV/Excelæ ¼å¼ï¼‰
âœ… æ™ºèƒ½å­—æ®µåˆ†æä¸ææ€§æ£€æµ‹
âœ… ä¸‹ç•Œå‹éš¶å±åº¦å‡½æ•°è®¡ç®—
âœ… TOPSIS/VIKORå¤šå‡†åˆ™å†³ç­–ç®—æ³•
âœ… å†…å­˜èµ„æºç®¡ç†ç³»ç»Ÿï¼ˆMVPé˜¶æ®µï¼‰
âœ… èµ„æºä¾èµ–é“¾è·Ÿè¸ªä¸æ ¼å¼å…¼å®¹

æŠ€æœ¯æ ˆï¼š
- FastMCP + æ ‡å‡† MCP Server æ··åˆå®ç°
- pandas + numpy æ•°æ®å¤„ç†
- æ¨¡ç³Šæ•°å­¦ + å¤šå‡†åˆ™å†³ç­–ç®—æ³•
- å†…å­˜ç¼“å­˜æœºåˆ¶ï¼ˆMVPé˜¶æ®µï¼‰

ä½œè€…ï¼šliuhongjiang
"""

# =========================================================================================
# ğŸ“¦ ä¾èµ–å¯¼å…¥ä¸åŸºç¡€é…ç½®
# =========================================================================================
## å¯¼å…¥æ¨¡å—
import io
import uuid
import pandas as pd
import numpy as np
import os
import json
import pickle
from typing import Dict, Any, Optional, List
from mcp.server.fastmcp import FastMCP
from modules.data_layer.field_analyzer import analyze_numeric_fields, analyze_categorical_fields, auto_detect_polarity, apply_polarity_adjustment, generate_polarity_report
from modules.algorithm_layer.membership_functions import LowerBoundMembershipFunctions
from modules.algorithm_layer.topsis_comprehensive_evaluation import TOPSISComprehensiveEvaluation
from modules.algorithm_layer.vikor_comprehensive_evaluation import VIKORComprehensiveEvaluation
from modules.algorithm_layer.grade_comprehensive_assessment import GradeComprehensiveAssessment

## åˆ›å»º MCP æœåŠ¡å™¨
mcp = FastMCP("SmartDataAnalyzer")

## å®šä¹‰å¸¸é‡
### æ•°æ®ç¼“å­˜ï¼ˆå†…å­˜ç¼“å­˜ï¼ŒSession çº§ï¼‰
DATA_CACHE: Dict[str, pd.DataFrame] = {}

### æŒä¹…åŒ–å­˜å‚¨ç›®å½•
PERSISTENT_STORAGE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "persistent_resources")

### èµ„æºç´¢å¼•æ–‡ä»¶è·¯å¾„
RESOURCE_INDEX_FILE = os.path.join(PERSISTENT_STORAGE_DIR, "resource_index.json")

### åŠ è½½èµ„æºç´¢å¼•ï¼ˆMVPé˜¶æ®µï¼šä¸åŠ è½½ç£ç›˜ç´¢å¼•æ–‡ä»¶ï¼‰
RESOURCE_INDEX = {}

### èµ„æºå‘½åè§„åˆ™å¸¸é‡
RESOURCE_TYPES = {
    "raw_data": "raw",           # åŸå§‹æ•°æ®ï¼ˆç”¨æˆ·ä¸Šä¼ ï¼‰
    "field_analysis": "fa",      # å­—æ®µåˆ†æç»“æœ
    "membership_calc": "mc",     # éš¶å±åº¦è®¡ç®—ç»“æœï¼ˆç›´æ¥ä½¿ç”¨rawæ•°æ®ï¼‰
    "multi_criteria": "mcr",     # å¤šå‡†åˆ™è®¡ç®—ç»“æœ
    "binary_semantic": "bs",     # äºŒå…ƒè¯­ä¹‰åˆ†å‰²ç»“æœ
    "other": "other"             # å…¶ä»–è®¡ç®—ç»“æœ
}

# =========================================================================================
# ğŸ—„ï¸ æ•°æ®å­˜å‚¨ä¸ç¼“å­˜ç®¡ç†
# =========================================================================================
### ä»æŒä¹…åŒ–å­˜å‚¨åŠ è½½èµ„æº
def load_resource_from_persistent_storage(resource_id: str) -> pd.DataFrame:
    """
    ä»æŒä¹…åŒ–å­˜å‚¨åŠ è½½èµ„æºï¼ˆMVPé˜¶æ®µï¼šä»…ä»å†…å­˜åŠ è½½ï¼‰
    
    Args:
        resource_id: èµ„æºIDï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        
    Returns:
        åŠ è½½çš„DataFrame
        
    Raises:
        ValueError: èµ„æºä¸å­˜åœ¨æˆ–ä¸åœ¨å†…å­˜ä¸­
    """
    # å¦‚æœæ²¡æœ‰æä¾›resource_idï¼Œå°è¯•è‡ªåŠ¨å‘ç°mcèµ„æº
    if resource_id is None:
        resource_id = find_latest_membership_resource()
        if resource_id is None:
            raise ValueError("æœªæ‰¾åˆ°éš¶å±åº¦è®¡ç®—ç»“æœèµ„æºï¼ˆmcå¼€å¤´çš„èµ„æºï¼‰ï¼Œè¯·å…ˆæ‰§è¡Œéš¶å±åº¦è®¡ç®—")
        print(f"è‡ªåŠ¨å‘ç°éš¶å±åº¦èµ„æº: {resource_id}")
    
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id and resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    if resource_id not in RESOURCE_INDEX:
        raise ValueError(f"èµ„æºä¸å­˜åœ¨äºå­˜å‚¨: {resource_id}")
    
    # MVPé˜¶æ®µï¼šä»…ä»å†…å­˜ç¼“å­˜åŠ è½½
    uri = RESOURCE_INDEX[resource_id]["uri"]
    if uri not in DATA_CACHE:
        raise ValueError(f"èµ„æºä¸åœ¨å†…å­˜ç¼“å­˜ä¸­ï¼ˆMVPé˜¶æ®µä¸æ”¯æŒç£ç›˜åŠ è½½ï¼‰: {resource_id}")
    
    return DATA_CACHE[uri]

### åˆ—å‡ºå­˜å‚¨ä¸­çš„æ‰€æœ‰èµ„æº
@mcp.tool()
def list_persistent_resources(resource_type: str = None) -> Dict[str, Any]:
    """
    åˆ—å‡ºå­˜å‚¨ä¸­çš„æ‰€æœ‰èµ„æºï¼ˆMVPé˜¶æ®µï¼šä»…æ˜¾ç¤ºå†…å­˜ä¸­çš„èµ„æºï¼‰
    
    Args:
        resource_type: èµ„æºç±»å‹ç­›é€‰ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        å†…å­˜èµ„æºåˆ—è¡¨
    """
    resources_by_type = {}
    
    for resource_id, resource_info in RESOURCE_INDEX.items():
        res_type = resource_info.get("type", "æœªçŸ¥ç±»å‹")
        
        if resource_type and res_type != resource_type:
            continue
            
        if res_type not in resources_by_type:
            resources_by_type[res_type] = []
        
        # æ£€æŸ¥èµ„æºæ˜¯å¦åœ¨å†…å­˜ä¸­
        is_in_memory = resource_info.get("uri", "") in DATA_CACHE
        
        resources_by_type[res_type].append({
            "resource_id": resource_id,
            "uri": resource_info.get("uri", ""),
            "data_shape": resource_info.get("data_shape", "æœªçŸ¥"),
            "created_time": resource_info.get("created_time", "æœªçŸ¥"),
            "file_path": resource_info.get("file_path", ""),
            "parent_hash": resource_info.get("parent_hash", ""),
            "step_name": resource_info.get("step_name", ""),
            "in_memory": is_in_memory,
            "storage_type": "å†…å­˜ç¼“å­˜" if is_in_memory else "ä»…ç´¢å¼•ï¼ˆMVPé˜¶æ®µä¸æŒä¹…åŒ–ï¼‰"
        })
    
    return {
        "total_resources": len(RESOURCE_INDEX),
        "resources_by_type": resources_by_type,
        "resource_types_found": list(resources_by_type.keys()),
        "storage_mode": "MVPé˜¶æ®µï¼šä»…å†…å­˜å­˜å‚¨ï¼Œä¸æŒä¹…åŒ–åˆ°ç£ç›˜"
    }

### åˆ é™¤å­˜å‚¨ä¸­çš„æŒ‡å®šèµ„æº
@mcp.tool()
def delete_persistent_resource(resource_id: str) -> Dict[str, Any]:
    """
    åˆ é™¤å­˜å‚¨ä¸­çš„æŒ‡å®šèµ„æºï¼ˆMVPé˜¶æ®µï¼šä»…ä»å†…å­˜å’Œç´¢å¼•ä¸­åˆ é™¤ï¼‰
    
    Args:
        resource_id: è¦åˆ é™¤çš„èµ„æºIDï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        
    Returns:
        åˆ é™¤æ“ä½œç»“æœ
    """
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    if resource_id not in RESOURCE_INDEX:
        return {
            "success": False,
            "message": f"èµ„æºä¸å­˜åœ¨: {resource_id}"
        }
    
    try:
        # MVPé˜¶æ®µï¼šä¸åˆ é™¤ç£ç›˜æ–‡ä»¶ï¼ˆå› ä¸ºæ²¡æœ‰æŒä¹…åŒ–ï¼‰
        # file_path = RESOURCE_INDEX[resource_id]["file_path"]
        # if os.path.exists(file_path):
        #     os.remove(file_path)
        
        # ä»å†…å­˜ç¼“å­˜ä¸­åˆ é™¤
        uri = RESOURCE_INDEX[resource_id]["uri"]
        if uri in DATA_CACHE:
            del DATA_CACHE[uri]
        
        # ä»ç´¢å¼•ä¸­åˆ é™¤
        del RESOURCE_INDEX[resource_id]
        
        # MVPé˜¶æ®µï¼šä¸ä¿å­˜ç´¢å¼•æ–‡ä»¶åˆ°ç£ç›˜
        # with open(RESOURCE_INDEX_FILE, 'w', encoding='utf-8') as f:
        #     json.dump(RESOURCE_INDEX, f, ensure_ascii=False, indent=2)
        
        return {
            "success": True,
            "message": f"èµ„æº {resource_id} å·²ä»å†…å­˜ä¸­åˆ é™¤ï¼ˆMVPé˜¶æ®µä¸æŒä¹…åŒ–åˆ°ç£ç›˜ï¼‰"
        }
        
    except Exception as e:
        return {
            "success": False,
            "message": f"åˆ é™¤èµ„æºå¤±è´¥: {str(e)}"
        }

### ä¿å­˜èµ„æºåˆ°æŒä¹…åŒ–å­˜å‚¨ï¼ˆMVPé˜¶æ®µï¼šä»…å†…å­˜å­˜å‚¨ï¼Œä¸æŒä¹…åŒ–åˆ°æ–‡ä»¶ï¼‰
def save_resource_to_persistent_storage(resource_id: str, df: pd.DataFrame) -> str:
    """
    å°†èµ„æºä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨ï¼ˆMVPé˜¶æ®µï¼šä»…å†…å­˜å­˜å‚¨ï¼Œä¸æŒä¹…åŒ–åˆ°æ–‡ä»¶ï¼‰
    
    Args:
        resource_id: èµ„æºID
        df: è¦ä¿å­˜çš„DataFrame
        
    Returns:
        è™šæ‹Ÿæ–‡ä»¶è·¯å¾„ï¼ˆä»…ç”¨äºå…¼å®¹ï¼‰
    """
    # MVPé˜¶æ®µï¼šæ›´æ–°å†…å­˜ç´¢å¼•å’Œæ•°æ®ç¼“å­˜
    parsed_info = parse_resource_id(resource_id)
    uri = get_resource_uri(resource_id)
    
    # æ›´æ–°èµ„æºç´¢å¼•
    RESOURCE_INDEX[resource_id] = {
        "uri": uri,
        "type": parsed_info["type"],
        "type_code": parsed_info["type_code"],
        "data_shape": f"{len(df)} è¡Œ Ã— {len(df.columns)} åˆ—",
        "file_path": f"memory://{resource_id}",  # è™šæ‹Ÿå†…å­˜è·¯å¾„
        "created_time": pd.Timestamp.now().isoformat(),
        "parent_hash": parsed_info.get("parent_hash", ""),
        "step_name": parsed_info.get("step_name", "")
    }
    
    # ä¿å­˜åˆ°å†…å­˜ç¼“å­˜
    DATA_CACHE[uri] = df.copy()
    
    # ä¸ä¿å­˜ç´¢å¼•æ–‡ä»¶åˆ°ç£ç›˜ï¼ˆMVPé˜¶æ®µï¼‰
    # with open(RESOURCE_INDEX_FILE, 'w', encoding='utf-8') as f:
    #     json.dump(RESOURCE_INDEX, f, ensure_ascii=False, indent=2)
    
    return f"memory://{resource_id}"


# =========================================================================================
# ğŸ”§ èµ„æºç®¡ç†
# =========================================================================================
### ç”Ÿæˆæ ‡å‡†åŒ–çš„èµ„æºID
def generate_resource_id(resource_type: str, parent_resource_id: str = None, step_name: str = None) -> str:
    """
    ç”Ÿæˆæ ‡å‡†åŒ–çš„èµ„æºID
    
    Args:
        resource_type: èµ„æºç±»å‹ï¼ˆä½¿ç”¨RESOURCE_TYPESä¸­çš„é”®ï¼‰
        parent_resource_id: çˆ¶èµ„æºIDï¼ˆç”¨äºå»ºç«‹ä¾èµ–å…³ç³»ï¼‰
        step_name: æ­¥éª¤åç§°ï¼ˆå¯é€‰ï¼Œç”¨äºæè¿°æ€§æ ‡è¯†ï¼‰
        
    Returns:
        æ ‡å‡†åŒ–çš„èµ„æºIDæ ¼å¼ï¼š{type}_{uuid}_{parent_hash}_{step}
    """
    import hashlib
    import uuid
    
    # éªŒè¯èµ„æºç±»å‹
    if resource_type not in RESOURCE_TYPES:
        resource_type = "other"
    
    type_prefix = RESOURCE_TYPES[resource_type]
    
    # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
    unique_id = str(uuid.uuid4())[:8]
    
    # è®¡ç®—çˆ¶èµ„æºå“ˆå¸Œï¼ˆç”¨äºå»ºç«‹ä¾èµ–é“¾ï¼‰
    parent_hash = ""
    if parent_resource_id:
        parent_hash = hashlib.md5(parent_resource_id.encode()).hexdigest()[:6]
    
    # æ­¥éª¤åç§°å¤„ç†
    step_suffix = ""
    if step_name:
        # æ¸…ç†æ­¥éª¤åç§°ï¼Œåªä¿ç•™å­—æ¯æ•°å­—
        step_suffix = "_" + "".join(c for c in step_name if c.isalnum()).lower()[:10]
    
    # æ„å»ºå®Œæ•´èµ„æºID
    resource_id = f"{type_prefix}_{unique_id}"
    if parent_hash:
        resource_id += f"_{parent_hash}"
    if step_suffix:
        resource_id += step_suffix
    
    return resource_id

### æ ¹æ®èµ„æºIDç”Ÿæˆæ ‡å‡†URI
def get_resource_uri(resource_id: str) -> str:
    """
    æ ¹æ®èµ„æºIDç”Ÿæˆæ ‡å‡†URI
    
    Args:
        resource_id: æ ‡å‡†åŒ–çš„èµ„æºID
        
    Returns:
        èµ„æºURIæ ¼å¼ï¼šdata://{resource_id}
    """
    return f"data://{resource_id}"

### è§£æèµ„æºIDï¼Œæå–ç±»å‹ã€çˆ¶èµ„æºç­‰ä¿¡æ¯
def parse_resource_id(resource_id: str) -> Dict[str, str]:
    """
    è§£æèµ„æºIDï¼Œæå–ç±»å‹ã€çˆ¶èµ„æºç­‰ä¿¡æ¯
    
    Args:
        resource_id: æ ‡å‡†åŒ–çš„èµ„æºID
        
    Returns:
        è§£æåçš„èµ„æºä¿¡æ¯
    """
    parts = resource_id.split("_")
    
    if len(parts) < 2:
        return {"type": "unknown", "id": resource_id}
    
    type_code = parts[0]
    resource_type = "unknown"
    
    # åå‘æŸ¥æ‰¾ç±»å‹
    for type_name, code in RESOURCE_TYPES.items():
        if code == type_code:
            resource_type = type_name
            break
    
    result = {
        "type": resource_type,
        "type_code": type_code,
        "unique_id": parts[1],
        "id": resource_id
    }
    
    # è§£æçˆ¶èµ„æºå“ˆå¸Œ
    if len(parts) > 2 and len(parts[2]) == 6:
        result["parent_hash"] = parts[2]
    
    # è§£ææ­¥éª¤åç§°
    if len(parts) > 3:
        result["step_name"] = parts[3]
    
    return result

### è·å–èµ„æºçš„ä¾èµ–é“¾ä¿¡æ¯ - è¿½è¸ªèµ„æºç”Ÿæˆè·¯å¾„
@mcp.tool()
def get_resource_dependency_chain(resource_id: str) -> Dict[str, Any]:
    """
    è·å–èµ„æºçš„ä¾èµ–é“¾ä¿¡æ¯ - è¿½è¸ªèµ„æºç”Ÿæˆè·¯å¾„
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - åˆ†ææŒ‡å®šèµ„æºçš„å®Œæ•´ç”Ÿæˆè·¯å¾„å’Œä¾èµ–å…³ç³»
    - æ”¯æŒçº¯IDå’Œå®Œæ•´URIä¸¤ç§æ ¼å¼çš„èµ„æºæ ‡è¯†ç¬¦
    - è‡ªåŠ¨å¤„ç†èµ„æºIDæ ¼å¼å…¼å®¹æ€§ï¼ˆç§»é™¤data://å‰ç¼€ï¼‰
    
    åº”ç”¨åœºæ™¯ï¼š
    - è°ƒè¯•èµ„æºç”Ÿæˆæµç¨‹
    - åˆ†ææ•°æ®å¤„ç†é“¾
    - èµ„æºå…³ç³»å¯è§†åŒ–
    
    Args:
        resource_id: èµ„æºæ ‡è¯†ç¬¦ï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        
    Returns:
        Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
        - resource_id: åŸå§‹èµ„æºID
        - parsed_info: è§£æåçš„èµ„æºä¿¡æ¯
        - dependency_chain: ä¾èµ–é“¾åˆ—è¡¨ï¼ˆä»å½“å‰èµ„æºåˆ°æ ¹èµ„æºï¼‰
        - chain_length: ä¾èµ–é“¾é•¿åº¦
        
    ç¤ºä¾‹ï¼š
    >>> get_resource_dependency_chain("raw_abc123")
    {
        "resource_id": "raw_abc123",
        "parsed_info": {...},
        "dependency_chain": [...],
        "chain_length": 1
    }
    """
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    parsed_info = parse_resource_id(resource_id)
    
    # è·å–æ‰€æœ‰èµ„æºä¿¡æ¯
    all_resources = {}
    for uri in DATA_CACHE.keys():
        if uri.startswith("data://"):
            res_id = uri[7:]  # ç§»é™¤ "data://" å‰ç¼€
            all_resources[res_id] = parse_resource_id(res_id)
    
    # æ„å»ºä¾èµ–é“¾
    dependency_chain = []
    current_resource = parsed_info
    
    while current_resource:
        dependency_chain.append(current_resource)
        
        # æŸ¥æ‰¾çˆ¶èµ„æº
        parent_hash = current_resource.get("parent_hash")
        if not parent_hash:
            break
            
        # åœ¨æ‰€æœ‰èµ„æºä¸­æŸ¥æ‰¾åŒ¹é…çš„çˆ¶èµ„æº
        parent_resource = None
        for res_id, res_info in all_resources.items():
            if res_info.get("unique_id") == parent_hash:
                parent_resource = res_info
                break
        
        if not parent_resource:
            break
            
        current_resource = parent_resource
    
    return {
        "resource_id": resource_id,
        "parsed_info": parsed_info,
        "dependency_chain": dependency_chain,
        "chain_length": len(dependency_chain)
    }


# =========================================================================================
# ğŸ“Š æ•°æ®ä¸Šä¼ ä¸è§£ææ¨¡å—
# =========================================================================================
### ä¸Šä¼  CSV æ–‡æœ¬æ•°æ®å¹¶ç¼“å­˜ä¸ºèµ„æºï¼Œè‡ªåŠ¨è§¦å‘å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹
@mcp.tool()
def upload_csv(csv_text: str) -> Dict[str, Any]:
    """
    ä¸Šä¼  CSV æ–‡æœ¬æ•°æ®å¹¶ç¼“å­˜ä¸ºèµ„æºï¼Œè‡ªåŠ¨è§¦å‘å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹
    
    Args:
        csv_text: CSV æ ¼å¼çš„æ–‡æœ¬å†…å®¹
        
    Returns:
        åŒ…å«åŸå§‹èµ„æºURIå’Œææ€§æ£€æµ‹ç»“æœçš„å­—å…¸
    """
    try:
        # ä½¿ç”¨ pandas è§£æ CSV æ–‡æœ¬
        df = pd.read_csv(io.StringIO(csv_text))
        
        # ä½¿ç”¨æ ‡å‡†åŒ–çš„èµ„æºå‘½åè§„åˆ™
        resource_id = generate_resource_id("raw_data")
        uri = get_resource_uri(resource_id)
        
        # ç¼“å­˜æ•°æ®åˆ°å†…å­˜
        DATA_CACHE[uri] = df
        
        # æ›´æ–°èµ„æºç´¢å¼•ï¼ˆä¿®å¤èµ„æºä¸å­˜åœ¨çš„æ ¹æœ¬é—®é¢˜ï¼‰
        RESOURCE_INDEX[resource_id] = {
            'uri': uri,
            'type': 'raw_data',
            'rows': len(df),
            'columns': len(df.columns),
            'column_names': list(df.columns)
        }
        
        # MVPé˜¶æ®µï¼šä¸ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨ï¼ˆå·²åœ¨å†…å­˜ä¸­ï¼‰
        # save_resource_to_persistent_storage(resource_id, df)
        
        # è®°å½•ä¸Šä¼ ä¿¡æ¯
        print(f"æ•°æ®ä¸Šä¼ æˆåŠŸ: {uri}")
        print(f"èµ„æºç±»å‹: åŸå§‹æ•°æ® (raw_data)")
        print(f"æ•°æ®é›†ä¿¡æ¯: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        print(f"åˆ—å: {list(df.columns)}")
        print(f"âœ… èµ„æºå·²ç¼“å­˜åˆ°å†…å­˜ï¼ˆMVPé˜¶æ®µä¸æŒä¹…åŒ–åˆ°ç£ç›˜ï¼‰")
        
        # è‡ªåŠ¨è§¦å‘å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹
        print("\nğŸ” å¼€å§‹å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹...")
        
        # åˆ†ææ•°å€¼å­—æ®µ
        numeric_analysis = analyze_numeric_fields(df)
        
        if not numeric_analysis:
            print("âš ï¸  æœªå‘ç°æ•°å€¼å‹å­—æ®µï¼Œè·³è¿‡ææ€§æ£€æµ‹")
            return {
                "original_resource_uri": uri,
                "polarity_detection": {
                    "status": "skipped",
                    "message": "æœªå‘ç°æ•°å€¼å‹å­—æ®µï¼Œæ— éœ€ææ€§æ£€æµ‹"
                },
                "adjusted_resource_uri": None,
                "next_actions": [
                    "åˆ†æè¿™äº›æ•°æ®å­—æ®µçš„ç‰¹å¾ï¼ˆä½¿ç”¨analyze_data_fieldså·¥å…·ï¼‰",
                    "æŸ¥çœ‹æ•°æ®å†…å®¹ï¼ˆä½¿ç”¨export_resource_to_csvå·¥å…·ï¼‰"
                ]
            }
        
        # æ£€æµ‹å­—æ®µææ€§
        polarity_results = auto_detect_polarity(df, numeric_analysis)
        
        # ç”Ÿæˆææ€§æ£€æµ‹æŠ¥å‘Š
        polarity_report = generate_polarity_report(polarity_results)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ³•è¯„ä¼°çš„å­—æ®µ
        failed_detections = [col for col, result in polarity_results.items() if not result['detection_successful']]
        
        if failed_detections:
            print("âŒ å‘ç°æ— æ³•è¯„ä¼°ææ€§çš„å­—æ®µ")
            print(polarity_report)
            
            # æ„å»ºææ€§é…ç½®æ¨¡æ¿
            polarity_config = {}
            for col, result in polarity_results.items():
                if result['detection_successful']:
                    polarity_config[col] = result['suggested_polarity']
            
            return {
                "original_resource_uri": uri,
                "polarity_detection": {
                    "status": "requires_confirmation",
                    "message": "å‘ç°æ— æ³•è¯„ä¼°ææ€§çš„å­—æ®µï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤ææ€§é…ç½®",
                    "failed_fields": failed_detections,
                    "detected_polarities": polarity_config,
                    "report": polarity_report
                },
                "adjusted_resource_uri": None,
                "next_actions": [
                    "ä½¿ç”¨apply_polarity_adjustmentå·¥å…·æ‰‹åŠ¨ç¡®è®¤ææ€§é…ç½®",
                    "ä¿®æ”¹è¡¨å¤´åç§°ï¼ŒåŒ…å«æ˜ç¡®çš„ææ€§æŒ‡ç¤ºè¯",
                    "æŸ¥çœ‹æ•°æ®å†…å®¹ï¼ˆä½¿ç”¨export_resource_to_csvå·¥å…·ï¼‰"
                ]
            }
        
        # æ‰€æœ‰å­—æ®µææ€§æ£€æµ‹æˆåŠŸï¼Œè‡ªåŠ¨åº”ç”¨è°ƒæ•´
        print("ğŸ”„ åº”ç”¨ææ€§è°ƒæ•´ç­–ç•¥...")
        adjusted_df = apply_polarity_adjustment(df, polarity_results)
        
        # ä¿å­˜è°ƒæ•´åçš„æ•°æ®ä¸ºç¼“å­˜èµ„æº
        adjusted_resource_id = generate_resource_id("raw_data", resource_id, "polarity_adjusted")
        adjusted_uri = get_resource_uri(adjusted_resource_id)
        
        # ç¼“å­˜è°ƒæ•´åçš„æ•°æ®
        DATA_CACHE[adjusted_uri] = adjusted_df
        
        # æ›´æ–°èµ„æºç´¢å¼•ï¼ˆä¿®å¤ææ€§è°ƒæ•´åèµ„æºä¸å­˜åœ¨çš„æ ¹æœ¬é—®é¢˜ï¼‰
        RESOURCE_INDEX[adjusted_resource_id] = {
            'uri': adjusted_uri,
            'type': 'polarity_adjusted',
            'rows': len(adjusted_df),
            'columns': len(adjusted_df.columns),
            'column_names': list(adjusted_df.columns),
            'parent_resource_id': resource_id
        }
        
        # MVPé˜¶æ®µï¼šä¸ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨ï¼ˆå·²åœ¨å†…å­˜ä¸­ï¼‰
        # save_resource_to_persistent_storage(adjusted_resource_id, adjusted_df)
        
        print(f"âœ… ææ€§è°ƒæ•´åçš„æ•°æ®å·²ç¼“å­˜: {adjusted_uri}")
        
        # è¾“å‡ºææ€§æ£€æµ‹æŠ¥å‘Š
        print("\n" + "="*60)
        print("ğŸ“Š å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹å®Œæˆ")
        print("="*60)
        print(polarity_report)
        
        # è¾“å‡ºè°ƒæ•´ç­–ç•¥ä¿¡æ¯
        print("\nğŸ”„ ææ€§è°ƒæ•´ç­–ç•¥å·²åº”ç”¨ï¼š")
        for col, result in polarity_results.items():
            if result['suggested_polarity'] == 'max':
                print(f"   - {col}: è¶Šå¤§è¶Šå¥½ â†’ è¶Šå°è¶Šå¥½ï¼ˆé€šè¿‡å–å€’æ•°è½¬æ¢ï¼‰")
            else:
                print(f"   - {col}: è¶Šå°è¶Šå¥½ï¼ˆæ— éœ€è°ƒæ•´ï¼‰")
        
        print(f"\nğŸ’¾ è°ƒæ•´åæ•°æ®å·²ç¼“å­˜ä¸º: {adjusted_uri}")
        
        return {
            "original_resource_uri": uri,
            "polarity_detection": {
                "status": "success",
                "message": "å­—æ®µææ€§æ£€æµ‹å®Œæˆï¼Œææ€§è°ƒæ•´å·²åº”ç”¨",
                "report": polarity_report,
                "polarity_results": polarity_results
            },
            "adjusted_resource_uri": adjusted_uri,
            "next_actions": [
                "åˆ†æè¿™äº›æ•°æ®å­—æ®µçš„ç‰¹å¾ï¼ˆä½¿ç”¨analyze_data_fieldså·¥å…·ï¼‰",
                "ç”Ÿæˆéš¶å±åº¦è®¡ç®—é…ç½®æ¨¡æ¿ï¼ˆä½¿ç”¨generate_membership_config_templateå·¥å…·ï¼‰",
                "æŸ¥çœ‹æ•°æ®å†…å®¹ï¼ˆä½¿ç”¨export_resource_to_csvå·¥å…·ï¼‰"
            ]
        }
        
    except Exception as e:
        raise ValueError(f"CSV è§£æå¤±è´¥: {str(e)}")

### ä¸Šä¼  Excel æ–‡ä»¶æ•°æ®å¹¶ç¼“å­˜ä¸ºèµ„æºï¼Œè‡ªåŠ¨è§¦å‘å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹
@mcp.tool()
def upload_excel(file_content: str, sheet_name: Optional[str] = None) -> Dict[str, Any]:
    """
    ä¸Šä¼  Excel æ–‡ä»¶æ•°æ®å¹¶ç¼“å­˜ä¸ºèµ„æºï¼Œè‡ªåŠ¨è§¦å‘å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹
    
    Args:
        file_content: Base64 ç¼–ç çš„ Excel æ–‡ä»¶å†…å®¹
        sheet_name: å·¥ä½œè¡¨åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºç¬¬ä¸€ä¸ªå·¥ä½œè¡¨ï¼‰
        
    Returns:
        åŒ…å«åŸå§‹èµ„æºURIå’Œææ€§æ£€æµ‹ç»“æœçš„å­—å…¸
    """
    try:
        # è§£ç  Base64 å†…å®¹
        import base64
        excel_bytes = base64.b64decode(file_content)
        
        # ä½¿ç”¨ pandas è§£æ Excel æ–‡ä»¶
        if sheet_name:
            df = pd.read_excel(io.BytesIO(excel_bytes), sheet_name=sheet_name)
        else:
            df = pd.read_excel(io.BytesIO(excel_bytes))
        
        # ä½¿ç”¨æ ‡å‡†åŒ–çš„èµ„æºå‘½åè§„åˆ™
        resource_id = generate_resource_id("raw_data")
        uri = get_resource_uri(resource_id)
        
        # ç¼“å­˜æ•°æ®åˆ°å†…å­˜
        DATA_CACHE[uri] = df
        
        # æ›´æ–°èµ„æºç´¢å¼•ï¼ˆä¿®å¤Excelä¸Šä¼ èµ„æºä¸å­˜åœ¨çš„æ ¹æœ¬é—®é¢˜ï¼‰
        RESOURCE_INDEX[resource_id] = {
            'uri': uri,
            'type': 'raw_data',
            'rows': len(df),
            'columns': len(df.columns),
            'column_names': list(df.columns)
        }
        
        # MVPé˜¶æ®µï¼šä¸ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨ï¼ˆå·²åœ¨å†…å­˜ä¸­ï¼‰
        # save_resource_to_persistent_storage(resource_id, df)
        
        # è®°å½•ä¸Šä¼ ä¿¡æ¯
        print(f"Excel æ•°æ®ä¸Šä¼ æˆåŠŸ: {uri}")
        print(f"èµ„æºç±»å‹: åŸå§‹æ•°æ® (raw_data)")
        print(f"æ•°æ®é›†ä¿¡æ¯: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        print(f"åˆ—å: {list(df.columns)}")
        if sheet_name:
            print(f"å·¥ä½œè¡¨: {sheet_name}")
        print(f"âœ… èµ„æºå·²ç¼“å­˜åˆ°å†…å­˜ï¼ˆMVPé˜¶æ®µä¸æŒä¹…åŒ–åˆ°ç£ç›˜ï¼‰")
        
        # è‡ªåŠ¨è§¦å‘å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹
        print("\nğŸ” å¼€å§‹å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹...")
        
        # åˆ†ææ•°å€¼å­—æ®µ
        numeric_analysis = analyze_numeric_fields(df)
        
        if not numeric_analysis:
            print("âš ï¸  æœªå‘ç°æ•°å€¼å‹å­—æ®µï¼Œè·³è¿‡ææ€§æ£€æµ‹")
            return {
                "original_resource_uri": uri,
                "polarity_detection": {
                    "status": "skipped",
                    "message": "æœªå‘ç°æ•°å€¼å‹å­—æ®µï¼Œæ— éœ€ææ€§æ£€æµ‹"
                },
                "adjusted_resource_uri": None,
                "next_actions": [
                    "åˆ†æè¿™äº›æ•°æ®å­—æ®µçš„ç‰¹å¾ï¼ˆä½¿ç”¨analyze_data_fieldså·¥å…·ï¼‰",
                    "æŸ¥çœ‹æ•°æ®å†…å®¹ï¼ˆä½¿ç”¨export_resource_to_csvå·¥å…·ï¼‰"
                ]
            }
        
        # æ£€æµ‹å­—æ®µææ€§
        polarity_results = auto_detect_polarity(df, numeric_analysis)
        
        # ç”Ÿæˆææ€§æ£€æµ‹æŠ¥å‘Š
        polarity_report = generate_polarity_report(polarity_results)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ³•è¯„ä¼°çš„å­—æ®µ
        failed_detections = [col for col, result in polarity_results.items() if not result['detection_successful']]
        
        if failed_detections:
            print("âŒ å‘ç°æ— æ³•è¯„ä¼°ææ€§çš„å­—æ®µ")
            print(polarity_report)
            
            # æ„å»ºææ€§é…ç½®æ¨¡æ¿
            polarity_config = {}
            for col, result in polarity_results.items():
                if result['detection_successful']:
                    polarity_config[col] = result['suggested_polarity']
            
            return {
                "original_resource_uri": uri,
                "polarity_detection": {
                    "status": "requires_confirmation",
                    "message": "å‘ç°æ— æ³•è¯„ä¼°ææ€§çš„å­—æ®µï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤ææ€§é…ç½®",
                    "failed_fields": failed_detections,
                    "detected_polarities": polarity_config,
                    "report": polarity_report
                },
                "adjusted_resource_uri": None,
                "next_actions": [
                    "ä½¿ç”¨apply_polarity_adjustment_toolæ‰‹åŠ¨ç¡®è®¤ææ€§é…ç½®",
                    "ä¿®æ”¹è¡¨å¤´åç§°ï¼ŒåŒ…å«æ˜ç¡®çš„ææ€§æŒ‡ç¤ºè¯",
                    "æŸ¥çœ‹æ•°æ®å†…å®¹ï¼ˆä½¿ç”¨export_resource_to_csvå·¥å…·ï¼‰"
                ]
            }
        
        # åº”ç”¨ææ€§è°ƒæ•´
        print("ğŸ”„ åº”ç”¨ææ€§è°ƒæ•´ç­–ç•¥...")
        adjusted_df = apply_polarity_adjustment(df, polarity_results)
        
        # ä¿å­˜è°ƒæ•´åçš„æ•°æ®ä¸ºç¼“å­˜èµ„æº
        adjusted_resource_id = generate_resource_id("raw_data", resource_id, "polarity_adjusted")
        adjusted_uri = get_resource_uri(adjusted_resource_id)
        
        # ç¼“å­˜è°ƒæ•´åçš„æ•°æ®
        DATA_CACHE[adjusted_uri] = adjusted_df
        
        # æ›´æ–°èµ„æºç´¢å¼•ï¼ˆä¿®å¤ææ€§è°ƒæ•´åèµ„æºä¸å­˜åœ¨çš„æ ¹æœ¬é—®é¢˜ï¼‰
        RESOURCE_INDEX[adjusted_resource_id] = {
            'uri': adjusted_uri,
            'type': 'polarity_adjusted',
            'rows': len(adjusted_df),
            'columns': len(adjusted_df.columns),
            'column_names': list(adjusted_df.columns),
            'parent_resource_id': resource_id
        }
        
        # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
        save_resource_to_persistent_storage(adjusted_resource_id, adjusted_df)
        
        print(f"âœ… ææ€§è°ƒæ•´åçš„æ•°æ®å·²ä¿å­˜: {adjusted_uri}")
        
        # è¾“å‡ºææ€§æ£€æµ‹æŠ¥å‘Š
        print("\n" + "="*60)
        print("ğŸ“Š å­—æ®µææ€§æ™ºèƒ½æ£€æµ‹å®Œæˆ")
        print("="*60)
        print(polarity_report)
        
        # è¾“å‡ºè°ƒæ•´ç­–ç•¥ä¿¡æ¯
        print("\nğŸ”„ ææ€§è°ƒæ•´ç­–ç•¥å·²åº”ç”¨ï¼š")
        for col, result in polarity_results.items():
            if result['suggested_polarity'] == 'max':
                print(f"   - {col}: è¶Šå¤§è¶Šå¥½ â†’ è¶Šå°è¶Šå¥½ï¼ˆé€šè¿‡å–å€’æ•°è½¬æ¢ï¼‰")
            else:
                print(f"   - {col}: è¶Šå°è¶Šå¥½ï¼ˆæ— éœ€è°ƒæ•´ï¼‰")
        
        print(f"\nğŸ’¾ è°ƒæ•´åæ•°æ®å·²ä¿å­˜ä¸º: {adjusted_uri}")
        
        return {
            "original_resource_uri": uri,
            "polarity_detection": {
                "status": "success",
                "message": "å­—æ®µææ€§æ£€æµ‹å®Œæˆï¼Œææ€§è°ƒæ•´å·²åº”ç”¨",
                "report": polarity_report,
                "polarity_results": polarity_results
            },
            "adjusted_resource_uri": adjusted_uri,
            "next_actions": [
                "åˆ†æè¿™äº›æ•°æ®å­—æ®µçš„ç‰¹å¾ï¼ˆä½¿ç”¨analyze_data_fieldså·¥å…·ï¼‰",
                "ç”Ÿæˆéš¶å±åº¦è®¡ç®—é…ç½®æ¨¡æ¿ï¼ˆä½¿ç”¨generate_membership_config_templateå·¥å…·ï¼‰",
                "æŸ¥çœ‹æ•°æ®å†…å®¹ï¼ˆä½¿ç”¨export_resource_to_csvå·¥å…·ï¼‰"
            ]
        }
        
    except Exception as e:
        raise ValueError(f"Excel è§£æå¤±è´¥: {str(e)}")

### é€šè¿‡ URI è®¿é—®å·²ç¼“å­˜çš„æ•°æ®èµ„æº
@mcp.resource("data://{resource_id}")
def get_data_resource(resource_id: str) -> Dict[str, Any]:
    """
    é€šè¿‡ URI è®¿é—®å·²ç¼“å­˜çš„æ•°æ®èµ„æº
    
    Args:
        resource_id: èµ„æºæ ‡è¯†ç¬¦ï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        
    Returns:
        æ•°æ®é›†çš„å­—å…¸è¡¨ç¤ºï¼ˆå‰100è¡Œé¢„è§ˆï¼‰ï¼Œç”¨markdownåŠ è½½æ•°æ®é¢„è§ˆ
    """
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    uri = f"data://{resource_id}"
    
    if uri not in DATA_CACHE:
        raise ValueError(f"èµ„æºä¸å­˜åœ¨: {uri}")
    
    df = DATA_CACHE[uri]
    
    # è¿”å›æ•°æ®é¢„è§ˆï¼ˆå‰100è¡Œï¼‰
    preview_data = df.head(100).to_dict(orient='records')
    
    return {
        "resource_uri": uri,
        "dataset_info": {
            "rows": len(df),
            "columns": len(df.columns),
            "column_names": list(df.columns),
            "data_types": {col: str(df[col].dtype) for col in df.columns}
        },
        "preview_data": preview_data
    }

### åˆ—å‡ºå½“å‰ç¼“å­˜çš„æ‰€æœ‰æ•°æ®èµ„æº
@mcp.tool()
def list_data_resources() -> Dict[str, Any]:
    """
    åˆ—å‡ºå½“å‰ç¼“å­˜çš„æ‰€æœ‰æ•°æ®èµ„æº
    
    Returns:
        èµ„æºåˆ—è¡¨å’Œç»Ÿè®¡ä¿¡æ¯
    """
    resources_info = {}
    
    for uri, df in DATA_CACHE.items():
        resources_info[uri] = {
            "rows": len(df),
            "columns": len(df.columns),
            "column_names": list(df.columns)
        }
    
    return {
        "total_resources": len(DATA_CACHE),
        "resources": resources_info
    }

### æŒ‰ç±»å‹åˆ—å‡ºèµ„æº
@mcp.tool()
def list_resources_by_type(resource_type: str = None) -> Dict[str, Any]:
    """
    æŒ‰ç±»å‹åˆ—å‡ºèµ„æº
    
    Args:
        resource_type: èµ„æºç±»å‹ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™åˆ—å‡ºæ‰€æœ‰ï¼‰
        
    Returns:
        æŒ‰ç±»å‹åˆ†ç±»çš„èµ„æºåˆ—è¡¨
    """
    resources_by_type = {}
    
    for uri, df in DATA_CACHE.items():
        if uri.startswith("data://"):
            resource_id = uri[7:]  # ç§»é™¤ "data://" å‰ç¼€
            parsed_info = parse_resource_id(resource_id)
            
            res_type = parsed_info["type"]
            
            if resource_type and res_type != resource_type:
                continue
                
            if res_type not in resources_by_type:
                resources_by_type[res_type] = []
            
            resources_by_type[res_type].append({
                "resource_id": resource_id,
                "uri": uri,
                "data_shape": f"{len(df)} è¡Œ Ã— {len(df.columns)} åˆ—",
                "parsed_info": parsed_info
            })
    
    return {
        "total_resources": len(DATA_CACHE),
        "resources_by_type": resources_by_type,
        "resource_types_found": list(resources_by_type.keys())
    }

### æ¸…ç©ºæ•°æ®ç¼“å­˜
@mcp.tool()
def clear_data_cache() -> Dict[str, Any]:
    """
    æ¸…ç©ºæ•°æ®ç¼“å­˜
    
    Returns:
        æ“ä½œç»“æœ
    """
    cache_size = len(DATA_CACHE)
    DATA_CACHE.clear()
    
    return {
        "success": True,
        "message": f"å·²æ¸…ç©º {cache_size} ä¸ªæ•°æ®èµ„æº"
    } 


# =========================================================================================
# ğŸ” å­—æ®µåˆ†æä¸ææ€§æ£€æµ‹æ¨¡å—
# =========================================================================================
### åˆ†ææ•°æ®é›†çš„å­—æ®µç‰¹å¾
@mcp.tool()
def analyze_data_fields(resource_id: str, cache_result: bool = True) -> Dict[str, Any]:
    """
    åˆ†ææ•°æ®é›†çš„å­—æ®µç‰¹å¾ï¼Œè¿”å›å›ºå®šçš„åˆ†æç»“æœ
    
    **åˆ†æå†…å®¹**:
    - å­—æ®µç±»å‹è¯†åˆ«ï¼ˆæ•°å€¼å‹/åˆ†ç±»å‹ï¼‰
    - å¸¸è§ç»Ÿè®¡ç‰¹å¾ï¼ˆæœ€å°å€¼ã€æœ€å¤§å€¼ã€å¹³å‡å€¼ã€æ ‡å‡†å·®ç­‰ï¼‰
    - ç¼ºå¤±å€¼åˆ†æï¼ˆç¼ºå¤±æ•°é‡ã€ç¼ºå¤±ç‡ï¼‰
    - æ•°æ®è´¨é‡è¯„ä¼°
    - å­—æ®µææ€§è‡ªåŠ¨æ£€æµ‹å’Œå»ºè®®
    
    **é‡è¦æç¤º**:
    - åç»­çš„éš¶å±åº¦è®¡ç®—å’Œå¤šå‡†åˆ™åˆ†æè¦æ±‚æ‰€æœ‰å­—æ®µçš„ææ€§å¿…é¡»ä¸€è‡´
    - è¯·ä»”ç»†æ£€æŸ¥ææ€§å»ºè®®ï¼Œç¡®ä¿æ•°æ®ç¬¦åˆè®¡ç®—è¦æ±‚
    
    Args:
        resource_id: èµ„æºæ ‡è¯†ç¬¦ï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        cache_result: æ˜¯å¦ç¼“å­˜åˆ†æç»“æœï¼ˆé»˜è®¤Trueï¼‰
        
    Returns:
        Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„åˆ†æç»“æœï¼š
        - dataset_info: æ•°æ®é›†åŸºæœ¬ä¿¡æ¯
        - field_analysis: å­—æ®µåˆ†æç»“æœ
        - data_quality_summary: æ•°æ®è´¨é‡æ‘˜è¦
        - analysis_summary: åˆ†ææ€»ç»“
        - polarity_warnings: ææ€§ä¸€è‡´æ€§è­¦å‘Š
        - result_resource_id: åˆ†æç»“æœèµ„æºIDï¼ˆå¦‚æœç¼“å­˜äº†ç»“æœï¼‰
    """
    # å¦‚æœæ²¡æœ‰æä¾›resource_idï¼Œå°è¯•è‡ªåŠ¨å‘ç°mcèµ„æº
    if resource_id is None:
        resource_id = find_latest_membership_resource()
        if resource_id is None:
            raise ValueError("æœªæ‰¾åˆ°å¯ç”¨çš„éš¶å±åº¦èµ„æº(mc_å¼€å¤´)ï¼Œè¯·å…ˆæ‰§è¡Œéš¶å±åº¦è®¡ç®—æˆ–ä¸Šä¼ éš¶å±åº¦æ•°æ®")
        print(f"ğŸ¤– è‡ªåŠ¨å‘ç°éš¶å±åº¦èµ„æº: {resource_id}")
    
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id and resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    uri = get_resource_uri(resource_id)
    
    if uri not in DATA_CACHE:
        raise ValueError(f"èµ„æºä¸å­˜åœ¨: {uri}")
    
    df = DATA_CACHE[uri]
    
    # æ‰§è¡Œå­—æ®µåˆ†æ
    numeric_analysis = analyze_numeric_fields(df)
    categorical_analysis = analyze_categorical_fields(df)
    
    # è‡ªåŠ¨æ£€æµ‹å­—æ®µææ€§
    polarity_suggestions = auto_detect_polarity(df, numeric_analysis)
    
    # æ£€æŸ¥ææ€§ä¸€è‡´æ€§å¹¶ç”Ÿæˆè­¦å‘Š
    polarity_warnings = check_polarity_consistency(polarity_suggestions)
    
    # ç”Ÿæˆæ•°æ®è´¨é‡æ‘˜è¦
    data_quality_summary = generate_data_quality_summary(df)
    
    # ç”Ÿæˆåˆ†ææ€»ç»“ï¼ˆåŒ…å«ææ€§æ£€æŸ¥ç»“æœï¼‰
    analysis_summary = generate_analysis_summary(df, numeric_analysis, categorical_analysis, data_quality_summary, polarity_warnings)
    
    # æ„å»ºåˆ†æç»“æœ
    analysis_result = {
        "dataset_info": {
            "resource_uri": uri,
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "column_names": list(df.columns)
        },
        "field_analysis": {
            "numeric_fields": numeric_analysis,
            "categorical_fields": categorical_analysis,
            "field_polarity": polarity_suggestions
        },
        "data_quality_summary": data_quality_summary,
        "analysis_summary": analysis_summary,
        "polarity_warnings": polarity_warnings
    }
    
    # å¦‚æœéœ€è¦ç¼“å­˜ç»“æœ
    if cache_result:
        # ç”Ÿæˆåˆ†æç»“æœèµ„æºID
        result_resource_id = generate_resource_id(
            "field_analysis", 
            parent_resource_id=resource_id,
            step_name="field_analysis"
        )
        result_uri = get_resource_uri(result_resource_id)
        
        # å°†åˆ†æç»“æœè½¬æ¢ä¸ºDataFrameæ ¼å¼ç¼“å­˜
        # è¿™é‡Œæˆ‘ä»¬ç¼“å­˜ä¸€ä¸ªåŒ…å«åˆ†ææ‘˜è¦çš„ç®€åŒ–DataFrame
        analysis_df = pd.DataFrame([{
            'analysis_type': 'field_analysis',
            'parent_resource': resource_id,
            'numeric_fields_count': len(numeric_analysis),
            'categorical_fields_count': len(categorical_analysis),
            'overall_missing_rate': data_quality_summary['overall_quality']['overall_missing_rate'],
            'quality_rating': data_quality_summary['overall_quality']['quality_rating'],
            'polarity_consistency': polarity_warnings['is_consistent']
        }])
        
        DATA_CACHE[result_uri] = analysis_df
        analysis_result["result_resource_id"] = result_resource_id
        
        print(f"å­—æ®µåˆ†æç»“æœå·²ç¼“å­˜: {result_uri}")
    
    return analysis_result

### æ£€æŸ¥å­—æ®µææ€§ä¸€è‡´æ€§
def check_polarity_consistency(polarity_suggestions: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ£€æŸ¥å­—æ®µææ€§ä¸€è‡´æ€§
    
    Args:
        polarity_suggestions: å­—æ®µææ€§å»ºè®®å­—å…¸
        
    Returns:
        ææ€§ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ
    """
    if not polarity_suggestions:
        return {
            "is_consistent": True,
            "message": "æ— æ•°å€¼å­—æ®µï¼Œæ— éœ€ææ€§æ£€æŸ¥",
            "polarity_distribution": {},
            "warnings": []
        }
    
    # ç»Ÿè®¡ææ€§åˆ†å¸ƒ
    polarity_counts = {}
    for field_info in polarity_suggestions.values():
        polarity = field_info.get('suggested_polarity', 'unknown')
        polarity_counts[polarity] = polarity_counts.get(polarity, 0) + 1
    
    # æ£€æŸ¥ä¸€è‡´æ€§
    is_consistent = len(polarity_counts) <= 1
    
    # ç”Ÿæˆè­¦å‘Šä¿¡æ¯
    warnings = []
    if not is_consistent:
        warnings.append("âš ï¸ æ£€æµ‹åˆ°æ··åˆææ€§å­—æ®µï¼åç»­è®¡ç®—å¯èƒ½äº§ç”Ÿé”™è¯¯ç»“æœ")
        warnings.append("   å»ºè®®ç»Ÿä¸€æ‰€æœ‰å­—æ®µçš„ææ€§ï¼ˆå…¨éƒ¨ä¸ºæå¤§å‹æˆ–æå°å‹ï¼‰")
    
    if 'unknown' in polarity_counts:
        warnings.append("âš ï¸ éƒ¨åˆ†å­—æ®µææ€§æ— æ³•è‡ªåŠ¨è¯†åˆ«ï¼Œè¯·æ‰‹åŠ¨ç¡®è®¤")
    
    return {
        "is_consistent": is_consistent,
        "message": "ææ€§ä¸€è‡´" if is_consistent else "ææ€§ä¸ä¸€è‡´",
        "polarity_distribution": polarity_counts,
        "warnings": warnings
    }

### ç”Ÿæˆæ•°æ®è´¨é‡æ‘˜è¦
def generate_data_quality_summary(df: pd.DataFrame) -> Dict[str, Any]:
    """ç”Ÿæˆæ•°æ®è´¨é‡æ‘˜è¦"""
    
    total_rows = len(df)
    total_columns = len(df.columns)
    
    # è®¡ç®—æ€»ä½“ç¼ºå¤±æƒ…å†µ
    total_missing = df.isnull().sum().sum()
    total_cells = total_rows * total_columns
    overall_missing_rate = (total_missing / total_cells) * 100 if total_cells > 0 else 0
    
    # æŒ‰åˆ—ç»Ÿè®¡ç¼ºå¤±æƒ…å†µ
    column_missing_stats = {}
    for col in df.columns:
        missing_count = df[col].isnull().sum()
        missing_rate = (missing_count / total_rows) * 100
        column_missing_stats[col] = {
            "missing_count": int(missing_count),
            "missing_rate": float(missing_rate),
            "data_type": str(df[col].dtype)
        }
    
    # æ•°æ®è´¨é‡è¯„çº§
    if overall_missing_rate < 1:
        quality_rating = "ä¼˜ç§€"
    elif overall_missing_rate < 5:
        quality_rating = "è‰¯å¥½"
    elif overall_missing_rate < 10:
        quality_rating = "ä¸€èˆ¬"
    else:
        quality_rating = "è¾ƒå·®"
    
    return {
        "overall_quality": {
            "total_rows": total_rows,
            "total_columns": total_columns,
            "total_missing_cells": int(total_missing),
            "overall_missing_rate": float(overall_missing_rate),
            "quality_rating": quality_rating
        },
        "column_quality": column_missing_stats
    }

### åº”ç”¨ææ€§è°ƒæ•´å·¥å…·
@mcp.tool()
def apply_polarity_adjustment_tool(original_resource_uri: str, polarity_config: Dict[str, str]) -> Dict[str, Any]:
    """
    æ ¹æ®ç”¨æˆ·ç¡®è®¤çš„ææ€§é…ç½®åº”ç”¨ææ€§è°ƒæ•´
    
    Args:
        original_resource_uri: åŸå§‹æ•°æ®èµ„æºURI
        polarity_config: ææ€§é…ç½®å­—å…¸ï¼Œæ ¼å¼å¦‚ {"å­—æ®µå": "min/max"}
        
    Returns:
        ææ€§è°ƒæ•´ç»“æœï¼ŒåŒ…å«è°ƒæ•´åèµ„æºURIå’Œè°ƒæ•´è¯¦æƒ…
    """
    try:
        # ä»URIæå–èµ„æºID
        if not original_resource_uri.startswith("data://"):
            raise ValueError("èµ„æºURIæ ¼å¼ä¸æ­£ç¡®ï¼Œåº”ä»¥'data://'å¼€å¤´")
        
        resource_id = original_resource_uri[7:]  # ç§»é™¤ "data://" å‰ç¼€
        
        # æ£€æŸ¥èµ„æºæ˜¯å¦å­˜åœ¨
        if original_resource_uri not in DATA_CACHE:
            # å°è¯•ä»æŒä¹…åŒ–å­˜å‚¨åŠ è½½
            try:
                df = load_resource_from_persistent_storage(resource_id)
                DATA_CACHE[original_resource_uri] = df
            except Exception as e:
                raise ValueError(f"èµ„æºä¸å­˜åœ¨æˆ–æ— æ³•åŠ è½½: {original_resource_uri}")
        
        df = DATA_CACHE[original_resource_uri]
        
        print(f"ğŸ” å¼€å§‹åº”ç”¨ææ€§è°ƒæ•´...")
        print(f"åŸå§‹èµ„æº: {original_resource_uri}")
        print(f"ææ€§é…ç½®: {polarity_config}")
        
        # éªŒè¯ææ€§é…ç½®
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        for col in polarity_config.keys():
            if col not in numeric_columns:
                raise ValueError(f"å­—æ®µ '{col}' ä¸å­˜åœ¨æˆ–ä¸æ˜¯æ•°å€¼å‹å­—æ®µ")
        
        # æ„å»ºææ€§ç»“æœç»“æ„
        polarity_results = {}
        for col, polarity in polarity_config.items():
            series = df[col].dropna()
            min_val = float(series.min())
            max_val = float(series.max())
            
            # è®¡ç®—éš¶å±åº¦å‚æ•°
            if polarity == 'max':
                a = min_val
                c = max_val
                b = (a + c) / 2
            else:  # min
                a = max_val
                c = min_val
                b = (a + c) / 2
            
            polarity_results[col] = {
                'suggested_polarity': polarity,
                'confidence': 'user_confirmed',
                'reasoning': 'ç”¨æˆ·æ‰‹åŠ¨ç¡®è®¤ææ€§é…ç½®',
                'membership_rules': f"""
    ## å¯¹äº{'è¶Šå¤§è¶Šå¥½' if polarity == 'max' else 'è¶Šå°è¶Šå¥½'}çš„å­—æ®µï¼ˆ{col}ï¼‰ï¼š
    - {'a < b < c' if polarity == 'max' else 'a > b > c'} ï¼ˆå¦‚{col}ï¼š{a:.2f}â†’{b:.2f}â†’{c:.2f}ï¼‰
    - å½“å®é™…å€¼ {'â‰¤' if polarity == 'max' else 'â‰¥'} aæ—¶ï¼šéš¶å±åº¦=0%
    - å½“å®é™…å€¼ {'â‰¥' if polarity == 'max' else 'â‰¤'} cæ—¶ï¼šéš¶å±åº¦=100%
    """.strip(),
                'adjustment_strategy': f"ææ€§è°ƒæ•´æ–¹æ³•ï¼šé€šè¿‡çº¿æ€§å˜æ¢çš„æ–¹æ³•å°†è¶Šå¤§è¶Šå¥½çš„å­—æ®µè½¬æ¢ä¸ºè¶Šå°è¶Šå¥½" if polarity == 'max' else "æ— éœ€è°ƒæ•´ï¼Œå·²ä¸ºè¶Šå°è¶Šå¥½ç±»å‹",
                'parameters': {'a': a, 'b': b, 'c': c},
                'detection_successful': True
            }
        
        # åº”ç”¨ææ€§è°ƒæ•´
        print("ğŸ”„ åº”ç”¨ææ€§è°ƒæ•´ç­–ç•¥...")
        adjusted_df = df.copy()
        
        for col, result in polarity_results.items():
            if result['suggested_polarity'] == 'max':
                # å¯¹è¶Šå¤§è¶Šå¥½çš„å­—æ®µä½¿ç”¨çº¿æ€§å˜æ¢ï¼ˆä¿®å¤å–å€’æ•°å¯¼è‡´å€¼è¿‡å°çš„é—®é¢˜ï¼‰
                max_val = df[col].max() * 1.5  # ä½¿ç”¨1.5å€æœ€å¤§å€¼ä½œä¸ºå‚è€ƒ
                adjusted_df[col] = max_val - df[col]
                print(f"âœ… å·²å¯¹å­—æ®µ '{col}' åº”ç”¨ææ€§è°ƒæ•´ï¼ˆçº¿æ€§å˜æ¢: {max_val:.2f} - xï¼‰")
            else:
                print(f"âœ… å­—æ®µ '{col}' ä¸ºè¶Šå°è¶Šå¥½ç±»å‹ï¼Œæ— éœ€è°ƒæ•´")
        
        # ä¿å­˜è°ƒæ•´åçš„æ•°æ®ä¸ºç¼“å­˜èµ„æº
        adjusted_resource_id = generate_resource_id("raw_data", resource_id, "polarity_adjusted")
        adjusted_uri = get_resource_uri(adjusted_resource_id)
        
        # ç¼“å­˜è°ƒæ•´åçš„æ•°æ®
        DATA_CACHE[adjusted_uri] = adjusted_df
        
        # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
        save_resource_to_persistent_storage(adjusted_resource_id, adjusted_df)
        
        print(f"âœ… ææ€§è°ƒæ•´åçš„æ•°æ®å·²ä¿å­˜: {adjusted_uri}")
        
        # ç”Ÿæˆææ€§æ£€æµ‹æŠ¥å‘Š
        polarity_report = generate_polarity_report(polarity_results)
        
        # è¾“å‡ºè°ƒæ•´ç­–ç•¥ä¿¡æ¯
        print("\nğŸ”„ ææ€§è°ƒæ•´ç­–ç•¥å·²åº”ç”¨ï¼š")
        for col, result in polarity_results.items():
            if result['suggested_polarity'] == 'max':
                print(f"   - {col}: è¶Šå¤§è¶Šå¥½ â†’ è¶Šå°è¶Šå¥½ï¼ˆé€šè¿‡å–å€’æ•°è½¬æ¢ï¼‰")
            else:
                print(f"   - {col}: è¶Šå°è¶Šå¥½ï¼ˆæ— éœ€è°ƒæ•´ï¼‰")
        
        print(f"\nğŸ’¾ è°ƒæ•´åæ•°æ®å·²ä¿å­˜ä¸º: {adjusted_uri}")
        
        return {
            "original_resource_uri": original_resource_uri,
            "adjusted_resource_uri": adjusted_uri,
            "polarity_config_applied": polarity_config,
            "polarity_report": polarity_report,
            "next_actions": [
                "åˆ†æè¿™äº›æ•°æ®å­—æ®µçš„ç‰¹å¾ï¼ˆä½¿ç”¨analyze_data_fieldså·¥å…·ï¼‰",
                "ç”Ÿæˆéš¶å±åº¦è®¡ç®—é…ç½®æ¨¡æ¿ï¼ˆä½¿ç”¨generate_membership_config_templateå·¥å…·ï¼‰",
                "æŸ¥çœ‹æ•°æ®å†…å®¹ï¼ˆä½¿ç”¨export_resource_to_csvå·¥å…·ï¼‰"
            ]
        }
        
    except Exception as e:
        raise ValueError(f"ææ€§è°ƒæ•´å¤±è´¥: {str(e)}")


# =========================================================================================
# ğŸ” å­—æ®µåˆ†æä¸ææ€§æ£€æµ‹æ¨¡å—
# =========================================================================================
### ç”Ÿæˆä¸‹ç•Œå‹éš¶å±åº¦è®¡ç®—é…ç½®æ¨¡æ¿
@mcp.tool()
def generate_membership_config_template(resource_id: str) -> str:
    """
    ç”Ÿæˆä¸‹ç•Œå‹éš¶å±åº¦è®¡ç®—é…ç½®æ¨¡æ¿ - è‡ªåŠ¨åŒ–é…ç½®ç”Ÿæˆ
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - åŸºäºæ•°æ®ç‰¹å¾è‡ªåŠ¨ç”Ÿæˆä¸‹ç•Œå‹éš¶å±åº¦å‡½æ•°çš„é…ç½®æ¨¡æ¿
    - æ™ºèƒ½æ£€æµ‹å¹¶ä¼˜å…ˆä½¿ç”¨ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº
    - æ”¯æŒçº¯IDå’Œå®Œæ•´URIä¸¤ç§æ ¼å¼çš„èµ„æºæ ‡è¯†ç¬¦
    - è‡ªåŠ¨å¤„ç†èµ„æºIDæ ¼å¼å…¼å®¹æ€§ï¼ˆç§»é™¤data://å‰ç¼€ï¼‰
    
    ç®—æ³•ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨ä¸‹ç•Œå‹çº§åˆ«éš¶å±å‡½æ•°ï¼ˆåŸºäºå…¬å¼4-11åˆ°4-13ï¼‰
    - ä¸ºæ¯ä¸ªæ•°å€¼å­—æ®µç”Ÿæˆé»˜è®¤çš„3çº§åˆ«å‚æ•°é…ç½®
    - å‚æ•°åŸºäºå­—æ®µç»Ÿè®¡ç‰¹å¾ï¼ˆæœ€å¤§å€¼ã€æœ€å°å€¼ã€å¹³å‡å€¼ï¼‰
    
    å·¥ä½œæµç¨‹ï¼š
    1. æ£€æµ‹æ˜¯å¦å­˜åœ¨ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº
    2. åˆ†ææ•°æ®é›†çš„æ•°å€¼å­—æ®µç‰¹å¾
    3. ä¸ºæ¯ä¸ªå­—æ®µç”Ÿæˆé»˜è®¤çº§åˆ«å‚æ•°
    4. ç”ŸæˆMarkdownæ ¼å¼çš„é…ç½®æ¨¡æ¿
    
    Args:
        resource_id: åŸå§‹æ•°æ®èµ„æºæ ‡è¯†ç¬¦ï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        
    Returns:
        str: Markdownæ ¼å¼çš„ä¸‹ç•Œå‹å‡½æ•°çº§åˆ«å‚æ•°é…ç½®æ¨¡æ¿ï¼ŒåŒ…å«ï¼š
        - èµ„æºä¿¡æ¯æ‘˜è¦
        - å­—æ®µç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼
        - æ¨èé…ç½®æ¨¡æ¿ï¼ˆJSONæ ¼å¼ï¼‰
        - é…ç½®è¯´æ˜å’Œä½¿ç”¨æŒ‡å—
        
    ç¤ºä¾‹è¾“å‡ºï¼š
    ```markdown
    # ä¸‹ç•Œå‹éš¶å±åº¦è®¡ç®—é…ç½®æ¨¡æ¿
    
    ## èµ„æºä¿¡æ¯
    - **èµ„æºID**: `raw_abc123`
    - **æ•°å€¼å­—æ®µ**: age, salary
    - **æ•°æ®å½¢çŠ¶**: 100 è¡Œ Ã— 5 åˆ—
    
    ## å­—æ®µç»Ÿè®¡ä¿¡æ¯
    | å­—æ®µ | æœ€å°å€¼ | æœ€å¤§å€¼ | å¹³å‡å€¼ | æ ‡å‡†å·® |
    |------|--------|--------|--------|--------|
    | age | 20.00 | 60.00 | 35.50 | 8.25 |
    
    ## æ¨èé…ç½®æ¨¡æ¿
    ```json
    {
      "çº§åˆ«å‚æ•°": {
        "age": [48.0, 30.0, 12.0],
        "salary": [80000.0, 50000.0, 20000.0]
      }
    }
    ```
    """
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº
    polarity_adjusted_resource_id = None
    for cached_uri in DATA_CACHE.keys():
        if cached_uri.startswith("data://") and "polarity_adjusted" in cached_uri:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å½“å‰èµ„æºçš„ææ€§è°ƒæ•´ç‰ˆæœ¬
            adjusted_parsed = parse_resource_id(cached_uri[7:])  # ç§»é™¤ "data://" å‰ç¼€
            if (adjusted_parsed["type"] == "raw_data" and 
                adjusted_parsed.get("parent_hash") == resource_id):
                polarity_adjusted_resource_id = cached_uri[7:]  # ç§»é™¤ "data://" å‰ç¼€
                break
    
    # ä¼˜å…ˆä½¿ç”¨ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº
    if polarity_adjusted_resource_id:
        print(f"ğŸ” æ£€æµ‹åˆ°ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº: {polarity_adjusted_resource_id}")
        print("ğŸ“Š å°†ä½¿ç”¨ææ€§è°ƒæ•´åçš„æ•°æ®è¿›è¡Œéš¶å±åº¦è®¡ç®—é…ç½®")
        uri = get_resource_uri(polarity_adjusted_resource_id)
        used_resource_id = polarity_adjusted_resource_id
    else:
        uri = get_resource_uri(resource_id)
        used_resource_id = resource_id
    
    if uri not in DATA_CACHE:
        raise ValueError(f"èµ„æºä¸å­˜åœ¨: {uri}")
    
    df = DATA_CACHE[uri]
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºrawæ•°æ®
    parsed_info = parse_resource_id(used_resource_id)
    if parsed_info["type"] != "raw_data":
        print(f"âš ï¸ è­¦å‘Š: ä½¿ç”¨éåŸå§‹æ•°æ®ç”Ÿæˆé…ç½®æ¨¡æ¿ï¼Œå»ºè®®ä½¿ç”¨rawæ•°æ®")
    
    # è·å–æ•°å€¼å­—æ®µ
    import numpy as np
    numeric_fields = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if not numeric_fields:
        raise ValueError("æ•°æ®é›†ä¸­æ— æ•°å€¼å­—æ®µï¼Œæ— æ³•ç”Ÿæˆéš¶å±åº¦è®¡ç®—é…ç½®")
    
    # ä¸ºæ¯ä¸ªæ•°å€¼å­—æ®µç”Ÿæˆé»˜è®¤çº§åˆ«å‚æ•°ï¼ˆä¸‹ç•Œå‹å‡½æ•°éœ€è¦å¤šä¸ªçº§åˆ«å‚æ•°ï¼‰
    level_params_template = {}
    field_statistics = {}
    for field in numeric_fields:
        field_data = df[field].dropna()
        if len(field_data) > 0:
            min_val = float(field_data.min())
            max_val = float(field_data.max())
            
            # ä¸ºæ¯ä¸ªå­—æ®µç”Ÿæˆé»˜è®¤çº§åˆ«å‚æ•°åˆ—è¡¨ï¼ˆ3ä¸ªçº§åˆ«ï¼‰
            level_params_template[field] = [
                round(max_val * 0.8, 2),  # çº§åˆ«1å‚æ•°
                round(max_val * 0.5, 2),  # çº§åˆ«2å‚æ•°
                round(max_val * 0.2, 2)   # çº§åˆ«3å‚æ•°
            ]
            
            field_statistics[field] = {
                "min": min_val,
                "max": max_val,
                "mean": float(field_data.mean()),
                "std": float(field_data.std())
            }
    
    # ç”ŸæˆMarkdownæ ¼å¼çš„é…ç½®æ¨¡æ¿
    markdown_content = f"""
    # ä¸‹ç•Œå‹éš¶å±åº¦è®¡ç®—é…ç½®æ¨¡æ¿
    ## èµ„æºä¿¡æ¯
    - **èµ„æºID**: `{used_resource_id}`
    - **èµ„æºç±»å‹**: {'ææ€§è°ƒæ•´åçš„æ•°æ®' if polarity_adjusted_resource_id else 'åŸå§‹æ•°æ®'}
    - **æ•°å€¼å­—æ®µ**: {', '.join(numeric_fields)}
    - **æ•°æ®å½¢çŠ¶**: {len(df)} è¡Œ Ã— {len(df.columns)} åˆ—

    ## å­—æ®µç»Ÿè®¡ä¿¡æ¯

    | å­—æ®µ | æœ€å°å€¼ | æœ€å¤§å€¼ | å¹³å‡å€¼ | æ ‡å‡†å·® |
    |------|--------|--------|--------|--------|
    """
    
    # æ·»åŠ å­—æ®µç»Ÿè®¡è¡¨æ ¼è¡Œ
    for field in numeric_fields:
        stats = field_statistics[field]
        markdown_content += f"| {field} | {stats['min']:.2f} | {stats['max']:.2f} | {stats['mean']:.2f} | {stats['std']:.2f} |\n"
    
    markdown_content += """
    ## æ¨èé…ç½®æ¨¡æ¿

    ```json
    {
    "çº§åˆ«å‚æ•°": {
    """
    
    # æ·»åŠ çº§åˆ«å‚æ•°é…ç½®
    for i, field in enumerate(numeric_fields):
        params = level_params_template[field]
        markdown_content += f'    "{field}": {params}'
        if i < len(numeric_fields) - 1:
            markdown_content += ","
        markdown_content += "\n"
    
    markdown_content += """  }
    }
    ```

    ## é…ç½®è¯´æ˜

    ### ä¸‹ç•Œå‹éš¶å±åº¦å‡½æ•°ç‰¹ç‚¹
    - ä½¿ç”¨ä¸‹ç•Œå‹çº§åˆ«éš¶å±å‡½æ•°ï¼ˆåŸºäºå…¬å¼4-11åˆ°4-13ï¼‰
    - æ¯ä¸ªå­—æ®µéœ€è¦é…ç½®çº§åˆ«å‚æ•°åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š`[15, 10, 5]` å¯¹åº”3ä¸ªçº§åˆ«
    - çº§åˆ«å‚æ•°åº”æŒ‰ä»å¤§åˆ°å°é¡ºåºæ’åˆ—
    - æ€»çº§åˆ«æ•°åº”ä¸çº§åˆ«å‚æ•°æ•°é‡ä¸€è‡´

    ### ä½¿ç”¨æµç¨‹
    1. å¤åˆ¶ä¸Šé¢çš„é…ç½®æ¨¡æ¿
    2. æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´çº§åˆ«å‚æ•°
    3. è°ƒç”¨ `validate_membership_config(resource_id, config)` éªŒè¯é…ç½®
    4. éªŒè¯é€šè¿‡åè°ƒç”¨ `calculate_membership_with_config(resource_id, config)` æ‰§è¡Œè®¡ç®—

    ### é…ç½®ç¤ºä¾‹
    ```json
    {
    "çº§åˆ«å‚æ•°": {
        "å­—æ®µ1": [15, 10, 5],
        "å­—æ®µ2": [180, 90, 60]
    }
    }
    ```
    **æ³¨æ„**: é…ç½®ä¸­ä¸å†éœ€è¦"éš¶å±å‡½æ•°"å­—æ®µï¼Œç³»ç»Ÿé»˜è®¤ä½¿ç”¨ä¸‹ç•Œå‹å‡½æ•°ã€‚
    """
    return markdown_content

### éªŒè¯éš¶å±åº¦è®¡ç®—é…ç½®
@mcp.tool()
def validate_membership_config(resource_id: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    éªŒè¯ä¸‹ç•Œå‹éš¶å±åº¦è®¡ç®—é…ç½®
    
    Args:
        resource_id: åŸå§‹æ•°æ®èµ„æºæ ‡è¯†ç¬¦ï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        config: ç”¨æˆ·æä¾›çš„é…ç½®ä¿¡æ¯
        
    Returns:
        éªŒè¯ç»“æœ
    """
    # å¦‚æœæ²¡æœ‰æä¾›resource_idï¼Œå°è¯•è‡ªåŠ¨å‘ç°mcèµ„æº
    if resource_id is None:
        resource_id = find_latest_membership_resource()
        if resource_id is None:
            raise ValueError("æœªæ‰¾åˆ°éš¶å±åº¦è®¡ç®—ç»“æœèµ„æºï¼ˆmcå¼€å¤´çš„èµ„æºï¼‰ï¼Œè¯·å…ˆæ‰§è¡Œéš¶å±åº¦è®¡ç®—")
        print(f"è‡ªåŠ¨å‘ç°éš¶å±åº¦èµ„æº: {resource_id}")
    
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    uri = get_resource_uri(resource_id)
    
    if uri not in DATA_CACHE:
        raise ValueError(f"èµ„æºä¸å­˜åœ¨: {uri}")
    
    df = DATA_CACHE[uri]
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº
    is_polarity_adjusted = "polarity_adjusted" in resource_id
    
    # è·å–æ•°å€¼å­—æ®µ
    import numpy as np
    numeric_fields = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # éªŒè¯é…ç½®ç»“æ„
    errors = []
    warnings = []
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    if "çº§åˆ«å‚æ•°" not in config:
        errors.append("é…ç½®ä¸­ç¼ºå°‘'çº§åˆ«å‚æ•°'å­—æ®µ")
    
    if errors:
        return {
            "is_valid": False,
            "errors": errors,
            "warnings": warnings
        }
    
    # è®¾ç½®éš¶å±å‡½æ•°ç±»å‹ä¸ºä¸‹ç•Œå‹
    membership_type = "ä¸‹ç•Œå‹"
    
    # éªŒè¯çº§åˆ«å‚æ•°
    level_params = config["çº§åˆ«å‚æ•°"]
    if not isinstance(level_params, dict):
        errors.append("'çº§åˆ«å‚æ•°'å­—æ®µå¿…é¡»ä¸ºå­—å…¸ç±»å‹")
    else:
        # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨
        for field in level_params.keys():
            if field not in numeric_fields:
                errors.append(f"å­—æ®µ '{field}' åœ¨æ•°æ®é›†ä¸­ä¸å­˜åœ¨")
        
        # éªŒè¯æ‰€æœ‰å­—æ®µçš„çº§åˆ«å‚æ•°æ•°é‡ä¸€è‡´
        level_counts = {}
        for field, params in level_params.items():
            if field in numeric_fields:
                if not isinstance(params, list):
                    errors.append(f"å­—æ®µ '{field}' çš„çº§åˆ«å‚æ•°å¿…é¡»ä¸ºåˆ—è¡¨")
                else:
                    # æ£€æŸ¥çº§åˆ«å‚æ•°æ•°é‡
                    if len(params) < 2:
                        errors.append(f"å­—æ®µ '{field}' çš„çº§åˆ«å‚æ•°è‡³å°‘éœ€è¦2ä¸ªï¼Œå½“å‰æä¾›{len(params)}ä¸ª")
                    
                    # æ£€æŸ¥çº§åˆ«å‚æ•°æ˜¯å¦æŒ‰ä»å¤§åˆ°å°é¡ºåºæ’åˆ—
                    if len(params) > 1:
                        for i in range(len(params) - 1):
                            # æ£€æŸ¥å‚æ•°aå€¼æ˜¯å¦æŒ‰ä»å¤§åˆ°å°é¡ºåºæ’åˆ—
                            # å¤„ç†ä¸¤ç§æ ¼å¼çš„å‚æ•°ï¼šæ•°å€¼åˆ—è¡¨æˆ–åŒ…å«"a"é”®çš„å­—å…¸åˆ—è¡¨
                            if isinstance(params[i], dict) and "a" in params[i]:
                                current_val = params[i].get("a", 0)
                                next_val = params[i + 1].get("a", 0)
                            else:
                                current_val = params[i]
                                next_val = params[i + 1]
                            
                            if current_val <= next_val:
                                errors.append(f"å­—æ®µ '{field}' çš„çº§åˆ«å‚æ•°åº”æŒ‰ä»å¤§åˆ°å°é¡ºåºæ’åˆ—")
                                break
                    
                    level_counts[field] = len(params)
        
        # æ£€æŸ¥çº§åˆ«å‚æ•°æ•°é‡æ˜¯å¦åœ¨æ‰€æœ‰å­—æ®µä¸­ä¸€è‡´
        if level_counts:
            unique_counts = set(level_counts.values())
            if len(unique_counts) > 1:
                warnings.append(f"å„å­—æ®µçš„çº§åˆ«å‚æ•°æ•°é‡ä¸ä¸€è‡´: {level_counts}ï¼Œå»ºè®®ä¿æŒä¸€è‡´ä»¥è·å¾—æ›´å¥½çš„è¯„ä¼°ç»“æœ")
    
    # éªŒè¯æ€»çº§åˆ«æ•°
    if "æ€»çº§åˆ«æ•°" in config:
        total_levels = config["æ€»çº§åˆ«æ•°"]
        if not isinstance(total_levels, int) or total_levels < 2:
            errors.append("'æ€»çº§åˆ«æ•°'å¿…é¡»ä¸ºå¤§äºç­‰äº2çš„æ•´æ•°")
        elif level_params:
            # æ£€æŸ¥æ€»çº§åˆ«æ•°æ˜¯å¦ä¸çº§åˆ«å‚æ•°æ•°é‡ä¸€è‡´
            first_field_params = next(iter(level_params.values()))
            if isinstance(first_field_params, list) and total_levels != len(first_field_params):
                warnings.append(f"æ€»çº§åˆ«æ•°({total_levels})ä¸ç¬¬ä¸€ä¸ªå­—æ®µçš„çº§åˆ«å‚æ•°æ•°é‡({len(first_field_params)})ä¸ä¸€è‡´")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é…ç½®ä½†æ•°æ®ä¸­ä¸å­˜åœ¨çš„å­—æ®µ
    configured_fields = set(level_params.keys())
    available_fields = set(numeric_fields)
    missing_config = available_fields - configured_fields
    if missing_config:
        warnings.append(f"ä»¥ä¸‹æ•°å€¼å­—æ®µæœªé…ç½®çº§åˆ«å‚æ•°: {list(missing_config)}")
    
    return {
        "is_valid": len(errors) == 0,
        "errors": errors,
        "warnings": warnings,
        "config_summary": {
            "membership_type": membership_type,
            "configured_fields": list(configured_fields),
            "available_fields": numeric_fields,
            "level_params_summary": level_counts if level_params else {}
        }
    }

### æ‰§è¡Œéš¶å±åº¦è®¡ç®—
@mcp.tool()
def calculate_membership_with_config(resource_id: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä½¿ç”¨ä¸‹ç•Œå‹å‡½æ•°é…ç½®ä¿¡æ¯æ‰§è¡Œéš¶å±åº¦è®¡ç®— - å¤šçº§åˆ«æ¨¡ç³Šç»¼åˆè¯„ä»·å·¥å…·
    
    è¯¥å‡½æ•°å®ç°åŸºäºä¸‹ç•Œå‹éš¶å±åº¦å‡½æ•°çš„æ¨¡ç³Šç»¼åˆè¯„ä»·ï¼Œå°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºéš¶å±åº¦çŸ©é˜µæ ¼å¼ï¼Œ
    ä¸ºåç»­çš„å¤šå‡†åˆ™å†³ç­–åˆ†ææä¾›æ ‡å‡†åŒ–çš„è¾“å…¥æ•°æ®ã€‚
    
    Args:
        resource_id: åŸå§‹æ•°æ®èµ„æºæ ‡è¯†ç¬¦ï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
                     ä¾‹å¦‚ï¼š'raw_data_001' æˆ– 'data://raw_data_001'
        config: ç”¨æˆ·æä¾›çš„ä¸‹ç•Œå‹å‡½æ•°é…ç½®ä¿¡æ¯ï¼ŒåŒ…å«å„è¯„ä»·å› å­çš„çº§åˆ«å‚æ•°

    é‡è¦ï¼šå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ¨¡ç‰ˆè¾“å‡ºç»“æœï¼Œä¸å¾—æœ‰æ¨¡ç‰ˆä»¥å¤–çš„ä»»ä½•å…¶å®ƒä¿¡æ¯ï¼ï¼            
    Returns:
        å›å¤æ¨¡ç‰ˆ
        â€˜â€™â€˜
        ## ğŸ“Š éš¶å±åº¦è®¡ç®—æ¦‚è¿°
        - **è®¡ç®—æ–¹æ³•**ï¼šä¸‹ç•Œå‹éš¶å±åº¦å‡½æ•°
        - **ä¸šåŠ¡å¯¹è±¡æ•°é‡**ï¼š3ä¸ª
        - **è¯„ä»·å› å­æ•°é‡**ï¼š4ä¸ª
        - **è¯„ä¼°çº§åˆ«æ•°é‡**ï¼š4çº§ï¼ˆçº§åˆ«1-çº§åˆ«4ï¼‰
        - **éš¶å±åº¦è®¡ç®—åèµ„æºID**ï¼š
        
        ## ğŸ“ˆ éš¶å±åº¦çŸ©é˜µç»“æœï¼ˆè¿™é‡Œä»…å±•ç¤ºä¸€ä¸ªå¯¹è±¡çš„æ•°æ®ï¼Œç”¨æˆ·å¯ä»¥å›å¤è¦æ±‚ä¸‹è½½å…¨éƒ¨æ•°æ®ï¼‰
        
        ### ğŸ“Œ ä¸šåŠ¡å¯¹è±¡1
        | è¯„ä»·å› å­ | çº§åˆ«1 | çº§åˆ«2 | çº§åˆ«3 | çº§åˆ«4 |
        |---------|-------|-------|-------|-------|
        | è¯„ä»·å› å­1 | 1.0000 | 0.8865 | 0.5910 | 0.2955 |
        | è¯„ä»·å› å­2 | 0.7797 | 1.0000 | 0.5619 | 0.3746 |
        | è¯„ä»·å› å­3 | 1.0000 | 0.2836 | 0.1418 | 0.0709 |
        | è¯„ä»·å› å­4 | 1.0000 | 0.8571 | 0.5714 | 0.2857 |
        
        ## ğŸ“‹ å¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
        1. **éš¶å±åº¦çŸ©é˜µ** - æ ¸å¿ƒè®¡ç®—ç»“æœï¼Œå¯ç›´æ¥ç”¨äºTOPSIS/VIKORè¯„ä¼°
        2. **åŸå§‹æ•°æ®è¡¨** - è®¡ç®—å‰çš„åŸå§‹è¾“å…¥æ•°æ®
        3. **é…ç½®å‚æ•°è¡¨** - ä½¿ç”¨çš„éš¶å±åº¦å‡½æ•°å‚æ•°é…ç½®
        
        ## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®
        - ä½¿ç”¨TOPSISæ–¹æ³•è¿›è¡Œç»¼åˆè¯„ä¼°ï¼ˆperform_topsis_comprehensive_evaluationå·¥å…·ï¼‰
        - ä½¿ç”¨VIKORæ–¹æ³•è¿›è¡Œå¦¥åæ’åºåˆ†æï¼ˆperform_vikor_comprehensive_evaluationå·¥å…·ï¼‰
        - æŸ¥çœ‹éš¶å±åº¦çŸ©é˜µè¯¦ç»†ä¿¡æ¯ï¼ˆexport_resource_to_csvå·¥å…·ï¼‰
        - éªŒè¯éš¶å±åº¦è®¡ç®—ç»“æœï¼ˆvalidate_membership_configå·¥å…·ï¼‰
        â€™â€˜â€™
    """
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº
    polarity_adjusted_resource_id = None
    for cached_uri in DATA_CACHE.keys():
        if cached_uri.startswith("data://") and "polarity_adjusted" in cached_uri:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å½“å‰èµ„æºçš„ææ€§è°ƒæ•´ç‰ˆæœ¬
            adjusted_parsed = parse_resource_id(cached_uri[7:])  # ç§»é™¤ "data://" å‰ç¼€
            if (adjusted_parsed["type"] == "raw_data" and 
                adjusted_parsed.get("parent_hash") == resource_id):
                polarity_adjusted_resource_id = cached_uri[7:]  # ç§»é™¤ "data://" å‰ç¼€
                break
    
    # ä¼˜å…ˆä½¿ç”¨ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº
    if polarity_adjusted_resource_id:
        print(f"ğŸ” æ£€æµ‹åˆ°ææ€§è°ƒæ•´åçš„æ•°æ®èµ„æº: {polarity_adjusted_resource_id}")
        print("ğŸ“Š å°†ä½¿ç”¨ææ€§è°ƒæ•´åçš„æ•°æ®è¿›è¡Œéš¶å±åº¦è®¡ç®—")
        used_resource_id = polarity_adjusted_resource_id
    else:
        used_resource_id = resource_id
    
    # è‡ªåŠ¨éªŒè¯é…ç½®
    validation_result = validate_membership_config(used_resource_id, config)
    if not validation_result["is_valid"]:
        raise ValueError(f"é…ç½®éªŒè¯å¤±è´¥: {validation_result['errors']}")
    
    uri = get_resource_uri(used_resource_id)
    df = DATA_CACHE[uri]
    
    # è·å–é…ç½®ä¿¡æ¯
    level_params = config["çº§åˆ«å‚æ•°"]
    
    # è®¡ç®—éš¶å±åº¦çŸ©é˜µ
    membership_results = []
    
    for _, row in df.iterrows():
        membership_row = {}
        for field, params in level_params.items():
            if field in df.columns:
                value = row[field]
                total_levels = len(params)
                
                # è®¡ç®—æ¯ä¸ªçº§åˆ«çš„éš¶å±åº¦
                level_memberships = {}
                # æå–å‚æ•°å€¼åˆ—è¡¨ï¼ˆaå€¼ï¼‰
                # å¤„ç†ä¸¤ç§æ ¼å¼çš„å‚æ•°ï¼šæ•°å€¼åˆ—è¡¨æˆ–åŒ…å«"a"é”®çš„å­—å…¸åˆ—è¡¨
                if isinstance(params[0], dict) and "a" in params[0]:
                    param_values = [p["a"] for p in params]
                else:
                    param_values = params  # ç›´æ¥ä½¿ç”¨æ•°å€¼åˆ—è¡¨
                
                for level in range(1, total_levels + 1):
                    membership = LowerBoundMembershipFunctions.lower_bound_level_membership(
                        value, param_values, level, total_levels
                    )
                    level_memberships[f"çº§åˆ«{level}"] = round(float(membership), 4)
                
                membership_row[field] = level_memberships
        
        membership_results.append(membership_row)
    
    # åˆ›å»ºæ‰å¹³åŒ–çš„éš¶å±åº¦çŸ©é˜µDataFrameï¼ˆç¬¦åˆTOPSISè¦æ±‚æ ¼å¼ï¼‰
    flattened_results = []
    
    for idx, (_, row) in enumerate(df.iterrows()):
        # è·å–ä¸šåŠ¡å¯¹è±¡æ ‡è¯†
        obj_name = f"ä¸šåŠ¡å¯¹è±¡{idx+1}"
        if len(df.columns) > 0:
            # å°è¯•ä½¿ç”¨ç¬¬ä¸€åˆ—çš„å€¼ä½œä¸ºå¯¹è±¡å
            first_col_value = row.iloc[0]
            if pd.notna(first_col_value):
                obj_name = f"ä¸šåŠ¡å¯¹è±¡{idx+1}({str(first_col_value)[:20]})"
        
        # ä¸ºæ¯ä¸ªå­—æ®µå’Œçº§åˆ«åˆ›å»ºæ‰å¹³åŒ–è®°å½•
        for field, params in level_params.items():
            if field in df.columns and field in membership_results[idx]:
                total_levels = len(params)
                for level in range(1, total_levels + 1):
                    level_name = f"çº§åˆ«{level}"
                    membership_value = membership_results[idx][field].get(level_name, 0.0)
                    
                    flattened_results.append({
                        'ä¸šåŠ¡å¯¹è±¡': obj_name,
                        'è¯„ä»·å› å­': field,
                        'è¯„ä¼°çº§åˆ«': level_name,
                        'éš¶å±åº¦': membership_value
                    })
    
    # åˆ›å»ºç¬¦åˆTOPSISè¦æ±‚çš„éš¶å±åº¦çŸ©é˜µ
    membership_df = pd.DataFrame(flattened_results)
    
    # ç”Ÿæˆè®¡ç®—ç»“æœèµ„æºID
    result_resource_id = generate_resource_id(
        "membership_calc", 
        parent_resource_id=resource_id,
        step_name="membership_calculation"
    )
    result_uri = get_resource_uri(result_resource_id)
    
    # ç¼“å­˜è®¡ç®—ç»“æœåˆ°å†…å­˜
    DATA_CACHE[result_uri] = membership_df
    
    # æ›´æ–°èµ„æºç´¢å¼•
    RESOURCE_INDEX[result_resource_id] = {
        'uri': result_uri,
        'type': 'membership_calc',
        'rows': len(membership_df),
        'columns': len(membership_df.columns),
        'column_names': list(membership_df.columns),
        'parent_resource_id': resource_id
    }
    
    # ç”Ÿæˆæ¯ä¸ªä¸šåŠ¡å¯¹è±¡çš„éš¶å±åº¦çŸ©é˜µæ ¼å¼åŒ–è¾“å‡º
    membership_matrices_formatted = []
    for idx, (_, row) in enumerate(df.iterrows()):
        # è·å–ä¸šåŠ¡å¯¹è±¡æ ‡è¯†ï¼ˆä½¿ç”¨ç´¢å¼•æˆ–ç¬¬ä¸€åˆ—ä½œä¸ºæ ‡è¯†ï¼‰
        obj_name = f"ä¸šåŠ¡å¯¹è±¡{idx+1}"
        if len(df.columns) > 0:
            # å°è¯•ä½¿ç”¨ç¬¬ä¸€åˆ—çš„å€¼ä½œä¸ºå¯¹è±¡å
            first_col_value = row.iloc[0]
            if pd.notna(first_col_value):
                obj_name += f"({str(first_col_value)[:20]})"  # æˆªå–å‰20ä¸ªå­—ç¬¦
        
        # æ„å»ºçŸ©é˜µè¡¨æ ¼
        matrix_lines = [f"### ğŸ“Œ {obj_name}"]
        matrix_lines.append("")
        
        # è·å–æ‰€æœ‰çº§åˆ«
        all_levels = []
        for field in level_params.keys():
            if field in df.columns and field in membership_results[idx]:
                levels = list(membership_results[idx][field].keys())
                if levels:
                    all_levels = levels
                    break
        
        # æ„å»ºè¡¨å¤´
        if all_levels:
            header = "| è¯„ä»·å› å­ | " + " | ".join(all_levels) + " |"
            separator = "|---------" + "|-------" * len(all_levels) + "|"
            matrix_lines.append(header)
            matrix_lines.append(separator)
            
            # æ·»åŠ æ¯è¡Œæ•°æ®
            for field in level_params.keys():
                if field in df.columns and field in membership_results[idx]:
                    row_data = [field]
                    for level in all_levels:
                        if level in membership_results[idx][field]:
                            row_data.append(f"{membership_results[idx][field][level]:.4f}")
                        else:
                            row_data.append("0.0000")
                    matrix_lines.append("| " + " | ".join(row_data) + " |")
        
        matrix_lines.append("")  # æ·»åŠ ç©ºè¡Œåˆ†éš”
        membership_matrices_formatted.append("\n".join(matrix_lines))
    
    # è®¡ç®—æ‘˜è¦
    calculation_summary = {
        "input_data_shape": f"{len(df)} è¡Œ Ã— {len(df.columns)} åˆ—",
        "membership_type": "ä¸‹ç•Œå‹",
        "configured_fields": list(level_params.keys()),
        "total_levels": {field: len(params) for field, params in level_params.items()},
        "membership_matrix_shape": f"{len(membership_df)} è¡Œ Ã— {len(membership_df.columns)} åˆ—",
        "calculation_time": "å®æ—¶è®¡ç®—",
        "total_objects": len(df),
        "formatted_matrices": membership_matrices_formatted  # æ·»åŠ æ ¼å¼åŒ–çŸ©é˜µ
    }
    
    # ç”Ÿæˆå¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
    available_tables = """
    1. **éš¶å±åº¦çŸ©é˜µ** - æ ¸å¿ƒè®¡ç®—ç»“æœï¼Œå¯ç›´æ¥ç”¨äºTOPSIS/VIKORè¯„ä¼°
    2. **åŸå§‹æ•°æ®è¡¨** - è®¡ç®—å‰çš„åŸå§‹è¾“å…¥æ•°æ®
    3. **é…ç½®å‚æ•°è¡¨** - ä½¿ç”¨çš„éš¶å±åº¦å‡½æ•°å‚æ•°é…ç½®
    """
    
    # ç”Ÿæˆä¸‹ä¸€æ­¥æ“ä½œå»ºè®®
    next_actions = """
    - ä½¿ç”¨TOPSISæ–¹æ³•è¿›è¡Œç»¼åˆè¯„ä¼°ï¼ˆperform_topsis_comprehensive_evaluationå·¥å…·ï¼‰
    - ä½¿ç”¨VIKORæ–¹æ³•è¿›è¡Œå¦¥åæ’åºåˆ†æï¼ˆperform_vikor_comprehensive_evaluationå·¥å…·ï¼‰
    - æŸ¥çœ‹éš¶å±åº¦çŸ©é˜µè¯¦ç»†ä¿¡æ¯ï¼ˆexport_resource_to_csvå·¥å…·ï¼‰
    - éªŒè¯éš¶å±åº¦è®¡ç®—ç»“æœï¼ˆvalidate_membership_configå·¥å…·ï¼‰
    """
    
    # ç”Ÿæˆæ ¼å¼åŒ–æŠ¥å‘Šå¹¶æ·»åŠ åˆ°ç»“æœä¸­
    result = {
        "membership_matrix": membership_df.to_dict("records"),
        "formatted_membership_matrices": membership_matrices_formatted,  # æ ¼å¼åŒ–çŸ©é˜µè¾“å‡º
        "calculation_summary": calculation_summary,
        "original_resource_uri": get_resource_uri(resource_id),  # åŸå§‹èµ„æºå®Œæ•´URI
        "result_resource_uri": result_uri,  # è®¡ç®—ç»“æœå®Œæ•´URI
        "result_resource_id": result_resource_id,
        "config_used": config,
        "validation_warnings": validation_result["warnings"],
        "next_actions": [
            "ä½¿ç”¨TOPSISæ–¹æ³•è¿›è¡Œç»¼åˆè¯„ä¼°ï¼ˆperform_topsis_comprehensive_evaluationå·¥å…·ï¼‰",
            "ä½¿ç”¨VIKORæ–¹æ³•è¿›è¡Œå¦¥åæ’åºåˆ†æï¼ˆperform_vikor_comprehensive_evaluationå·¥å…·ï¼‰",
            "æŸ¥çœ‹éš¶å±åº¦çŸ©é˜µè¯¦ç»†ä¿¡æ¯ï¼ˆexport_resource_to_csvå·¥å…·ï¼‰",
            "éªŒè¯éš¶å±åº¦è®¡ç®—ç»“æœï¼ˆvalidate_membership_configå·¥å…·ï¼‰"
        ],
        "warnings": [
            "âœ… ä¸‹ç•Œå‹éš¶å±åº¦è®¡ç®—å®Œæˆï¼Œä½¿ç”¨é…ç½®ä¿¡æ¯",
            f"ğŸ“Š è®¡ç®—ç»“æœå·²ç¼“å­˜: {result_uri}",
            f"ğŸ’¾ è®¡ç®—ç»“æœå·²ç¼“å­˜åˆ°å†…å­˜ï¼ˆMVPé˜¶æ®µä¸æŒä¹…åŒ–åˆ°ç£ç›˜ï¼‰",
            "ğŸ’¡ å¯ä½¿ç”¨list_resources_by_type('membership_calc')æŸ¥çœ‹æ‰€æœ‰è®¡ç®—ç»“æœ"
        ]
    }
    
    # ç”Ÿæˆæ ¼å¼åŒ–æŠ¥å‘Š
    result["formatted_report"] = f"""
    ## ğŸ“Š éš¶å±åº¦è®¡ç®—æ¦‚è¿°
    - **è®¡ç®—æ–¹æ³•**ï¼šä¸‹ç•Œå‹éš¶å±åº¦å‡½æ•°
    - **ä¸šåŠ¡å¯¹è±¡æ•°é‡**ï¼š{len(df)} ä¸ª
    - **è¯„ä»·å› å­æ•°é‡**ï¼š{len(level_params)} ä¸ª
    - **è¯„ä¼°çº§åˆ«æ•°é‡**ï¼š{len(all_levels) if all_levels else 4}çº§ï¼ˆ{all_levels[0] if all_levels else 'çº§åˆ«1'}-{all_levels[-1] if all_levels else 'çº§åˆ«4'}ï¼‰
    - **åŸå§‹èµ„æºID**ï¼š`{resource_id}`
    - **éš¶å±åº¦è®¡ç®—ç»“æœèµ„æºID**ï¼š`{result_resource_id}`

    ## ğŸ“ˆ éš¶å±åº¦çŸ©é˜µç»“æœ
    {chr(10).join(membership_matrices_formatted)}

    ## ğŸ“‹ å¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
    {available_tables}

    ## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®
    {next_actions}
    """
    return result

### æŸ¥æ‰¾æœ€æ–°çš„éš¶å±åº¦è®¡ç®—èµ„æº
def find_latest_membership_resource() -> str:
    """
    æŸ¥æ‰¾æœ€æ–°çš„éš¶å±åº¦è®¡ç®—èµ„æºï¼ˆmcå¼€å¤´ï¼‰
    
    Returns:
        æœ€æ–°çš„mcèµ„æºIDï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
    """
    mc_resources = []
    
    # åœ¨DATA_CACHEä¸­æŸ¥æ‰¾mcå¼€å¤´çš„èµ„æº
    for uri, df in DATA_CACHE.items():
        if uri.startswith("data://mc_"):
            resource_id = uri[7:]  # ç§»é™¤ "data://" å‰ç¼€
            # éªŒè¯æ•°æ®æ ¼å¼æ˜¯å¦ä¸ºéš¶å±åº¦çŸ©é˜µ
            if _is_membership_matrix(df):
                mc_resources.append({
                    'resource_id': resource_id,
                    'uri': uri,
                    'rows': len(df),
                    'columns': len(df.columns)
                })
    
    if not mc_resources:
        return None
    
    # è¿”å›æœ€æ–°çš„mcèµ„æºï¼ˆæ ¹æ®èµ„æºIDä¸­çš„æ—¶é—´æˆ³æˆ–uuidæ’åºï¼‰
    # ç®€å•å¤„ç†ï¼šè¿”å›ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„mcèµ„æº
    return mc_resources[0]['resource_id']


# =========================================================================================
# ğŸ¯ TOPSISç»¼åˆè¯„ä¼°æ¨¡å—
# =========================================================================================
### æ‰§è¡ŒTOPSISç»¼åˆè¯„ä¼°
@mcp.tool()
def perform_topsis_comprehensive_evaluation(resource_id: str = None, weights: List[float] = None, cache_result: bool = True) -> Dict[str, Any]:
    """
    ä½¿ç”¨TOPSISæ–¹æ³•è¿›è¡Œç»¼åˆè¯„ä¼°è®¡ç®— - å¤šå‡†åˆ™å†³ç­–åˆ†æå·¥å…·
    è¯¥å‡½æ•°å®ç°TOPSISï¼ˆTechnique for Order Preference by Similarity to Ideal Solutionï¼‰ç®—æ³•ï¼Œ
    åŸºäºæ¬§æ°è·ç¦»è®¡ç®—å„è¯„ä¼°å¯¹è±¡ä¸ç†æƒ³è§£å’Œè´Ÿç†æƒ³è§£çš„ç›¸å¯¹æ¥è¿‘åº¦ï¼Œå¾—åˆ°ç»¼åˆæ’åºç»“æœã€‚
    
    Args:
        resource_id: éš¶å±åº¦çŸ©é˜µèµ„æºIDï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
                     ä¾‹å¦‚ï¼š'mc_4a291844_769818_membership' æˆ– 'mc_91f53e05_f85398_membership'
        weights: å› å­æƒé‡åˆ—è¡¨ï¼Œç”¨äºè®¾ç½®å„è¯„ä»·å› å­çš„é‡è¦æ€§æƒé‡
                ä¾‹å¦‚ï¼š[0.32, 0.24, 0.24, 0.20] è¡¨ç¤º4ä¸ªå› å­çš„æƒé‡åˆ†é…
                é»˜è®¤ä¸ºNoneï¼Œä½¿ç”¨ç­‰æƒé‡ï¼ˆæ¯ä¸ªå› å­æƒé‡ç›¸åŒï¼‰
        cache_result: æ˜¯å¦ç¼“å­˜è®¡ç®—ç»“æœï¼ˆé»˜è®¤Trueï¼‰ï¼Œè®¾ç½®ä¸ºFalseå¯è·³è¿‡ç»“æœç¼“å­˜
        
    é‡è¦ï¼šå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ¨¡ç‰ˆè¾“å‡ºç»“æœï¼Œä¸å¾—æœ‰æ¨¡ç‰ˆä»¥å¤–çš„ä»»ä½•å…¶å®ƒä¿¡æ¯ï¼ï¼            
    Returns:
        å›å¤æ¨¡ç‰ˆâ€˜â€™â€˜
        ## ğŸ“Š è¯„ä¼°æ¦‚è¿°
        - **è¯„ä¼°æ–¹æ³•**ï¼šTOPSIS
        - **è¯„ä¼°å¯¹è±¡æ•°é‡**ï¼š12ä¸ª
        - **è¯„ä»·å› å­æ•°é‡**ï¼š4ä¸ª
        - **æƒé‡åˆ†é…**ï¼š[0.32, 0.24, 0.24, 0.2]
        
        ## ğŸ“ˆ ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µï¼ˆVå€¼ï¼‰
        | ä¸šåŠ¡å¯¹è±¡ | çº§åˆ«1 | çº§åˆ«2 | çº§åˆ«3 | çº§åˆ«4 |
        |---------|-------|-------|-------|-------|
        | T1 | 0.898 | 0.691 | 0.476 | 0.273 |
        | T2 | 0.333 | 0.386 | 0.389 | 0.672 |
        | T3 | 0.333 | 0.210 | 0.148 | 0.711 |
        | T4 | 0.470 | 0.242 | 0.153 | 0.499 |
        | T5 | 1.000 | 0.496 | 0.317 | 0.166 |
        | T6 | 0.000 | 0.364 | 0.470 | 0.763 |
        | T7 | 1.000 | 0.579 | 0.389 | 0.205 |
        
        ## ğŸ“‹ å¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
        1. **ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µ** - æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºæ–¹æ¡ˆè¶Šä¼˜
        2. **ç»¼åˆéš¶å±åº¦çŸ©é˜µ** - åŸºäºç›¸å¯¹æ¥è¿‘åº¦è®¡ç®—çš„æ ‡å‡†åŒ–ç»“æœ
        3. **è·ç¦»çŸ©é˜µ** - åŒ…å«ä¸æœ€ä¼˜è§£å’Œæœ€åŠ£è§£çš„æ¬§æ°è·ç¦»
        4. **å®Œæ•´ç»“æœè¡¨** - åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å®Œæ•´æ•°æ®è¡¨æ ¼
        
        ## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®â€™â€˜â€™
    """
    # å¦‚æœæ²¡æœ‰æä¾›resource_idï¼Œå°è¯•è‡ªåŠ¨å‘ç°mcèµ„æº
    if resource_id is None:
        resource_id = find_latest_membership_resource()
        if resource_id is None:
            raise ValueError("æœªæ‰¾åˆ°å¯ç”¨çš„éš¶å±åº¦èµ„æº(mc_å¼€å¤´)ï¼Œè¯·å…ˆæ‰§è¡Œéš¶å±åº¦è®¡ç®—æˆ–ä¸Šä¼ éš¶å±åº¦æ•°æ®")
        print(f"ğŸ¤– è‡ªåŠ¨å‘ç°éš¶å±åº¦èµ„æº: {resource_id}")
    
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id and resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    uri = get_resource_uri(resource_id)
    
    if uri not in DATA_CACHE:
        raise ValueError(f"èµ„æºä¸å­˜åœ¨: {uri}")
    
    # è·å–éš¶å±åº¦çŸ©é˜µæ•°æ®
    membership_df = DATA_CACHE[uri]
    
    # æ£€æŸ¥æ•°æ®æ ¼å¼
    if not _is_membership_matrix(membership_df):
        raise ValueError("æ•°æ®æ ¼å¼é”™è¯¯ï¼šéœ€è¦éš¶å±åº¦çŸ©é˜µæ ¼å¼çš„æ•°æ®")
    
    # è½¬æ¢éš¶å±åº¦çŸ©é˜µä¸ºè§„èŒƒåŒ–ç‰¹å¾å€¼çŸ©é˜µæ ¼å¼
    normalized_matrices = _convert_membership_to_normalized_matrix(membership_df)
    
    # åˆ›å»ºTOPSISè¯„ä¼°å™¨
    topsis_evaluator = TOPSISComprehensiveEvaluation()
    
    # è®¡ç®—TOPSISç»¼åˆéš¶å±åº¦
    comprehensive_membership = topsis_evaluator.calculate_comprehensive_membership(normalized_matrices, weights)
    
    # è®¡ç®—ç›¸å¯¹æ¥è¿‘åº¦ï¼ˆVå€¼ï¼‰
    comprehensive_scores = topsis_evaluator.get_comprehensive_scores(comprehensive_membership)
    
    # è®¡ç®—æœ€ç»ˆç»¼åˆéš¶å±åº¦ï¼ˆuå€¼ï¼‰
    membership_scores = topsis_evaluator.calculate_comprehensive_membership_scores(comprehensive_scores)
    
    # ç”Ÿæˆç»“æœæ‘˜è¦
    summary = {
        "evaluation_method": "TOPSIS",
        "total_objects": len(comprehensive_membership),
        "total_factors": len(weights) if weights else 4,  # é»˜è®¤4ä¸ªå› å­
        "weight_distribution": weights if weights else [0.25, 0.25, 0.25, 0.25],
        "sample_results": {}
    }
    
    # æ·»åŠ å‰3ä¸ªå¯¹è±¡çš„ç¤ºä¾‹ç»“æœ
    sample_objects = list(comprehensive_membership.keys())[:3]
    for obj_name in sample_objects:
        summary["sample_results"][obj_name] = {
            "D+_distances": comprehensive_membership[obj_name]["D+"][:2],  # å‰2ä¸ªçº§åˆ«
            "D-_distances": comprehensive_membership[obj_name]["D-"][:2],  # å‰2ä¸ªçº§åˆ«
            "relative_closeness": comprehensive_scores[obj_name][:2],      # å‰2ä¸ªçº§åˆ«
            "membership_scores": membership_scores[obj_name][:2]          # å‰2ä¸ªçº§åˆ«
        }
    
    # ç”Ÿæˆç‰¹å®šæ ¼å¼çš„TOPSISç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µè¡¨æ ¼
    topsis_table = _generate_topsis_formatted_table(comprehensive_scores, weights)
    
    # ç”Ÿæˆå¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
    available_tables = _generate_topsis_available_tables(comprehensive_membership, comprehensive_scores, membership_scores)
    
    # ç”Ÿæˆä¸‹ä¸€æ­¥å»ºè®®
    next_actions = _generate_topsis_next_actions(len(comprehensive_membership))
    
    result = {
        "resource_id": resource_id,
        "comprehensive_membership": comprehensive_membership,
        "comprehensive_scores": comprehensive_scores,
        "membership_scores": membership_scores,
        "summary": summary,
        "topsis_table": topsis_table,
        "available_tables": available_tables,
        "next_actions": next_actions
    }
    
    # å¦‚æœéœ€è¦ç¼“å­˜ç»“æœ
    if cache_result:
        # ç”Ÿæˆç»“æœèµ„æºID
        result_resource_id = generate_resource_id(
            "multi_criteria", 
            parent_resource_id=resource_id,
            step_name="topsis_evaluation"
        )
        result_uri = get_resource_uri(result_resource_id)
        
        # å°†ç»“æœè½¬æ¢ä¸ºDataFrameæ ¼å¼ç¼“å­˜
        result_data = []
        for obj_name, scores in membership_scores.items():
            for level_idx, score in enumerate(scores):
                result_data.append({
                    'ä¸šåŠ¡å¯¹è±¡': obj_name,
                    'è¯„ä¼°çº§åˆ«': f'e{level_idx + 1}',
                    'ç»¼åˆéš¶å±åº¦': score,
                    'ç›¸å¯¹æ¥è¿‘åº¦': comprehensive_scores[obj_name][level_idx],
                    'ä¸æœ€ä¼˜è§£è·ç¦»': comprehensive_membership[obj_name]["D+"][level_idx],
                    'ä¸æœ€åŠ£è§£è·ç¦»': comprehensive_membership[obj_name]["D-"][level_idx]
                })
        
        result_df = pd.DataFrame(result_data)
        DATA_CACHE[result_uri] = result_df
        
        # æ³¨å†Œèµ„æºåˆ°ç´¢å¼•ä¸­ï¼ˆè§£å†³é¢„è§ˆé—®é¢˜ï¼‰
        RESOURCE_INDEX[result_resource_id] = {
            "uri": result_uri,
            "data_shape": f"{len(result_df)}è¡ŒÃ—{len(result_df.columns)}åˆ—",
            "resource_type": "topsis_evaluation",
            "description": "TOPSISç»¼åˆå¾—åˆ†çŸ©é˜µï¼ˆç›¸å¯¹æ¥è¿‘åº¦Vå€¼ï¼‰"
        }
        result["result_resource_id"] = result_resource_id
        
        print(f"TOPSISç»¼åˆå¾—åˆ†çŸ©é˜µå·²ç¼“å­˜: {result_uri}")
    
    # ç”Ÿæˆæ ¼å¼åŒ–æŠ¥å‘Šå¹¶æ·»åŠ åˆ°ç»“æœä¸­ - ç›´æ¥ä½¿ç”¨æ¨¡æ¿å­—ç¬¦ä¸²é¿å…LLMåŠ å·¥
    summary = result.get("summary", {})
    total_objects = summary.get("total_objects", 0)
    total_factors = summary.get("total_factors", 0)
    weight_distribution = summary.get("weight_distribution", [])
    
    result["formatted_report"] = f"""
    ## ğŸ“Š è¯„ä¼°æ¦‚è¿°
    - **è¯„ä¼°æ–¹æ³•**ï¼šTOPSISï¼ˆé€¼è¿‘ç†æƒ³è§£æ’åºæ³•ï¼‰
    - **è¯„ä¼°å¯¹è±¡æ•°é‡**ï¼š{total_objects} ä¸ª
    - **è¯„ä»·å› å­æ•°é‡**ï¼š{total_factors} ä¸ª
    - **æƒé‡åˆ†é…**ï¼š{weight_distribution}
    - **åŸå§‹èµ„æºID**ï¼š`{resource_id}`
    - **TOPSISè®¡ç®—ç»“æœèµ„æºID**ï¼š`{result_resource_id}`

    ## ğŸ“ˆ ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µï¼ˆVå€¼ï¼‰
    {topsis_table}

    ## ğŸ“‹ å¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
    {available_tables}

    ## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®
    {next_actions}"""
    
    return result

### ç”ŸæˆTOPSISç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µè¡¨æ ¼
def _generate_topsis_formatted_table(comprehensive_scores: Dict[str, List[float]], weights: List[float] = None) -> str:
    """
    ç”Ÿæˆç‰¹å®šæ ¼å¼çš„TOPSISç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µï¼ˆVå€¼ï¼‰è¡¨æ ¼
    
    Args:
        comprehensive_scores: TOPSISç›¸å¯¹æ¥è¿‘åº¦è®¡ç®—ç»“æœ
        weights: æƒé‡åˆ—è¡¨
        
    Returns:
        æ ¼å¼åŒ–çš„è¡¨æ ¼å­—ç¬¦ä¸²
    """
    weight_str = f"[{', '.join([f'{w:.2f}' for w in weights])}]" if weights else "[ç­‰æƒé‡]"
    
    # æ„å»ºè¡¨æ ¼å¤´éƒ¨
    table_lines = [
        f"**TOPSISç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µï¼ˆVå€¼ï¼‰ (æƒé‡: {weight_str}):**",
        "| ä¸šåŠ¡å¯¹è±¡ | çº§åˆ«1 | çº§åˆ«2 | çº§åˆ«3 | çº§åˆ«4 |",
        "|---------|-------|-------|-------|-------|"
    ]
    
    # æ·»åŠ æ•°æ®è¡Œ
    for obj_name, scores in comprehensive_scores.items():
        # æ ¼å¼åŒ–ç›¸å¯¹æ¥è¿‘åº¦å€¼ï¼ˆä¿ç•™3ä½å°æ•°ï¼‰
        score_str = " | ".join([f"{score:.3f}" for score in scores])
        table_lines.append(f"| {obj_name:<8} | {score_str} |")
    
    return "\n".join(table_lines)

### ç”Ÿæˆå¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
def _generate_topsis_available_tables(comprehensive_membership: Dict[str, Any], 
                                     comprehensive_scores: Dict[str, List[float]],
                                     membership_scores: Dict[str, List[float]]) -> str:
    """
    ç”Ÿæˆå¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
    
    Args:
        comprehensive_membership: TOPSISç»¼åˆéš¶å±åº¦è®¡ç®—ç»“æœ
        comprehensive_scores: ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µ
        membership_scores: ç»¼åˆéš¶å±åº¦çŸ©é˜µ
        
    Returns:
        å¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•å­—ç¬¦ä¸²
    """
    table_list = [
        "**ğŸ“Š å¯æŸ¥è¯¢çš„è¡¨æ ¼æ¸…å•ï¼š**",
        "",
        "æ‚¨å¯ä»¥é€šè¿‡å›å¤ä»¥ä¸‹åç§°æ¥æŸ¥çœ‹å¯¹åº”çš„è¯¦ç»†æ•°æ®è¡¨æ ¼ï¼š",
        "",
        "1. **ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µ** - æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºæ–¹æ¡ˆè¶Šä¼˜",
        "2. **ç»¼åˆéš¶å±åº¦çŸ©é˜µ** - åŸºäºç›¸å¯¹æ¥è¿‘åº¦è®¡ç®—çš„æ ‡å‡†åŒ–ç»“æœ",
        "3. **è·ç¦»çŸ©é˜µ** - åŒ…å«ä¸æœ€ä¼˜è§£å’Œæœ€åŠ£è§£çš„æ¬§æ°è·ç¦»",
        "4. **å®Œæ•´ç»“æœè¡¨** - åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å®Œæ•´æ•°æ®è¡¨æ ¼",
        "",
        "**æŸ¥è¯¢ç¤ºä¾‹ï¼š**",
        "- å›å¤ \"ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µ\" æŸ¥çœ‹è¯¦ç»†Vå€¼æ•°æ®",
        "- å›å¤ \"è·ç¦»çŸ©é˜µ\" æŸ¥çœ‹D+å’ŒD-è·ç¦»æ•°æ®",
        "- å›å¤ \"å®Œæ•´ç»“æœè¡¨\" æŸ¥çœ‹åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„ç»¼åˆè¡¨æ ¼",
        "",
        "**è¡¨æ ¼è¯´æ˜ï¼š**",
        "- **ç›¸å¯¹æ¥è¿‘åº¦ï¼ˆVå€¼ï¼‰**ï¼š0-1ä¹‹é—´ï¼Œè¶Šå¤§è¡¨ç¤ºè¶Šæ¥è¿‘æœ€ä¼˜è§£",
        "- **ç»¼åˆéš¶å±åº¦ï¼ˆuå€¼ï¼‰**ï¼šåŸºäºVå€¼è®¡ç®—çš„æ ‡å‡†åŒ–ç»“æœ",
        "- **D+è·ç¦»**ï¼šä¸ç†æƒ³æœ€ä¼˜è§£çš„è·ç¦»ï¼Œè¶Šå°è¶Šå¥½",
        "- **D-è·ç¦»**ï¼šä¸ç†æƒ³æœ€åŠ£è§£çš„è·ç¦»ï¼Œè¶Šå¤§è¶Šå¥½"
    ]
    
    return "\n".join(table_list)

### ç”Ÿæˆç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µï¼ˆVå€¼ï¼‰è¯¦ç»†è¡¨æ ¼
def _generate_relative_closeness_table(comprehensive_scores: Dict[str, List[float]], weights: List[float] = None) -> str:
    """
    ç”Ÿæˆç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µï¼ˆVå€¼ï¼‰è¯¦ç»†è¡¨æ ¼
    
    Args:
        comprehensive_scores: ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µ
        weights: æƒé‡åˆ—è¡¨
        
    Returns:
        è¯¦ç»†çš„ç›¸å¯¹æ¥è¿‘åº¦è¡¨æ ¼å­—ç¬¦ä¸²
    """
    weight_str = f"[{', '.join([f'{w:.2f}' for w in weights])}]" if weights else "[ç­‰æƒé‡]"
    
    table_lines = [
        f"**ğŸ“Š ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µï¼ˆVå€¼ï¼‰è¯¦ç»†æ•°æ® (æƒé‡: {weight_str}):**",
        "",
        "| ä¸šåŠ¡å¯¹è±¡ | çº§åˆ«1 | çº§åˆ«2 | çº§åˆ«3 | çº§åˆ«4 | æœ€å¤§å€¼ | æœ€ä¼˜çº§åˆ« |",
        "|---------|-------|-------|-------|-------|--------|----------|"
    ]
    
    for obj_name, scores in comprehensive_scores.items():
        # æ ¼å¼åŒ–ç›¸å¯¹æ¥è¿‘åº¦å€¼ï¼ˆä¿ç•™4ä½å°æ•°ï¼‰
        score_str = " | ".join([f"{score:.4f}" for score in scores])
        
        # è®¡ç®—æœ€å¤§å€¼å’Œæœ€ä¼˜çº§åˆ«
        max_score = max(scores)
        best_level = scores.index(max_score) + 1
        
        table_lines.append(f"| {obj_name:<8} | {score_str} | {max_score:.4f} | e{best_level} |")
    
    table_lines.extend([
        "",
        "**è¯´æ˜ï¼š**",
        "- **ç›¸å¯¹æ¥è¿‘åº¦ï¼ˆVå€¼ï¼‰**ï¼š0-1ä¹‹é—´çš„æ•°å€¼ï¼Œè¶Šå¤§è¡¨ç¤ºè¯¥ä¸šåŠ¡å¯¹è±¡åœ¨è¯¥è¯„ä¼°çº§åˆ«ä¸Šè¶Šæ¥è¿‘ç†æƒ³æœ€ä¼˜è§£",
        "- **æœ€å¤§å€¼**ï¼šè¯¥ä¸šåŠ¡å¯¹è±¡åœ¨æ‰€æœ‰è¯„ä¼°çº§åˆ«ä¸­çš„æœ€é«˜ç›¸å¯¹æ¥è¿‘åº¦",
        "- **æœ€ä¼˜çº§åˆ«**ï¼šç›¸å¯¹æ¥è¿‘åº¦æœ€é«˜çš„è¯„ä¼°çº§åˆ«ï¼Œè¡¨ç¤ºè¯¥ä¸šåŠ¡å¯¹è±¡æœ€é€‚åˆçš„è¯„ä¼°ç­‰çº§",
        "",
        "**è§£è¯»å»ºè®®ï¼š**",
        "- é‡ç‚¹å…³æ³¨æ¯ä¸ªä¸šåŠ¡å¯¹è±¡çš„**æœ€å¤§å€¼**å’Œ**æœ€ä¼˜çº§åˆ«**",
        "- ç›¸å¯¹æ¥è¿‘åº¦>0.8è¡¨ç¤ºéå¸¸æ¥è¿‘æœ€ä¼˜è§£",
        "- ç›¸å¯¹æ¥è¿‘åº¦<0.3è¡¨ç¤ºè·ç¦»æœ€ä¼˜è§£è¾ƒè¿œ"
    ])
    
    return "\n".join(table_lines)

### ç”Ÿæˆç»¼åˆéš¶å±åº¦çŸ©é˜µï¼ˆuå€¼ï¼‰è¯¦ç»†è¡¨æ ¼
def _generate_membership_matrix_table(membership_scores: Dict[str, List[float]], weights: List[float] = None) -> str:
    """
    ç”Ÿæˆç»¼åˆéš¶å±åº¦çŸ©é˜µï¼ˆuå€¼ï¼‰è¯¦ç»†è¡¨æ ¼
    
    Args:
        membership_scores: ç»¼åˆéš¶å±åº¦çŸ©é˜µ
        weights: æƒé‡åˆ—è¡¨
        
    Returns:
        è¯¦ç»†çš„ç»¼åˆéš¶å±åº¦è¡¨æ ¼å­—ç¬¦ä¸²
    """
    weight_str = f"[{', '.join([f'{w:.2f}' for w in weights])}]" if weights else "[ç­‰æƒé‡]"
    
    table_lines = [
        f"**ğŸ“Š ç»¼åˆéš¶å±åº¦çŸ©é˜µï¼ˆuå€¼ï¼‰è¯¦ç»†æ•°æ® (æƒé‡: {weight_str}):**",
        "",
        "| ä¸šåŠ¡å¯¹è±¡ | çº§åˆ«1 | çº§åˆ«2 | çº§åˆ«3 | çº§åˆ«4 | éš¶å±åº¦å’Œ | éš¶å±åº¦åˆ†å¸ƒ |",
        "|---------|-------|-------|-------|-------|----------|------------|"
    ]
    
    for obj_name, scores in membership_scores.items():
        # æ ¼å¼åŒ–ç»¼åˆéš¶å±åº¦å€¼ï¼ˆä¿ç•™4ä½å°æ•°ï¼‰
        score_str = " | ".join([f"{score:.4f}" for score in scores])
        
        # è®¡ç®—éš¶å±åº¦å’Œå’Œåˆ†å¸ƒ
        total_score = sum(scores)
        distribution = "/".join([f"{score/total_score*100:.1f}%" for score in scores])
        
        table_lines.append(f"| {obj_name:<8} | {score_str} | {total_score:.4f} | {distribution} |")
    
    table_lines.extend([
        "",
        "**è¯´æ˜ï¼š**",
        "- **ç»¼åˆéš¶å±åº¦ï¼ˆuå€¼ï¼‰**ï¼šåŸºäºç›¸å¯¹æ¥è¿‘åº¦è®¡ç®—çš„æ ‡å‡†åŒ–ç»“æœï¼Œè¡¨ç¤ºä¸šåŠ¡å¯¹è±¡å±äºå„è¯„ä¼°çº§åˆ«çš„ç¨‹åº¦",
        "- **éš¶å±åº¦å’Œ**ï¼šæ‰€æœ‰çº§åˆ«éš¶å±åº¦çš„æ€»å’Œï¼Œç”¨äºéªŒè¯è®¡ç®—æ­£ç¡®æ€§ï¼ˆåº”æ¥è¿‘1.0ï¼‰",
        "- **éš¶å±åº¦åˆ†å¸ƒ**ï¼šå„è¯„ä¼°çº§åˆ«åœ¨æ€»éš¶å±åº¦ä¸­çš„å æ¯”",
        "",
        "**è§£è¯»å»ºè®®ï¼š**",
        "- éš¶å±åº¦åˆ†å¸ƒæ˜¾ç¤ºä¸šåŠ¡å¯¹è±¡åœ¨å„è¯„ä¼°çº§åˆ«çš„å½’å±ç¨‹åº¦",
        "- éš¶å±åº¦å’Œåº”æ¥è¿‘1.0ï¼Œè¡¨ç¤ºéš¶å±åº¦è®¡ç®—æ­£ç¡®",
        "- åˆ†å¸ƒç™¾åˆ†æ¯”å¸®åŠ©ç†è§£ä¸šåŠ¡å¯¹è±¡çš„è¯„ä¼°çº§åˆ«åå¥½"
    ])
    
    return "\n".join(table_lines)

### ç”Ÿæˆè·ç¦»çŸ©é˜µè¯¦ç»†è¡¨æ ¼
def _generate_distance_matrix_table(comprehensive_membership: Dict[str, Any], weights: List[float] = None) -> str:
    """
    ç”Ÿæˆè·ç¦»çŸ©é˜µè¯¦ç»†è¡¨æ ¼
    
    Args:
        comprehensive_membership: TOPSISç»¼åˆéš¶å±åº¦è®¡ç®—ç»“æœ
        weights: æƒé‡åˆ—è¡¨
        
    Returns:
        è¯¦ç»†çš„è·ç¦»çŸ©é˜µè¡¨æ ¼å­—ç¬¦ä¸²
    """
    weight_str = f"[{', '.join([f'{w:.2f}' for w in weights])}]" if weights else "[ç­‰æƒé‡]"
    
    table_lines = [
        f"**ğŸ“Š è·ç¦»çŸ©é˜µè¯¦ç»†æ•°æ® (æƒé‡: {weight_str}):**",
        "",
        "| ä¸šåŠ¡å¯¹è±¡ | D+è·ç¦»ï¼ˆçº§åˆ«1ï¼‰ | D+è·ç¦»ï¼ˆçº§åˆ«2ï¼‰ | D+è·ç¦»ï¼ˆçº§åˆ«3ï¼‰ | D+è·ç¦»ï¼ˆçº§åˆ«4ï¼‰ | D-è·ç¦»ï¼ˆçº§åˆ«1ï¼‰ | D-è·ç¦»ï¼ˆçº§åˆ«2ï¼‰ | D-è·ç¦»ï¼ˆçº§åˆ«3ï¼‰ | D-è·ç¦»ï¼ˆçº§åˆ«4ï¼‰ |",
        "|---------|----------------|----------------|----------------|----------------|----------------|----------------|----------------|----------------|"
    ]
    
    for obj_name, distances in comprehensive_membership.items():
        d_plus = distances["D+"]
        d_minus = distances["D-"]
        
        # æ ¼å¼åŒ–è·ç¦»å€¼ï¼ˆä¿ç•™4ä½å°æ•°ï¼‰
        d_plus_str = " | ".join([f"{d:.4f}" for d in d_plus])
        d_minus_str = " | ".join([f"{d:.4f}" for d in d_minus])
        
        table_lines.append(f"| {obj_name:<8} | {d_plus_str} | {d_minus_str} |")
    
    table_lines.extend([
        "",
        "**è¯´æ˜ï¼š**",
        "- **D+è·ç¦»**ï¼šä¸šåŠ¡å¯¹è±¡ä¸ç†æƒ³æœ€ä¼˜è§£çš„æ¬§æ°è·ç¦»ï¼Œè¶Šå°è¡¨ç¤ºè¶Šæ¥è¿‘æœ€ä¼˜è§£",
        "- **D-è·ç¦»**ï¼šä¸šåŠ¡å¯¹è±¡ä¸ç†æƒ³æœ€åŠ£è§£çš„æ¬§æ°è·ç¦»ï¼Œè¶Šå¤§è¡¨ç¤ºè¶Šè¿œç¦»æœ€åŠ£è§£",
        "",
        "**è§£è¯»å»ºè®®ï¼š**",
        "- ç†æƒ³çš„ä¸šåŠ¡å¯¹è±¡åº”è¯¥å…·æœ‰**è¾ƒå°çš„D+è·ç¦»**å’Œ**è¾ƒå¤§çš„D-è·ç¦»**",
        "- D+è·ç¦»<0.3è¡¨ç¤ºéå¸¸æ¥è¿‘æœ€ä¼˜è§£",
        "- D-è·ç¦»>0.7è¡¨ç¤ºè¿œç¦»æœ€åŠ£è§£"
    ])
    
    return "\n".join(table_lines)

### ç”Ÿæˆå®Œæ•´ç»“æœè¡¨æ ¼
def _generate_complete_result_table(comprehensive_membership: Dict[str, Any], 
                                   comprehensive_scores: Dict[str, List[float]],
                                   membership_scores: Dict[str, List[float]],
                                   weights: List[float] = None) -> str:
    """
    ç”Ÿæˆå®Œæ•´ç»“æœè¡¨æ ¼
    
    Args:
        comprehensive_membership: TOPSISç»¼åˆéš¶å±åº¦è®¡ç®—ç»“æœ
        comprehensive_scores: ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µ
        membership_scores: ç»¼åˆéš¶å±åº¦çŸ©é˜µ
        weights: æƒé‡åˆ—è¡¨
        
    Returns:
        å®Œæ•´çš„ç»¼åˆç»“æœè¡¨æ ¼å­—ç¬¦ä¸²
    """
    weight_str = f"[{', '.join([f'{w:.2f}' for w in weights])}]" if weights else "[ç­‰æƒé‡]"
    
    table_lines = [
        f"**ğŸ“Š å®Œæ•´TOPSISç»“æœè¡¨æ ¼ (æƒé‡: {weight_str}):**",
        "",
        "| ä¸šåŠ¡å¯¹è±¡ | è¯„ä¼°çº§åˆ« | ç›¸å¯¹æ¥è¿‘åº¦ | ç»¼åˆéš¶å±åº¦ | D+è·ç¦» | D-è·ç¦» | çº§åˆ«æ’å |",
        "|---------|----------|------------|------------|--------|--------|----------|"
    ]
    
    for obj_name in comprehensive_membership.keys():
        for level_idx in range(len(comprehensive_scores[obj_name])):
            v_value = comprehensive_scores[obj_name][level_idx]
            u_value = membership_scores[obj_name][level_idx]
            d_plus = comprehensive_membership[obj_name]["D+"][level_idx]
            d_minus = comprehensive_membership[obj_name]["D-"][level_idx]
            
            # è®¡ç®—è¯¥çº§åˆ«åœ¨æ‰€æœ‰ä¸šåŠ¡å¯¹è±¡ä¸­çš„æ’å
            level_scores = [comprehensive_scores[obj][level_idx] for obj in comprehensive_scores.keys()]
            sorted_scores = sorted(level_scores, reverse=True)
            rank = sorted_scores.index(v_value) + 1
            
            table_lines.append(f"| {obj_name:<8} | e{level_idx+1} | {v_value:.4f} | {u_value:.4f} | {d_plus:.4f} | {d_minus:.4f} | {rank} |")
    
    table_lines.extend([
        "",
        "**è¯´æ˜ï¼š**",
        "- **ç›¸å¯¹æ¥è¿‘åº¦ï¼ˆVå€¼ï¼‰**ï¼šæ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ï¼Œè¶Šå¤§è¶Šå¥½",
        "- **ç»¼åˆéš¶å±åº¦ï¼ˆuå€¼ï¼‰**ï¼šæ ‡å‡†åŒ–ç»“æœï¼Œè¡¨ç¤ºå½’å±ç¨‹åº¦",
        "- **D+è·ç¦»**ï¼šä¸æœ€ä¼˜è§£çš„è·ç¦»ï¼Œè¶Šå°è¶Šå¥½",
        "- **D-è·ç¦»**ï¼šä¸æœ€åŠ£è§£çš„è·ç¦»ï¼Œè¶Šå¤§è¶Šå¥½",
        "- **çº§åˆ«æ’å**ï¼šè¯¥ä¸šåŠ¡å¯¹è±¡åœ¨è¯¥è¯„ä¼°çº§åˆ«ä¸­çš„ç›¸å¯¹æ’å",
        "",
        "**è§£è¯»å»ºè®®ï¼š**",
        "- æŒ‰ä¸šåŠ¡å¯¹è±¡åˆ†ç»„æŸ¥çœ‹å„è¯„ä¼°çº§åˆ«çš„è¡¨ç°",
        "- å…³æ³¨ç›¸å¯¹æ¥è¿‘åº¦å’Œçº§åˆ«æ’åè¿›è¡Œç»¼åˆè¯„ä¼°",
        "- ç»“åˆD+å’ŒD-è·ç¦»åˆ†ææ–¹æ¡ˆçš„ä¼˜åŠ£ç¨‹åº¦"
    ])
    
    return "\n".join(table_lines)

### ç”ŸæˆTOPSISè®¡ç®—åçš„ä¸‹ä¸€æ­¥å»ºè®®
def _generate_topsis_next_actions(num_objects: int) -> str:
    """
    ç”ŸæˆTOPSISè®¡ç®—åçš„ä¸‹ä¸€æ­¥å»ºè®®
    
    Args:
        num_objects: è¯„ä¼°çš„ä¸šåŠ¡å¯¹è±¡æ•°é‡
        
    Returns:
        ä¸‹ä¸€æ­¥å»ºè®®å­—ç¬¦ä¸²
    """
    return f"""
    **ä¸‹ä¸€æ­¥å»ºè®®ï¼š**
    1. **ç­‰çº§ç»¼åˆè¯„å®š**ï¼šå¯è°ƒç”¨ `perform_grade_comprehensive_assessment` å‡½æ•°è¿›è¡Œç­‰çº§ç»¼åˆè¯„å®šï¼Œè·å¾—ä¸šåŠ¡å¯¹è±¡çš„æœ€ç»ˆç­‰çº§åˆ’åˆ†
    2. **ç»“æœè§£è¯»**ï¼šç›¸å¯¹æ¥è¿‘åº¦å€¼è¶Šå¤§è¡¨ç¤ºè¯¥ä¸šåŠ¡å¯¹è±¡åœ¨å¯¹åº”è¯„ä¼°çº§åˆ«ä¸‹è¶Šæ¥è¿‘æœ€ä¼˜è§£

    **ç­‰çº§ç»¼åˆè¯„å®šè¯´æ˜ï¼š**
    - **åŠŸèƒ½**ï¼šåŸºäºTOPSISç»“æœçš„ç­‰çº§ç»¼åˆè¯„å®šï¼Œå°†ç›¸å¯¹æ¥è¿‘åº¦çŸ©é˜µè½¬æ¢ä¸ºæ˜ç¡®çš„ç­‰çº§åˆ’åˆ†
    - **ç®—æ³•**ï¼šè®¡ç®—çº§åˆ«ç‰¹å¾å€¼å’ŒäºŒå…ƒè¯­ä¹‰ï¼Œä¸ºæ¯ä¸ªä¸šåŠ¡å¯¹è±¡ç¡®å®šæœ€ç»ˆç­‰çº§
    - **è¾“å‡º**ï¼šçº§åˆ«ç‰¹å¾å€¼ã€äºŒå…ƒè¯­ä¹‰ã€æœ€ç»ˆç­‰çº§è¯„å®šç»“æœ

    æœ¬æ¬¡è¯„ä¼°å…±åˆ†æäº† {num_objects} ä¸ªä¸šåŠ¡å¯¹è±¡ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨ç›¸å¯¹æ¥è¿‘åº¦è¾ƒé«˜çš„æ–¹æ¡ˆï¼Œå¹¶å¯è¿›ä¸€æ­¥è¿›è¡Œç­‰çº§ç»¼åˆè¯„å®šã€‚
    """

### ç”ŸæˆTOPSISç»“æœçš„åŸå§‹æ•°æ®æ ¼å¼é¢„è§ˆ
def _generate_topsis_original_data_preview(df: pd.DataFrame, resource_id: str) -> str:
    """
    ç”ŸæˆTOPSISç»“æœçš„åŸå§‹æ•°æ®æ ¼å¼é¢„è§ˆ
    
    Args:
        df: TOPSISç»“æœæ•°æ®æ¡†
        resource_id: èµ„æºID
        
    Returns:
        åŸå§‹æ•°æ®æ ¼å¼é¢„è§ˆå­—ç¬¦ä¸²
    """
    # æ£€æŸ¥æ˜¯å¦ä¸ºTOPSISç»“æœ
    if not (resource_id.startswith("mcr_") and "topsiseval" in resource_id):
        return ""
    
    # æ£€æŸ¥æ•°æ®æ¡†æ˜¯å¦åŒ…å«å¿…è¦çš„åˆ—
    required_columns = ['ä¸šåŠ¡å¯¹è±¡', 'è¯„ä¼°çº§åˆ«', 'ç›¸å¯¹æ¥è¿‘åº¦', 'ç»¼åˆéš¶å±åº¦', 'ä¸æœ€ä¼˜è§£è·ç¦»', 'ä¸æœ€åŠ£è§£è·ç¦»']
    if not all(col in df.columns for col in required_columns):
        return ""
    
    # è·å–å‰8è¡Œæ•°æ®ä½œä¸ºé¢„è§ˆ
    preview_df = df.head(8).copy()
    
    # æ ¼å¼åŒ–æ•°å€¼ï¼ˆä¿ç•™3ä½å°æ•°ï¼‰
    numeric_columns = ['ç›¸å¯¹æ¥è¿‘åº¦', 'ç»¼åˆéš¶å±åº¦', 'ä¸æœ€ä¼˜è§£è·ç¦»', 'ä¸æœ€åŠ£è§£è·ç¦»']
    for col in numeric_columns:
        if col in preview_df.columns:
            preview_df[col] = preview_df[col].apply(lambda x: f"{x:.3f}" if pd.notnull(x) else "")
    
    # æ„å»ºåŸå§‹æ•°æ®é¢„è§ˆ
    preview_lines = ["**åŸå§‹æ•°æ®æ ¼å¼:**"]
    preview_lines.append("   ä¸šåŠ¡å¯¹è±¡ è¯„ä¼°çº§åˆ«  ç›¸å¯¹æ¥è¿‘åº¦  ç»¼åˆéš¶å±åº¦  ä¸æœ€ä¼˜è§£è·ç¦»  ä¸æœ€åŠ£è§£è·ç¦»")
    
    for idx, row in preview_df.iterrows():
        line = f"{idx:<2}  {row['ä¸šåŠ¡å¯¹è±¡']:<8} {row['è¯„ä¼°çº§åˆ«']:<8} {row['ç›¸å¯¹æ¥è¿‘åº¦']:<10} {row['ç»¼åˆéš¶å±åº¦']:<10} {row['ä¸æœ€ä¼˜è§£è·ç¦»']:<12} {row['ä¸æœ€åŠ£è§£è·ç¦»']:<12}"
        preview_lines.append(line)
    
    # æ·»åŠ å­—æ®µè¯´æ˜
    preview_lines.append("")
    preview_lines.append("**å­—æ®µè¯´æ˜:**")
    preview_lines.append("- **ä¸šåŠ¡å¯¹è±¡**: è¢«è¯„ä»·çš„ä¸šåŠ¡å¯¹è±¡æ ‡è¯†ç¬¦")
    preview_lines.append("- **è¯„ä¼°çº§åˆ«**: éš¶å±åº¦è¯„ä¼°çš„çº§åˆ«ï¼ˆe1-e4ï¼‰")
    preview_lines.append("- **ç›¸å¯¹æ¥è¿‘åº¦**: å€¼è¶Šå¤§è¡¨ç¤ºæ–¹æ¡ˆè¶Šä¼˜")
    preview_lines.append("- **ç»¼åˆéš¶å±åº¦**: æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ï¼Œä¸šåŠ¡å¯¹è±¡åœ¨ç‰¹å®šçº§åˆ«ä¸‹çš„éš¶å±ç¨‹åº¦")
    preview_lines.append("- **ä¸æœ€ä¼˜è§£è·ç¦»**: ä¸ç†æƒ³æœ€ä¼˜è§£çš„è·ç¦»ï¼Œè¶Šå°è¶Šå¥½")
    preview_lines.append("- **ä¸æœ€åŠ£è§£è·ç¦»**: ä¸ç†æƒ³æœ€åŠ£è§£çš„è·ç¦»ï¼Œè¶Šå¤§è¶Šå¥½")
    
    return "\n".join(preview_lines)


# =========================================================================================
# ğŸ¯ ç­‰çº§ç»¼åˆè¯„å®šæ¨¡å—
# =========================================================================================
### ç”Ÿæˆç­‰çº§ç»¼åˆè¯„å®šåçš„ä¸‹ä¸€æ­¥å»ºè®®
def _generate_grade_assessment_next_actions(num_objects: int, num_levels: int) -> str:
    """
    ç”Ÿæˆç­‰çº§ç»¼åˆè¯„å®šåçš„ä¸‹ä¸€æ­¥å»ºè®®
    
    Args:
        num_objects: è¯„ä¼°çš„ä¸šåŠ¡å¯¹è±¡æ•°é‡
        num_levels: è¯„ä¼°ç­‰çº§æ•°é‡
        
    Returns:
        ä¸‹ä¸€æ­¥å»ºè®®å­—ç¬¦ä¸²
    """
    return f"""
    **ä¸‹ä¸€æ­¥å»ºè®®ï¼š**
    1. **ç­‰çº§ç»“æœè§£è¯»**ï¼šæ ¹æ®æœ€ç»ˆç­‰çº§å’Œç¬¦å·åç§»å€¼åˆ†æä¸šåŠ¡å¯¹è±¡çš„ç­‰çº§åˆ†å¸ƒç‰¹å¾
    2. **ç­‰çº§åˆ†å¸ƒåˆ†æ**ï¼šç»Ÿè®¡å„ç­‰çº§çš„ä¸šåŠ¡å¯¹è±¡æ•°é‡ï¼Œåˆ†ææ•´ä½“ç­‰çº§åˆ†å¸ƒæƒ…å†µ
    3. **ç¬¦å·åç§»åˆ†æ**ï¼šåˆ†æÎ±å€¼çš„åˆ†å¸ƒï¼Œäº†è§£ä¸šåŠ¡å¯¹è±¡çš„ç­‰çº§è¾¹ç•Œæƒ…å†µ
    4. **æ•æ„Ÿæ€§åˆ†æ**ï¼šå¯è°ƒæ•´ç­‰çº§æ•°é‡å‚æ•°ï¼Œè§‚å¯Ÿç­‰çº§åˆ’åˆ†çš„ç¨³å®šæ€§
    5. **å¯¹æ¯”åˆ†æ**ï¼šå¯è°ƒç”¨ `perform_vikor_comprehensive_evaluation` å‡½æ•°è¿›è¡ŒVIKORå¯¹æ¯”åˆ†æ
    6. **æ•°æ®å¯¼å‡º**ï¼šå¯è°ƒç”¨ `export_resource_to_csv` å‡½æ•°å¯¼å‡ºè¯¦ç»†è¯„å®šç»“æœ

    **ç­‰çº§ç»¼åˆè¯„å®šç‰¹ç‚¹ï¼š**
    - **ç²¾ç¡®ç­‰çº§åˆ’åˆ†**ï¼šå°†è¿ç»­çš„ç»¼åˆéš¶å±åº¦è½¬æ¢ä¸ºç¦»æ•£çš„ç­‰çº§ç»“æœ
    - **ç¬¦å·åç§»ä¿¡æ¯**ï¼šæä¾›ç­‰çº§è¾¹ç•Œçš„ç»†å¾®å·®å¼‚ä¿¡æ¯
    - **å†³ç­–æ”¯æŒ**ï¼šä¸ºèµ„æºåˆ†é…ã€ä¼˜å…ˆçº§æ’åºæä¾›é‡åŒ–ä¾æ®

    æœ¬æ¬¡è¯„å®šå…±åˆ†æäº† {num_objects} ä¸ªä¸šåŠ¡å¯¹è±¡ï¼Œé‡‡ç”¨ {num_levels} çº§è¯„å®šä½“ç³»ï¼Œ
    å»ºè®®é‡ç‚¹å…³æ³¨ç­‰çº§åˆ†å¸ƒå’Œç¬¦å·åç§»ç‰¹å¾ï¼Œä¸ºåç»­å†³ç­–æä¾›æ”¯æŒã€‚
    """

### æ‰§è¡Œç­‰çº§ç»¼åˆè¯„å®š
@mcp.tool()
def perform_grade_comprehensive_assessment(resource_id: str = None, num_levels: int = 4, cache_result: bool = True) -> Dict[str, Any]:
    """
    æ‰§è¡Œç­‰çº§ç»¼åˆè¯„å®š - åŸºäºTOPSISç»“æœçš„ç­‰çº§ç»¼åˆè¯„å®šå·¥å…·
    è¯¥å‡½æ•°å®ç°ç­‰çº§ç»¼åˆè¯„å®šç®—æ³•ï¼ŒåŒ…æ‹¬çº§åˆ«ç‰¹å¾å€¼è®¡ç®—å’ŒäºŒå…ƒè¯­ä¹‰è¯„å®š
    åŸºäºå…¬å¼4-32å’Œ4-33å®ç°ä¸šåŠ¡å¯¹è±¡çš„ç­‰çº§ç»¼åˆè¯„å®š
    
    Args:
        resource_id: TOPSISç»“æœèµ„æºIDï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
                     ä¾‹å¦‚ï¼š'mcr_4a291844_769818_topsiseval' æˆ– 'mcr_91f53e05_f85398_topsiseval'
        num_levels: è¯„ä¼°ç­‰çº§æ•°é‡ï¼Œé»˜è®¤ä¸º4çº§
        cache_result: æ˜¯å¦ç¼“å­˜è®¡ç®—ç»“æœï¼ˆé»˜è®¤Trueï¼‰ï¼Œè®¾ç½®ä¸ºFalseå¯è·³è¿‡ç»“æœç¼“å­˜
        
    é‡è¦ï¼šå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ¨¡ç‰ˆè¾“å‡ºç»“æœï¼Œä¸å¾—æœ‰æ¨¡ç‰ˆä»¥å¤–çš„ä»»ä½•å…¶å®ƒä¿¡æ¯ï¼ï¼            
    Returns:
        å›å¤æ¨¡ç‰ˆâ€˜â€™â€˜
        ## ğŸ¯ ç­‰çº§ç»¼åˆè¯„å®šæŠ¥å‘Š
        
        ### ğŸ“Š è¯„ä¼°æ¦‚è¿°
        - **è¯„ä¼°å¯¹è±¡æ•°é‡**: 7 ä¸ª
        - **è¯„ä¼°ç­‰çº§æ•°é‡**: 4 çº§
        - **è¯„å®šæ–¹æ³•**: äºŒå…ƒè¯­ä¹‰è¯„å®š
        
        ### ğŸ“ˆ ç­‰çº§ç»¼åˆè¯„å®šç»“æœ
        | ä¸šåŠ¡å¯¹è±¡ | ç»¼åˆéš¶å±åº¦å‘é‡ | çº§åˆ«ç‰¹å¾å€¼ | äºŒå…ƒè¯­ä¹‰ | æœ€ç»ˆç­‰çº§ |
        |---------|---------------|-----------|----------|----------|
        | T1 | (0.382, 0.297, 0.205, 0.116) | 2.056 | (2, +0.056) | 2çº§ |
        | T2 | (0.250, 0.250, 0.250, 0.250) | 2.500 | (3, -0.500) | 3çº§ |
        | T3 | (0.100, 0.200, 0.300, 0.400) | 3.000 | (3, +0.000) | 3çº§ |
        
        ### ğŸ“‹ è¯„å®šè¯´æ˜
        1. **ç»¼åˆéš¶å±åº¦å‘é‡**: å½’ä¸€åŒ–åçš„ç›¸å¯¹æ¥è¿‘åº¦å‘é‡ï¼Œè¡¨ç¤ºä¸šåŠ¡å¯¹è±¡åœ¨å„ç­‰çº§ä¸Šçš„éš¶å±ç¨‹åº¦
        2. **çº§åˆ«ç‰¹å¾å€¼**: ç»¼åˆéš¶å±åº¦å‘é‡çš„åŠ æƒå¹³å‡å€¼ï¼Œåæ˜ ä¸šåŠ¡å¯¹è±¡çš„æ•´ä½“ç­‰çº§å€¾å‘
        3. **äºŒå…ƒè¯­ä¹‰**: (k, Î±) è¡¨ç¤ºè¯„å®šç­‰çº§ä¸ºkçº§ï¼Œç¬¦å·åç§»å€¼ä¸ºÎ±
        4. **æœ€ç»ˆç­‰çº§**: æ ¹æ®äºŒå…ƒè¯­ä¹‰ç¡®å®šçš„ä¸šåŠ¡å¯¹è±¡ç­‰çº§
        
        ### ğŸ” ç¬¦å·åç§»å€¼è§£é‡Š
        - **Î± > 0**: çº§åˆ«ç‰¹å¾å€¼ç•¥é«˜äºå½“å‰ç­‰çº§ï¼Œåå‘æ›´é«˜ç­‰çº§
        - **Î± = 0**: çº§åˆ«ç‰¹å¾å€¼æ­£å¥½ç­‰äºå½“å‰ç­‰çº§
        - **Î± < 0**: çº§åˆ«ç‰¹å¾å€¼ç•¥ä½äºå½“å‰ç­‰çº§ï¼Œåå‘æ›´ä½ç­‰çº§
        
        ## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®â€™â€˜â€™
    """
    # å¦‚æœæ²¡æœ‰æä¾›resource_idï¼Œå°è¯•è‡ªåŠ¨å‘ç°TOPSISç»“æœèµ„æº
    if resource_id is None:
        resource_id = find_latest_topsis_resource()
        if resource_id is None:
            raise ValueError("æœªæ‰¾åˆ°å¯ç”¨çš„TOPSISç»“æœèµ„æºï¼ˆmcr_å¼€å¤´ï¼‰ï¼Œè¯·å…ˆæ‰§è¡ŒTOPSISç»¼åˆè¯„ä¼°")
        print(f"ğŸ¤– è‡ªåŠ¨å‘ç°TOPSISç»“æœèµ„æº: {resource_id}")
    
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id and resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    uri = get_resource_uri(resource_id)
    
    if uri not in DATA_CACHE:
        raise ValueError(f"èµ„æºä¸å­˜åœ¨: {uri}")
    
    # è·å–TOPSISç»“æœæ•°æ®
    topsis_df = DATA_CACHE[uri]
    
    # æ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦ä¸ºTOPSISç»“æœ
    if not _is_topsis_result(topsis_df):
        raise ValueError("æ•°æ®æ ¼å¼é”™è¯¯ï¼šéœ€è¦TOPSISç»“æœæ ¼å¼çš„æ•°æ®")
    
    # ä»TOPSISç»“æœä¸­æå–ç»¼åˆéš¶å±åº¦å‘é‡
    membership_scores = _extract_membership_scores_from_topsis(topsis_df, num_levels)
    
    # åˆ›å»ºç­‰çº§ç»¼åˆè¯„å®šå™¨
    grade_assessor = GradeComprehensiveAssessment()
    
    # æ‰§è¡Œç­‰çº§ç»¼åˆè¯„å®š
    assessment_result = grade_assessor.assess_comprehensive_grade(membership_scores, num_levels)
    
    # ç”Ÿæˆç»“æœæ‘˜è¦
    summary = {
        "assessment_method": "ç­‰çº§ç»¼åˆè¯„å®š",
        "total_objects": assessment_result["num_objects"],
        "num_levels": assessment_result["num_levels"],
        "sample_results": {}
    }
    
    # æ·»åŠ å‰3ä¸ªå¯¹è±¡çš„ç¤ºä¾‹ç»“æœ
    sample_objects = list(membership_scores.keys())[:3]
    for obj_name in sample_objects:
        v_value = assessment_result["level_characteristic_values"][obj_name]
        k, alpha = assessment_result["binary_semantic_results"][obj_name]
        summary["sample_results"][obj_name] = {
            "membership_vector": membership_scores[obj_name],
            "level_characteristic_value": v_value,
            "binary_semantic": (k, alpha),
            "final_grade": k
        }
    
    result = {
        "resource_id": resource_id,
        "membership_scores": membership_scores,
        "assessment_result": assessment_result,
        "summary": summary
    }
    
    # å¦‚æœéœ€è¦ç¼“å­˜ç»“æœ
    if cache_result:
        # ç”Ÿæˆç»“æœèµ„æºID
        result_resource_id = generate_resource_id(
            "binary_semantic", 
            parent_resource_id=resource_id,
            step_name="grade_assessment"
        )
        result_uri = get_resource_uri(result_resource_id)
        
        # å°†ç»“æœè½¬æ¢ä¸ºDataFrameæ ¼å¼ç¼“å­˜
        result_data = []
        for obj_name in membership_scores.keys():
            v_value = assessment_result["level_characteristic_values"][obj_name]
            k, alpha = assessment_result["binary_semantic_results"][obj_name]
            
            result_data.append({
                'ä¸šåŠ¡å¯¹è±¡': obj_name,
                'ç»¼åˆéš¶å±åº¦å‘é‡': str(membership_scores[obj_name]),
                'çº§åˆ«ç‰¹å¾å€¼': v_value,
                'äºŒå…ƒè¯­ä¹‰_k': k,
                'äºŒå…ƒè¯­ä¹‰_alpha': alpha,
                'æœ€ç»ˆç­‰çº§': f"{k}çº§"
            })
        
        result_df = pd.DataFrame(result_data)
        DATA_CACHE[result_uri] = result_df
        
        # æ³¨å†Œèµ„æºåˆ°ç´¢å¼•ä¸­
        RESOURCE_INDEX[result_resource_id] = {
            "uri": result_uri,
            "data_shape": f"{len(result_df)}è¡ŒÃ—{len(result_df.columns)}åˆ—",
            "resource_type": "grade_assessment",
            "description": "ç­‰çº§ç»¼åˆè¯„å®šç»“æœï¼ˆçº§åˆ«ç‰¹å¾å€¼å’ŒäºŒå…ƒè¯­ä¹‰ï¼‰"
        }
        result["result_resource_id"] = result_resource_id
        
        print(f"ç­‰çº§ç»¼åˆè¯„å®šç»“æœå·²ç¼“å­˜: {result_uri}")
    
    # ç”Ÿæˆä¸‹ä¸€æ­¥æ“ä½œå»ºè®®
    next_actions = _generate_grade_assessment_next_actions(
        assessment_result["num_objects"], 
        assessment_result["num_levels"]
    )
    
    # ç”Ÿæˆå®Œæ•´çš„æ ¼å¼åŒ–æŠ¥å‘Š
    formatted_report = f"""
    {assessment_result["assessment_report"]}
    
    ## ğŸ“Š èµ„æºä¿¡æ¯
    - **åŸå§‹èµ„æºID**ï¼š`{resource_id}`
    - **ç­‰çº§ç»¼åˆè¯„å®šç»“æœèµ„æºID**ï¼š`{result_resource_id}`

    ## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®
    {next_actions}"""
    
    result["formatted_report"] = formatted_report
    
    return result

### æ£€æŸ¥æ˜¯å¦ä¸ºTOPSISç»“æœæ ¼å¼
def _is_topsis_result(df: pd.DataFrame) -> bool:
    """
    æ£€æŸ¥DataFrameæ˜¯å¦ä¸ºTOPSISç»“æœæ ¼å¼
    
    Args:
        df: è¦æ£€æŸ¥çš„DataFrame
        
    Returns:
        å¦‚æœæ˜¯TOPSISç»“æœæ ¼å¼è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if df.empty:
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„åˆ—
    required_columns = ['ä¸šåŠ¡å¯¹è±¡', 'è¯„ä¼°çº§åˆ«', 'ç›¸å¯¹æ¥è¿‘åº¦', 'ç»¼åˆéš¶å±åº¦']
    if all(col in df.columns for col in required_columns):
        return True
    
    return False

### ä»TOPSISç»“æœä¸­æå–ç»¼åˆéš¶å±åº¦å‘é‡
def _extract_membership_scores_from_topsis(topsis_df: pd.DataFrame, num_levels: int) -> Dict[str, List[float]]:
    """
    ä»TOPSISç»“æœä¸­æå–ç»¼åˆéš¶å±åº¦å‘é‡
    
    Args:
        topsis_df: TOPSISç»“æœDataFrame
        num_levels: è¯„ä¼°ç­‰çº§æ•°é‡
        
    Returns:
        ç»¼åˆéš¶å±åº¦å‘é‡å­—å…¸ï¼Œæ ¼å¼ä¸º{"T1": [u1, u2, u3, u4], ...}
    """
    membership_scores = {}
    
    # æŒ‰ä¸šåŠ¡å¯¹è±¡åˆ†ç»„
    grouped = topsis_df.groupby('ä¸šåŠ¡å¯¹è±¡')
    
    for obj_name, group in grouped:
        # æŒ‰è¯„ä¼°çº§åˆ«æ’åº
        group = group.sort_values('è¯„ä¼°çº§åˆ«')
        
        # æå–ç»¼åˆéš¶å±åº¦å€¼
        u_values = group['ç»¼åˆéš¶å±åº¦'].tolist()
        
        # ç¡®ä¿å‘é‡é•¿åº¦ä¸ç­‰çº§æ•°é‡ä¸€è‡´
        if len(u_values) == num_levels:
            membership_scores[obj_name] = u_values
        else:
            # å¦‚æœé•¿åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨ç›¸å¯¹æ¥è¿‘åº¦ä½œä¸ºåå¤‡
            v_values = group['ç›¸å¯¹æ¥è¿‘åº¦'].tolist()
            if len(v_values) == num_levels:
                # å½’ä¸€åŒ–ç›¸å¯¹æ¥è¿‘åº¦å‘é‡
                total_v = sum(v_values)
                if total_v > 0:
                    u_values = [v / total_v for v in v_values]
                    membership_scores[obj_name] = u_values
    
    return membership_scores

### æŸ¥æ‰¾æœ€æ–°çš„TOPSISç»“æœèµ„æº
def find_latest_topsis_resource() -> Optional[str]:
    """
    æŸ¥æ‰¾æœ€æ–°çš„TOPSISç»“æœèµ„æº
    
    Returns:
        æœ€æ–°çš„TOPSISç»“æœèµ„æºIDï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°è¿”å›None
    """
    topsis_resources = []
    
    for resource_id, resource_info in RESOURCE_INDEX.items():
        if resource_id.startswith("mcr_") and "topsiseval" in resource_id:
            topsis_resources.append(resource_id)
    
    if topsis_resources:
        # è¿”å›æœ€æ–°çš„èµ„æºï¼ˆæŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼‰
        return topsis_resources[-1]
    
    return None


# =========================================================================================
# âš–ï¸ VIKORç»¼åˆè¯„ä¼°æ¨¡å—
# =========================================================================================
### VIKORç»¼åˆè¯„ä¼°å‡½æ•°
@mcp.tool()
def perform_vikor_comprehensive_evaluation(resource_id: str, weights: List[float] = None, v: float = 0.5, cache_result: bool = True) -> Dict[str, Any]:
    """
    ä½¿ç”¨VIKORæ–¹æ³•è¿›è¡Œç»¼åˆè¯„ä¼°è®¡ç®— - å¤šå‡†åˆ™å¦¥åæ’åºåˆ†æå·¥å…·
    
    è¯¥å‡½æ•°å®ç°VIKORï¼ˆVIKOR compromise ranking methodï¼‰ç®—æ³•ï¼Œ
    åŸºäºå¦¥åè§£çš„æ¦‚å¿µè¿›è¡Œå¤šå±æ€§å†³ç­–åˆ†æï¼Œè®¡ç®—Så€¼ï¼ˆç¾¤ä½“æ•ˆç”¨ï¼‰ã€Rå€¼ï¼ˆä¸ªä½“é—æ†¾ï¼‰å’ŒQå€¼ï¼ˆå¦¥åè§£ï¼‰ã€‚
    
    
    Args:
        resource_id: éš¶å±åº¦çŸ©é˜µèµ„æºIDï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
                     ä¾‹å¦‚ï¼š'membership_matrix_001' æˆ– 'data://membership_matrix_001'
        weights: å› å­æƒé‡åˆ—è¡¨ï¼Œç”¨äºè®¾ç½®å„è¯„ä»·å› å­çš„é‡è¦æ€§æƒé‡
                ä¾‹å¦‚ï¼š[0.32, 0.24, 0.24, 0.20] è¡¨ç¤º4ä¸ªå› å­çš„æƒé‡åˆ†é…
                é»˜è®¤ä¸ºNoneï¼Œä½¿ç”¨ç­‰æƒé‡ï¼ˆæ¯ä¸ªå› å­æƒé‡ç›¸åŒï¼‰
        v: ç­–ç•¥æƒé‡ï¼Œ0â‰¤vâ‰¤1ï¼Œè°ƒèŠ‚ç¾¤ä½“æ•ˆç”¨ä¸ä¸ªä½“é—æ†¾çš„æƒè¡¡
           v=0ï¼šä»¥ä¸ªä½“æœ€å¤§é—æ†¾ä¸ºåŸºç¡€ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
           v=1ï¼šä»¥ç¾¤ä½“å¤šæ•°æ•ˆç”¨ä¸ºåŸºç¡€ï¼ˆæ¿€è¿›ç­–ç•¥ï¼‰
           v=0.5ï¼šå¹³è¡¡ç­–ç•¥ï¼ˆé»˜è®¤ï¼‰
        cache_result: æ˜¯å¦ç¼“å­˜è®¡ç®—ç»“æœï¼ˆé»˜è®¤Trueï¼‰ï¼Œè®¾ç½®ä¸ºFalseå¯è·³è¿‡ç»“æœç¼“å­˜
        
    Returnsï¼š
        
        formatted_reportæ¨¡æ¿æ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼æŒ‰ç…§æ­¤æ ¼å¼è¾“å‡ºï¼Œä¸å¾—æœ‰ä»»ä½•é¢å¤–å†…å®¹ï¼‰ï¼š
        
        ## ğŸ“Š è¯„ä¼°æ¦‚è¿°
        - **è¯„ä¼°æ–¹æ³•**ï¼šVIKORï¼ˆå¤šå‡†åˆ™å¦¥åæ’åºæ³•ï¼‰
        - **è¯„ä¼°å¯¹è±¡æ•°é‡**ï¼š5ä¸ª
        - **è¯„ä»·å› å­æ•°é‡**ï¼š4ä¸ª
        - **æƒé‡åˆ†é…**ï¼š[0.32, 0.24, 0.24, 0.2]
        - **ç­–ç•¥æƒé‡**ï¼šv = 0.5ï¼ˆ0=ä¿å®ˆç­–ç•¥ï¼Œ1=æ¿€è¿›ç­–ç•¥ï¼Œ0.5=å¹³è¡¡ç­–ç•¥ï¼‰
        
        ## ğŸ“ˆ å¦¥åæ’åºç»“æœï¼ˆQå€¼æ’åºï¼‰
        | ä¸šåŠ¡å¯¹è±¡ | Så€¼(ç¾¤ä½“æ•ˆç”¨) | Rå€¼(ä¸ªä½“é—æ†¾) | Qå€¼(å¦¥åè§£) | æ’åº |
        |----------|---------------|---------------|------------|------|
        | T2 | 0.4025 | 0.1364 | 0.0000 | 1 |
        | T1 | 0.3973 | 0.1455 | 0.2134 | 2 |
        | T4 | 0.4261 | 0.2070 | 0.7167 | 3 |
        | T3 | 0.5876 | 0.2006 | 0.9254 | 4 |
        | T5 | 0.5434 | 0.2229 | 1.0000 | 5 |
        
        ## ğŸ“‹ å¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
        - **VIKORç»¼åˆè¯„ä¼°è¡¨**ï¼šåŒ…å«Sã€Rã€Qå€¼çš„å®Œæ•´è¯„ä¼°ç»“æœ
        - **å¦¥åæ’åºè¡¨**ï¼šåŸºäºQå€¼çš„æ–¹æ¡ˆæ’åºç»“æœ
        - **æ•æ„Ÿæ€§åˆ†æè¡¨**ï¼šä¸åŒç­–ç•¥æƒé‡vä¸‹çš„ç»“æœå¯¹æ¯”
        
        ## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®
        1. **æŸ¥çœ‹è¯¦ç»†æ’åºç»“æœ**ï¼šåˆ†æQå€¼æ’åºï¼Œè¯†åˆ«æœ€ä¼˜å¦¥åè§£
        2. **æ•æ„Ÿæ€§åˆ†æ**ï¼šè°ƒæ•´ç­–ç•¥æƒé‡vï¼ˆå½“å‰v=0.5ï¼‰ï¼Œè§‚å¯Ÿç»“æœç¨³å®šæ€§
        3. **ç»“æœéªŒè¯**ï¼šç»“åˆSå€¼å’ŒRå€¼ï¼ŒéªŒè¯å¦¥åè§£çš„åˆç†æ€§
        4. **å†³ç­–æ”¯æŒ**ï¼šåŸºäºQå€¼æ’åºï¼Œä¸ºæœ€ç»ˆå†³ç­–æä¾›é‡åŒ–ä¾æ®
        
        ğŸ’¡ **VIKORç»“æœè§£è¯»**ï¼š
        - **Qå€¼è¶Šå°**ï¼šå¦¥åè§£è¶Šä¼˜ï¼Œæ¨èåº¦è¶Šé«˜
        - **Så€¼**ï¼šç¾¤ä½“æ•ˆç”¨ï¼Œè¶Šå°è¡¨ç¤ºæ•´ä½“è¡¨ç°è¶Šå¥½
        - **Rå€¼**ï¼šä¸ªä½“é—æ†¾ï¼Œè¶Šå°è¡¨ç¤ºæœ€å·®å±æ€§è¡¨ç°è¶Šå¥½
    """
    # å¤„ç†URIæ ¼å¼çš„èµ„æºID
    if resource_id.startswith("data://"):
        resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
    
    uri = get_resource_uri(resource_id)
    
    if uri not in DATA_CACHE:
        raise ValueError(f"èµ„æºä¸å­˜åœ¨: {uri}")
    
    # è·å–éš¶å±åº¦çŸ©é˜µæ•°æ®
    membership_df = DATA_CACHE[uri]
    
    # æ£€æŸ¥æ•°æ®æ ¼å¼
    if not _is_membership_matrix(membership_df):
        raise ValueError("æ•°æ®æ ¼å¼é”™è¯¯ï¼šéœ€è¦éš¶å±åº¦çŸ©é˜µæ ¼å¼çš„æ•°æ®")
    
    # è½¬æ¢éš¶å±åº¦çŸ©é˜µä¸ºè§„èŒƒåŒ–ç‰¹å¾å€¼çŸ©é˜µæ ¼å¼
    normalized_matrices = _convert_membership_to_normalized_matrix(membership_df)
    
    # åˆ›å»ºVIKORè¯„ä¼°å™¨
    vikor_evaluator = VIKORComprehensiveEvaluation()
    
    # è®¡ç®—VIKORç»¼åˆå¾—åˆ†
    comprehensive_scores = vikor_evaluator.calculate_comprehensive_scores(normalized_matrices, weights, v)
    
    # ç”Ÿæˆç»“æœæ‘˜è¦
    summary = {
        "evaluation_method": "VIKOR",
        "total_objects": len(comprehensive_scores),
        "total_factors": len(weights) if weights else 4,  # é»˜è®¤4ä¸ªå› å­
        "weight_distribution": weights if weights else [0.25, 0.25, 0.25, 0.25],
        "strategy_weight": v,
        "sample_results": {}
    }
    
    # æ·»åŠ å‰3ä¸ªå¯¹è±¡çš„ç¤ºä¾‹ç»“æœ
    sample_objects = list(comprehensive_scores.keys())[:3]
    for obj_name in sample_objects:
        summary["sample_results"][obj_name] = {
            "S_values": comprehensive_scores[obj_name]["S"][:2],  # å‰2ä¸ªçº§åˆ«
            "R_values": comprehensive_scores[obj_name]["R"][:2],  # å‰2ä¸ªçº§åˆ«
            "Q_values": comprehensive_scores[obj_name]["Q"][:2]   # å‰2ä¸ªçº§åˆ«
        }
    
    result = {
        "resource_id": resource_id,
        "comprehensive_scores": comprehensive_scores,
        "summary": summary
    }
    
    # å¦‚æœéœ€è¦ç¼“å­˜ç»“æœ
    if cache_result:
        # ç”Ÿæˆç»“æœèµ„æºID
        result_resource_id = generate_resource_id(
            "multi_criteria", 
            parent_resource_id=resource_id,
            step_name="vikor_evaluation"
        )
        result_uri = get_resource_uri(result_resource_id)
        
        # å°†ç»“æœè½¬æ¢ä¸ºDataFrameæ ¼å¼ç¼“å­˜
        result_data = []
        for obj_name, scores in comprehensive_scores.items():
            for level_idx in range(len(scores["S"])):
                result_data.append({
                    'ä¸šåŠ¡å¯¹è±¡': obj_name,
                    'è¯„ä¼°çº§åˆ«': f'e{level_idx + 1}',
                    'Så€¼': scores["S"][level_idx],
                    'Rå€¼': scores["R"][level_idx],
                    'Qå€¼': scores["Q"][level_idx]
                })
        
        result_df = pd.DataFrame(result_data)
        DATA_CACHE[result_uri] = result_df
        result["result_resource_id"] = result_resource_id
        
        print(f"VIKORç»¼åˆè¯„ä¼°ç»“æœå·²ç¼“å­˜: {result_uri}")
    
    # ç”ŸæˆVIKORæ ¼å¼åŒ–æŠ¥å‘Šå¹¶æ·»åŠ åˆ°ç»“æœä¸­ - ç›´æ¥ä½¿ç”¨æ¨¡æ¿å­—ç¬¦ä¸²é¿å…LLMåŠ å·¥
    # æ„å»ºVIKORç»“æœè¡¨æ ¼
    comprehensive_scores = result.get("comprehensive_scores", {})
    vikor_table = "| ä¸šåŠ¡å¯¹è±¡ | Så€¼(ç¾¤ä½“æ•ˆç”¨) | Rå€¼(ä¸ªä½“é—æ†¾) | Qå€¼(å¦¥åè§£) | æ’åº |\n"
    vikor_table += "|----------|---------------|---------------|------------|------|\n"
    
    # è®¡ç®—Qå€¼æ’åº
    q_values = []
    for obj_name, scores in comprehensive_scores.items():
        if scores["Q"]:
            q_values.append((obj_name, scores["Q"][0]))
    
    # æŒ‰Qå€¼å‡åºæ’åˆ—ï¼ˆQå€¼è¶Šå°è¶Šå¥½ï¼‰
    q_values.sort(key=lambda x: x[1])
    
    for rank, (obj_name, q_val) in enumerate(q_values, 1):
        scores = comprehensive_scores[obj_name]
        s_val = scores["S"][0] if scores["S"] else 0
        r_val = scores["R"][0] if scores["R"] else 0
        vikor_table += f"| {obj_name} | {s_val:.4f} | {r_val:.4f} | {q_val:.4f} | {rank} |\n"
    
    # ç”Ÿæˆå¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
    available_tables = """
    - **VIKORç»¼åˆè¯„ä¼°è¡¨**ï¼šåŒ…å«Sã€Rã€Qå€¼çš„å®Œæ•´è¯„ä¼°ç»“æœ
    - **å¦¥åæ’åºè¡¨**ï¼šåŸºäºQå€¼çš„æ–¹æ¡ˆæ’åºç»“æœ
    - **æ•æ„Ÿæ€§åˆ†æè¡¨**ï¼šä¸åŒç­–ç•¥æƒé‡vä¸‹çš„ç»“æœå¯¹æ¯”
    """
    
    # ç”Ÿæˆä¸‹ä¸€æ­¥æ“ä½œå»ºè®®
    next_actions = _generate_vikor_next_actions(
        summary.get("total_objects", 0), 
        v
    )
    
    result["formatted_report"] = f"""
    ## ğŸ“Š è¯„ä¼°æ¦‚è¿°
    - **è¯„ä¼°æ–¹æ³•**ï¼šVIKORï¼ˆå¤šå‡†åˆ™å¦¥åæ’åºæ³•ï¼‰
    - **è¯„ä¼°å¯¹è±¡æ•°é‡**ï¼š{summary.get("total_objects", 0)} ä¸ª
    - **è¯„ä»·å› å­æ•°é‡**ï¼š{summary.get("total_factors", 0)} ä¸ª
    - **æƒé‡åˆ†é…**ï¼š{summary.get("weight_distribution", [])}
    - **ç­–ç•¥æƒé‡**ï¼šv = {v}ï¼ˆ0=ä¿å®ˆç­–ç•¥ï¼Œ1=æ¿€è¿›ç­–ç•¥ï¼Œ0.5=å¹³è¡¡ç­–ç•¥ï¼‰
    - **åŸå§‹èµ„æºID**ï¼š`{resource_id}`
    - **VIKORè®¡ç®—ç»“æœèµ„æºID**ï¼š`{result_resource_id}`

    ## ğŸ“ˆ å¦¥åæ’åºç»“æœï¼ˆQå€¼æ’åºï¼‰
    {vikor_table}

    ## ğŸ“‹ å¯æŸ¥è¯¢è¡¨æ ¼æ¸…å•
    {available_tables}

    ## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®
    {next_actions}"""
        
    return result

### ç”ŸæˆVIKORè®¡ç®—åçš„ä¸‹ä¸€æ­¥å»ºè®®
def _generate_vikor_next_actions(num_objects: int, v: float) -> str:
    """
    ç”ŸæˆVIKORè®¡ç®—åçš„ä¸‹ä¸€æ­¥å»ºè®®
    
    Args:
        num_objects: è¯„ä¼°çš„ä¸šåŠ¡å¯¹è±¡æ•°é‡
        v: ç­–ç•¥æƒé‡å€¼
        
    Returns:
        ä¸‹ä¸€æ­¥å»ºè®®å­—ç¬¦ä¸²
    """
    return f"""
    **ä¸‹ä¸€æ­¥å»ºè®®ï¼š**
    1. **å¦¥åè§£åˆ†æ**ï¼šåˆ†æQå€¼æ’åºï¼Œè¯†åˆ«æœ€ä¼˜å¦¥åè§£å’Œæ¬¡ä¼˜æ–¹æ¡ˆ
    2. **ç¾¤ä½“æ•ˆç”¨åˆ†æ**ï¼šç»“åˆSå€¼åˆ†æå„æ–¹æ¡ˆçš„æ•´ä½“è¡¨ç°
    3. **ä¸ªä½“é—æ†¾åˆ†æ**ï¼šç»“åˆRå€¼åˆ†æå„æ–¹æ¡ˆçš„æœ€å·®å±æ€§è¡¨ç°
    4. **ç­‰çº§ç»¼åˆè¯„å®š**ï¼šå¯è°ƒç”¨ `perform_grade_comprehensive_assessment` å‡½æ•°è¿›è¡Œç­‰çº§ç»¼åˆè¯„å®š
    5. **æ•æ„Ÿæ€§åˆ†æ**ï¼šè°ƒæ•´ç­–ç•¥æƒé‡vï¼ˆå½“å‰v={v}ï¼‰ï¼Œè§‚å¯Ÿç»“æœç¨³å®šæ€§
    6. **å¯¹æ¯”åˆ†æ**ï¼šå¯è°ƒç”¨ `perform_topsis_comprehensive_evaluation` å‡½æ•°è¿›è¡ŒTOPSISå¯¹æ¯”åˆ†æ

    **VIKORæ–¹æ³•ç‰¹ç‚¹ï¼š**
    - **å¦¥åè§£æ¦‚å¿µ**ï¼šåŸºäºç¾¤ä½“æ•ˆç”¨å’Œä¸ªä½“é—æ†¾çš„å¹³è¡¡
    - **ç­–ç•¥å¯è°ƒ**ï¼šé€šè¿‡vå‚æ•°è°ƒèŠ‚ä¿å®ˆä¸æ¿€è¿›ç­–ç•¥
    - **å¤šç»´åº¦è¯„ä¼°**ï¼šåŒæ—¶è€ƒè™‘æ•´ä½“è¡¨ç°å’Œä¸ªä½“çŸ­æ¿

    æœ¬æ¬¡è¯„ä¼°å…±åˆ†æäº† {num_objects} ä¸ªä¸šåŠ¡å¯¹è±¡ï¼Œé‡‡ç”¨ç­–ç•¥æƒé‡v={v}ï¼Œ
    å»ºè®®é‡ç‚¹å…³æ³¨Qå€¼æ’åºå’Œå¦¥åè§£ç‰¹å¾ï¼Œä¸ºå¤šå‡†åˆ™å†³ç­–æä¾›æ”¯æŒã€‚
    """


# =========================================================================================
# ğŸ“‹ è¾…åŠ©å·¥å…·ä¸å®ç”¨å‡½æ•°
# =========================================================================================
### å¯¼å‡ºèµ„æºä¸ºCSVå·¥å…·
@mcp.tool()
def export_resource_to_csv(resource_id: str) -> Dict[str, Any]:
    """
    å°†æŒ‡å®šèµ„æºå¯¼å‡ºä¸ºCSVæ ¼å¼ï¼Œå¹¶æä¾›è¯¦ç»†çš„å­—æ®µè§£é‡Šå’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®
    
    Args:
        resource_id: è¦å¯¼å‡ºçš„èµ„æºIDï¼ˆå¯ä»¥æ˜¯çº¯IDæˆ–å®Œæ•´URIï¼‰
        
    Returns:
        - æŒ‡å®šèµ„æºçš„åŸå§‹æ•°æ®ï¼ŒåŒ…å«æ‰€æœ‰å­—æ®µå’Œå€¼
        - å­—æ®µè§£é‡Šï¼šå¯¹æ¯ä¸ªå­—æ®µçš„å«ä¹‰å’Œæ•°æ®ç±»å‹è¿›è¡Œè¯¦ç»†è¯´æ˜
        - ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®ï¼šæ ¹æ®æ•°æ®ç‰¹å¾å»ºè®®åç»­åˆ†ææ­¥éª¤
        - é™¤æ­¤ä»¥å¤–çš„ä»»ä½•ä¿¡æ¯éƒ½ä¸è¦è¾“å‡º
    """
    try:
        # å¤„ç†URIæ ¼å¼çš„èµ„æºID
        if resource_id.startswith("data://"):
            resource_id = resource_id[7:]  # ç§»é™¤ "data://" å‰ç¼€
        
        # é¦–å…ˆå°è¯•ä»å†…å­˜ç¼“å­˜åŠ è½½
        uri = get_resource_uri(resource_id)
        if uri in DATA_CACHE:
            df = DATA_CACHE[uri]
        else:
            # å¦‚æœå†…å­˜ç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æŒä¹…åŒ–å­˜å‚¨åŠ è½½
            df = load_resource_from_persistent_storage(resource_id)
        
        # æ™ºèƒ½è§£ææ•°æ®ç»“æ„
        # æ£€æŸ¥æ˜¯å¦ä¸ºéš¶å±åº¦çŸ©é˜µçš„åµŒå¥—ç»“æ„
        if _is_membership_matrix(df):
            # å°†åµŒå¥—ç»“æ„è½¬æ¢ä¸ºæ‰å¹³åŒ–ç»“æ„
            df = _flatten_membership_matrix(df)
        
        # è½¬æ¢ä¸ºCSVæ ¼å¼
        csv_content = df.to_csv(index=False, encoding='utf-8')
        
        # ç”Ÿæˆå­—æ®µè§£é‡Š
        field_descriptions = _generate_field_descriptions(df, resource_id)
        
        # ç”Ÿæˆä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®
        next_actions = _generate_next_actions(df, resource_id)
        
        # ç”Ÿæˆèµ„æºä¿¡æ¯
        resource_info = _generate_resource_info(resource_id, df)
        
        # ç”Ÿæˆæ•°æ®ç»Ÿè®¡æ‘˜è¦
        data_summary = _generate_data_summary(df)
        
        # ç”Ÿæˆæ ‡å‡†åŒ–çš„TOPSISè¡¨æ ¼é¢„è§ˆ
        standardized_table = _generate_standardized_topsis_table(df, resource_id)
        
        # å¦‚æœæ˜¯TOPSISç»“æœï¼Œç”ŸæˆåŸå§‹æ•°æ®æ ¼å¼é¢„è§ˆ
        original_data_preview = _generate_topsis_original_data_preview(df, resource_id)
        
        result = {
            "csv_content": csv_content,
            "field_descriptions": field_descriptions,
            "next_actions": next_actions,
            "resource_info": resource_info,
            "data_summary": data_summary
        }
        
        # å¦‚æœç”Ÿæˆäº†æ ‡å‡†åŒ–è¡¨æ ¼ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
        if standardized_table:
            result["standardized_table"] = standardized_table
            
        # å¦‚æœç”Ÿæˆäº†åŸå§‹æ•°æ®é¢„è§ˆï¼Œæ·»åŠ åˆ°ç»“æœä¸­
        if original_data_preview:
            result["original_data_preview"] = original_data_preview
        
        return result
        
    except Exception as e:
        raise ValueError(f"å¯¼å‡ºèµ„æºå¤±è´¥: {str(e)}")

### ç”Ÿæˆåˆ†ææ€»ç»“å‡½æ•°
def generate_analysis_summary(df: pd.DataFrame, numeric_analysis: Dict, categorical_analysis: Dict, data_quality_summary: Dict, polarity_warnings: Dict = None) -> Dict[str, Any]:
    """ç”Ÿæˆåˆ†ææ€»ç»“"""
    
    numeric_count = len(numeric_analysis)
    categorical_count = len(categorical_analysis)
    total_fields = numeric_count + categorical_count
    
    # ç»Ÿè®¡å…³é”®æŒ‡æ ‡
    summary_stats = {
        "dataset_size": f"{len(df)} è¡Œ Ã— {len(df.columns)} åˆ—",
        "field_types": {
            "numeric_fields": numeric_count,
            "categorical_fields": categorical_count,
            "total_fields": total_fields
        },
        "data_quality": {
            "missing_rate": f"{data_quality_summary['overall_quality']['overall_missing_rate']:.2f}%",
            "quality_rating": data_quality_summary['overall_quality']['quality_rating']
        }
    }
    
    # ç”Ÿæˆå…³é”®å‘ç°
    key_findings = []
    
    # æ•°å€¼å­—æ®µå‘ç°
    if numeric_analysis:
        for col, analysis in numeric_analysis.items():
            basic_stats = analysis['basic_stats']
            data_quality = analysis['data_quality']
            
            findings = f"æ•°å€¼å­—æ®µ '{col}': "
            findings += f"èŒƒå›´ [{basic_stats['min']:.2f}, {basic_stats['max']:.2f}], "
            findings += f"å‡å€¼ {basic_stats['mean']:.2f}, "
            findings += f"ç¼ºå¤±ç‡ {data_quality['missing_percentage']:.1f}%"
            key_findings.append(findings)
    
    # åˆ†ç±»å­—æ®µå‘ç°
    if categorical_analysis:
        for col, analysis in categorical_analysis.items():
            freq_analysis = analysis['frequency_analysis']
            data_quality = analysis['data_quality']
            
            findings = f"åˆ†ç±»å­—æ®µ '{col}': "
            findings += f"{data_quality['unique_count']} ä¸ªå”¯ä¸€å€¼, "
            findings += f"ç¼ºå¤±ç‡ {data_quality['missing_percentage']:.1f}%"
            key_findings.append(findings)
    
    # æ•°æ®è´¨é‡å‘ç°
    quality_stats = data_quality_summary['overall_quality']
    if quality_stats['overall_missing_rate'] > 5:
        key_findings.append(f"âš ï¸  æ•°æ®è´¨é‡æ³¨æ„: æ€»ä½“ç¼ºå¤±ç‡ {quality_stats['overall_missing_rate']:.1f}% è¾ƒé«˜")
    else:
        key_findings.append(f"âœ… æ•°æ®è´¨é‡è‰¯å¥½: æ€»ä½“ç¼ºå¤±ç‡ {quality_stats['overall_missing_rate']:.1f}%")
    
    # ææ€§æ£€æŸ¥å‘ç°
    if polarity_warnings:
        if not polarity_warnings.get('is_consistent', True):
            key_findings.append("âš ï¸  **é‡è¦è­¦å‘Š**: æ£€æµ‹åˆ°æ··åˆææ€§å­—æ®µï¼")
            key_findings.append("   åç»­éš¶å±åº¦è®¡ç®—è¦æ±‚æ‰€æœ‰å­—æ®µææ€§ä¸€è‡´")
        else:
            key_findings.append("âœ… ææ€§ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
    
    # ç”Ÿæˆå»ºè®®
    recommendations = [
        "å»ºè®®å¯¹ç¼ºå¤±å€¼è¾ƒå¤šçš„å­—æ®µè¿›è¡Œæ•°æ®æ¸…æ´—",
        "æ•°å€¼å­—æ®µå¯è¿›ä¸€æ­¥è¿›è¡Œç›¸å…³æ€§åˆ†æå’Œåˆ†å¸ƒæ£€éªŒ",
        "åˆ†ç±»å­—æ®µå¯è¿›è¡Œé¢‘æ¬¡åˆ†æå’Œå¯è§†åŒ–å±•ç¤º"
    ]
    
    # æ·»åŠ ææ€§ç›¸å…³å»ºè®®
    if polarity_warnings and not polarity_warnings.get('is_consistent', True):
        recommendations.insert(0, "**é‡è¦**: è¯·ç»Ÿä¸€æ‰€æœ‰æ•°å€¼å­—æ®µçš„ææ€§ï¼ˆå…¨éƒ¨ä¸ºæå¤§å‹æˆ–æå°å‹ï¼‰")
        recommendations.insert(1, "æ£€æŸ¥å­—æ®µææ€§å»ºè®®ï¼Œç¡®ä¿ç¬¦åˆå®é™…ä¸šåŠ¡é€»è¾‘")
    
    return {
        "summary_statistics": summary_stats,
        "key_findings": key_findings,
        "recommendations": recommendations
    }


# =========================================================================================
# ğŸ”§ å†…éƒ¨è¾…åŠ©å‡½æ•°
# =========================================================================================
### ç”Ÿæˆä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®å‡½æ•°
def _generate_next_actions(df: pd.DataFrame, resource_id: str) -> List[str]:
    """
    ç”Ÿæˆä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®
    
    Args:
        df: æ•°æ®æ¡†
        resource_id: èµ„æºID
        
    Returns:
        è¡ŒåŠ¨å»ºè®®åˆ—è¡¨
    """
    next_actions = []
    
    if resource_id.startswith("mcr_") and "topsiseval" in resource_id:
        # TOPSISç»“æœåçš„å»ºè®®
        next_actions = [
            "ğŸ“Š **åˆ†æç›¸å¯¹æ¥è¿‘åº¦æ’åº**ï¼šæŒ‰ç›¸å¯¹æ¥è¿‘åº¦ä»å¤§åˆ°å°æ’åºï¼Œè¯†åˆ«æœ€ä¼˜æ–¹æ¡ˆ",
            "ğŸ” **å¤šçº§åˆ«æ¯”è¾ƒ**ï¼šæ¯”è¾ƒä¸åŒè¯„ä¼°çº§åˆ«ä¸‹çš„æœ€ä¼˜æ–¹æ¡ˆ",
            "ğŸ“ˆ **å¯è§†åŒ–åˆ†æ**ï¼šå»ºè®®ä½¿ç”¨å›¾è¡¨å±•ç¤ºå„æ–¹æ¡ˆçš„ç›¸å¯¹æ¥è¿‘åº¦åˆ†å¸ƒ",
            "ğŸ¤ **å†³ç­–æ”¯æŒ**ï¼šåŸºäºç›¸å¯¹æ¥è¿‘åº¦ç»“æœåˆ¶å®šèµ„æºåˆ†é…æˆ–ä¼˜åŒ–ç­–ç•¥",
            "ğŸ”„ **æ•æ„Ÿæ€§åˆ†æ**ï¼šå¯è°ƒæ•´çº§åˆ«å‚æ•°è¿›è¡Œæ•æ„Ÿæ€§åˆ†æ",
            "ğŸ’¾ **æ•°æ®å¯¼å‡º**ï¼šå½“å‰å·²å¯¼å‡ºåŸå§‹æ•°æ®æ ¼å¼ï¼Œå¯ç›´æ¥ç”¨äºè¿›ä¸€æ­¥åˆ†æ"
        ]
    elif resource_id.startswith("mc_") and "membership" in resource_id:
        # éš¶å±åº¦è®¡ç®—åçš„å»ºè®®
        next_actions = [
            "ğŸ¯ **æ‰§è¡ŒTOPSISè¯„ä¼°**ï¼šä½¿ç”¨perform_topsis_comprehensive_evaluationè¿›è¡Œå¤šå‡†åˆ™å†³ç­–",
            "ğŸ“‹ **æ£€æŸ¥éš¶å±åº¦åˆ†å¸ƒ**ï¼šéªŒè¯éš¶å±åº¦çŸ©é˜µçš„åˆç†æ€§",
            "âš™ï¸ **å‚æ•°è°ƒä¼˜**ï¼šå¦‚æœ‰éœ€è¦ï¼Œå¯è°ƒæ•´çº§åˆ«å‚æ•°é‡æ–°è®¡ç®—",
            "ğŸ“Š **å¯è§†åŒ–å±•ç¤º**ï¼šå»ºè®®ä½¿ç”¨çƒ­åŠ›å›¾å±•ç¤ºéš¶å±åº¦åˆ†å¸ƒ"
        ]
    elif resource_id.startswith("raw_"):
        # åŸå§‹æ•°æ®åçš„å»ºè®®
        next_actions = [
            "ğŸ” **æ•°æ®è´¨é‡æ£€æŸ¥**ï¼šä½¿ç”¨analyze_data_fieldsåˆ†ææ•°æ®è´¨é‡",
            "ğŸ¯ **æ‰§è¡Œéš¶å±åº¦è®¡ç®—**ï¼šä½¿ç”¨calculate_membership_with_configè¿›è¡Œéš¶å±åº¦è®¡ç®—",
            "âš™ï¸ **é…ç½®çº§åˆ«å‚æ•°**ï¼šä¸ºæ¯ä¸ªå­—æ®µè®¾ç½®åˆé€‚çš„çº§åˆ«å‚æ•°",
            "ğŸ“‹ **ææ€§ç¡®è®¤**ï¼šç¡®ä¿æ‰€æœ‰å­—æ®µçš„ææ€§è®¾ç½®æ­£ç¡®"
        ]
    
    return next_actions

### ç”Ÿæˆèµ„æºåŸºæœ¬ä¿¡æ¯å‡½æ•°
def _generate_resource_info(resource_id: str, df: pd.DataFrame) -> Dict[str, Any]:
    """
    ç”Ÿæˆèµ„æºåŸºæœ¬ä¿¡æ¯
    
    Args:
        resource_id: èµ„æºID
        df: æ•°æ®æ¡†
        
    Returns:
        èµ„æºä¿¡æ¯å­—å…¸
    """
    resource_type = "æœªçŸ¥ç±»å‹"
    description = ""
    
    if resource_id.startswith("raw_"):
        resource_type = "åŸå§‹æ•°æ®"
        description = "ä¸Šä¼ çš„åŸå§‹ä¸šåŠ¡æ•°æ®ï¼ŒåŒ…å«ä¸šåŠ¡å¯¹è±¡å’Œå„è¯„ä»·æŒ‡æ ‡"
    elif resource_id.startswith("mc_") and "membership" in resource_id:
        resource_type = "éš¶å±åº¦çŸ©é˜µ"
        description = "ç»è¿‡éš¶å±åº¦è®¡ç®—åçš„ç»“æœï¼ŒåŒ…å«ä¸šåŠ¡å¯¹è±¡åœ¨å„è¯„ä»·å› å­å’Œçº§åˆ«ä¸‹çš„éš¶å±åº¦"
    elif resource_id.startswith("mcr_") and "topsiseval" in resource_id:
        resource_type = "TOPSISè¯„ä¼°ç»“æœ"
        description = "åŸºäºéš¶å±åº¦çŸ©é˜µçš„TOPSISå¤šå‡†åˆ™å†³ç­–åˆ†æç»“æœï¼ŒåŒ…å«ç›¸å¯¹æ¥è¿‘åº¦ç­‰å…³é”®æŒ‡æ ‡"
    
    return {
        "resource_id": resource_id,
        "resource_type": resource_type,
        "description": description,
        "data_shape": f"{len(df)} è¡Œ Ã— {len(df.columns)} åˆ—",
        "columns": list(df.columns)
    }

### ç”Ÿæˆæ•°æ®ç»Ÿè®¡æ‘˜è¦å‡½æ•°
def _generate_data_summary(df: pd.DataFrame) -> Dict[str, Any]:
    """
    ç”Ÿæˆæ•°æ®ç»Ÿè®¡æ‘˜è¦
    
    Args:
        df: æ•°æ®æ¡†
        
    Returns:
        æ•°æ®æ‘˜è¦å­—å…¸
    """
    summary = {
        "total_rows": len(df),
        "total_columns": len(df.columns),
        "column_names": list(df.columns),
        "data_types": {}
    }
    
    # åˆ†ææ•°å€¼åˆ—
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    if len(numeric_columns) > 0:
        summary["numeric_summary"] = {}
        for col in numeric_columns:
            summary["numeric_summary"][col] = {
                "min": float(df[col].min()),
                "max": float(df[col].max()),
                "mean": float(df[col].mean()),
                "std": float(df[col].std())
            }
    
    # åˆ†æåˆ†ç±»åˆ—
    categorical_columns = df.select_dtypes(include=['object']).columns
    if len(categorical_columns) > 0:
        summary["categorical_summary"] = {}
        for col in categorical_columns:
            summary["categorical_summary"][col] = {
                "unique_count": len(df[col].unique()),
                "sample_values": list(df[col].unique()[:5])  # æ˜¾ç¤ºå‰5ä¸ªå”¯ä¸€å€¼
            }
    
    return summary

### ç”Ÿæˆæ ‡å‡†åŒ–çš„TOPSISç»“æœè¡¨æ ¼å‡½æ•°
def _generate_standardized_topsis_table(df: pd.DataFrame, resource_id: str) -> str:
    """
    ç”Ÿæˆæ ‡å‡†åŒ–çš„TOPSISç»“æœè¡¨æ ¼
    
    Args:
        df: TOPSISç»“æœæ•°æ®æ¡†
        resource_id: èµ„æºID
        
    Returns:
        æ ‡å‡†åŒ–çš„è¡¨æ ¼å­—ç¬¦ä¸²
    """
    # æ£€æŸ¥æ˜¯å¦ä¸ºTOPSISç»“æœ
    if not (resource_id.startswith("mcr_") and "topsiseval" in resource_id):
        return ""
    
    # æ£€æŸ¥æ•°æ®æ¡†æ˜¯å¦åŒ…å«å¿…è¦çš„åˆ—
    required_columns = ['ä¸šåŠ¡å¯¹è±¡', 'è¯„ä¼°çº§åˆ«', 'ç›¸å¯¹æ¥è¿‘åº¦']
    if not all(col in df.columns for col in required_columns):
        return ""
    
    # æŒ‰ä¸šåŠ¡å¯¹è±¡å’Œè¯„ä¼°çº§åˆ«é‡æ–°ç»„ç»‡æ•°æ®
    try:
        # åˆ›å»ºé€è§†è¡¨ï¼šä¸šåŠ¡å¯¹è±¡ä¸ºè¡Œï¼Œè¯„ä¼°çº§åˆ«ä¸ºåˆ—ï¼Œç›¸å¯¹æ¥è¿‘åº¦ä¸ºå€¼
        pivot_df = df.pivot_table(
            index='ä¸šåŠ¡å¯¹è±¡', 
            columns='è¯„ä¼°çº§åˆ«', 
            values='ç›¸å¯¹æ¥è¿‘åº¦', 
            aggfunc='first'
        ).reset_index()
        
        # ç¡®ä¿åˆ—åæŒ‰çº§åˆ«é¡ºåºæ’åˆ—
        level_columns = [col for col in pivot_df.columns if col.startswith('e')]
        level_columns.sort()  # æŒ‰e1, e2, e3, e4æ’åº
        
        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
        pivot_df = pivot_df[['ä¸šåŠ¡å¯¹è±¡'] + level_columns]
        
        # é‡å‘½ååˆ—åä¸ºä¸­æ–‡
        column_mapping = {'ä¸šåŠ¡å¯¹è±¡': 'ä¸šåŠ¡å¯¹è±¡'}
        for i, level in enumerate(level_columns, 1):
            column_mapping[level] = f'çº§åˆ«{i}'
        
        pivot_df = pivot_df.rename(columns=column_mapping)
        
        # ç”Ÿæˆæ ‡å‡†åŒ–çš„è¡¨æ ¼å­—ç¬¦ä¸²
        table_lines = []
        
        # è¡¨å¤´
        headers = list(pivot_df.columns)
        table_lines.append("\t".join(headers))
        
        # æ•°æ®è¡Œ
        for _, row in pivot_df.iterrows():
            row_data = []
            for col in headers:
                value = row[col]
                if isinstance(value, (int, float)):
                    # æ•°å€¼æ ¼å¼åŒ–ï¼šä¿ç•™3ä½å°æ•°
                    row_data.append(f"{value:.3f}")
                else:
                    row_data.append(str(value))
            table_lines.append("\t".join(row_data))
        
        return "\n".join(table_lines)
        
    except Exception as e:
        print(f"ç”Ÿæˆæ ‡å‡†åŒ–è¡¨æ ¼æ—¶å‡ºé”™: {e}")
        return ""

### æ£€æŸ¥éš¶å±åº¦çŸ©é˜µæ ¼å¼å‡½æ•°
def _is_membership_matrix(df: pd.DataFrame) -> bool:
    """
    æ£€æŸ¥DataFrameæ˜¯å¦ä¸ºéš¶å±åº¦çŸ©é˜µæ ¼å¼
    
    Args:
        df: è¦æ£€æŸ¥çš„DataFrame
        
    Returns:
        å¦‚æœæ˜¯éš¶å±åº¦çŸ©é˜µç»“æ„è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if df.empty:
        return False
    
    # æ£€æŸ¥æ‰å¹³åŒ–æ ¼å¼ï¼ˆåŒ…å«å¿…è¦çš„åˆ—ï¼‰
    required_columns = ['ä¸šåŠ¡å¯¹è±¡', 'è¯„ä»·å› å­', 'è¯„ä¼°çº§åˆ«', 'éš¶å±åº¦']
    if all(col in df.columns for col in required_columns):
        return True
    
    # æ£€æŸ¥åµŒå¥—å­—å…¸ç»“æ„ï¼ˆåŸå§‹æ ¼å¼ï¼‰
    first_row = df.iloc[0]
    for value in first_row:
        if isinstance(value, dict):
            # æ£€æŸ¥å­—å…¸é”®æ˜¯å¦åŒ…å«"çº§åˆ«"å­—æ ·
            if any("çº§åˆ«" in str(key) for key in value.keys()):
                return True
    
    return False

### è½¬æ¢éš¶å±åº¦çŸ©é˜µä¸ºæ‰å¹³åŒ–æ ¼å¼å‡½æ•°
def _flatten_membership_matrix(df: pd.DataFrame) -> pd.DataFrame:
    """
    å°†éš¶å±åº¦çŸ©é˜µçš„åµŒå¥—ç»“æ„è½¬æ¢ä¸ºæ‰å¹³åŒ–ç»“æ„
    
    Args:
        df: éš¶å±åº¦çŸ©é˜µDataFrameï¼ˆåµŒå¥—ç»“æ„ï¼‰
        
    Returns:
        æ‰å¹³åŒ–åçš„DataFrame
    """
    flattened_results = []
    
    # è·å–åŸå§‹æ•°æ®çš„ä¸šåŠ¡å¯¹è±¡æ ‡è¯†ç¬¦ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    # å‡è®¾åŸå§‹æ•°æ®çš„ç¬¬ä¸€åˆ—æ˜¯ä¸šåŠ¡å¯¹è±¡æ ‡è¯†ç¬¦
    object_identifier = df.columns[0] if len(df.columns) > 0 else "ä¸šåŠ¡å¯¹è±¡"
    
    for row_idx, row in df.iterrows():
        # è·å–ä¸šåŠ¡å¯¹è±¡æ ‡è¯†ç¬¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        object_id = row[object_identifier] if object_identifier in row else f"å¯¹è±¡{row_idx + 1}"
        
        # éå†æ¯ä¸ªå­—æ®µçš„éš¶å±åº¦å­—å…¸
        for field_name, level_data in row.items():
            # è·³è¿‡ä¸šåŠ¡å¯¹è±¡æ ‡è¯†ç¬¦åˆ—
            if field_name == object_identifier:
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºåµŒå¥—å­—å…¸ç»“æ„
            if isinstance(level_data, dict):
                for level_name, membership_value in level_data.items():
                    flattened_results.append({
                        object_identifier: object_id,
                        "è¯„ä»·å› å­": field_name,
                        "çº§åˆ«": level_name,
                        "éš¶å±åº¦": membership_value
                    })
    
    # åˆ›å»ºæ‰å¹³åŒ–çš„DataFrame
    if flattened_results:
        return pd.DataFrame(flattened_results)
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åµŒå¥—ç»“æ„ï¼Œè¿”å›åŸå§‹DataFrame
        return df

### è½¬æ¢éš¶å±åº¦çŸ©é˜µä¸ºè§„èŒƒåŒ–ç‰¹å¾å€¼çŸ©é˜µå‡½æ•°
def _convert_membership_to_normalized_matrix(membership_df: pd.DataFrame) -> Dict[str, np.ndarray]:
    """
    å°†éš¶å±åº¦çŸ©é˜µè½¬æ¢ä¸ºè§„èŒƒåŒ–ç‰¹å¾å€¼çŸ©é˜µæ ¼å¼
    
    Args:
        membership_df: éš¶å±åº¦çŸ©é˜µDataFrameï¼ˆæ‰å¹³åŒ–æ ¼å¼ï¼‰
        
    Returns:
        è§„èŒƒåŒ–ç‰¹å¾å€¼çŸ©é˜µå­—å…¸ï¼Œæ ¼å¼ä¸º{"T1": çŸ©é˜µ, "T2": çŸ©é˜µ, ...}
    """
    # æŒ‰ä¸šåŠ¡å¯¹è±¡åˆ†ç»„
    grouped = membership_df.groupby('ä¸šåŠ¡å¯¹è±¡')
    
    normalized_matrices = {}
    
    for obj_name, group in grouped:
        # æŒ‰è¯„ä»·å› å­åˆ†ç»„
        factor_groups = group.groupby('è¯„ä»·å› å­')
        
        matrix = []
        factor_names = []
        
        for factor_name, factor_group in factor_groups:
            # æŒ‰çº§åˆ«æ’åº
            factor_group = factor_group.sort_values('è¯„ä¼°çº§åˆ«')
            
            # æå–éš¶å±åº¦å€¼ä½œä¸ºä¸€è¡Œ
            row = factor_group['éš¶å±åº¦'].tolist()
            matrix.append(row)
            factor_names.append(factor_name)
        
        if matrix:
            normalized_matrices[obj_name] = np.array(matrix)
    
    return normalized_matrices


# =========================================================================================
# ğŸš€ ä¸»ç¨‹åºå…¥å£
# =========================================================================================
if __name__ == "__main__":
    # è¿è¡Œ MCP æœåŠ¡å™¨
    mcp.run(transport="stdio")